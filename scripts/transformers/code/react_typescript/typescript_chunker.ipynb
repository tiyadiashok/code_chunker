{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124693f6",
   "metadata": {},
   "source": [
    "# Phase 1: Initialization and Setup - Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057acdcf",
   "metadata": {},
   "source": [
    "Configuration Settings (Markdown)\n",
    "This cell contains all the configuration parameters for the code chunking system. Modify these settings according to your project requirements before running the chunking process.\n",
    "Key Configuration Areas:\n",
    "\n",
    "Directory Paths: Set your target React/TypeScript project directory and desired output location\n",
    "Processing Parameters: Control chunk sizes, token limits, and recursion depth\n",
    "File Processing: Define supported file types and exclusion patterns\n",
    "Fallback Settings: Configure the fallback chunking algorithm parameters\n",
    "Logging: Set up logging verbosity and format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7738d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pip install tree-sitter-language-pack\n",
    "pip install tiktoken\n",
    "pip install pathspec  # for .gitignore parsing\n",
    "pip install pyyaml\n",
    "pip install ipykernel\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import logging\n",
    "import hashlib\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple, Union, Any\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "\n",
    "# Tree-sitter imports\n",
    "from tree_sitter_language_pack import get_binding, get_language, get_parser\n",
    "from tree_sitter import Tree, Node\n",
    "\n",
    "# Token counting\n",
    "import tiktoken\n",
    "\n",
    "# .gitignore parsing\n",
    "import pathspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d619932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded successfully\n",
      "ðŸ“ Target Directory: /Users/tiyadiashok/python-projects/code_chunker/rag_sources/code_sources/typescript\n",
      "ðŸ“ Output Directory: /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/code_sources/typescript\n",
      "ðŸŽ¯ Max Chunk Tokens: 1000\n",
      "ðŸ“ Supported Extensions: .tsx, .ts, .js, .jsx, .html, .css, .scss\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION SETTINGS - MODIFY THESE FOR YOUR PROJECT\n",
    "# =============================================================================\n",
    "\n",
    "# Directory paths - CHANGE THESE TO YOUR PROJECT PATHS\n",
    "TARGET_DIRECTORY = r\"/Users/tiyadiashok/python-projects/code_chunker/rag_sources/code_sources/typescript\"  # Your React/TypeScript project\n",
    "OUTPUT_DIRECTORY = r\"/Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/code_sources/typescript\"       # Where to save chunked files\n",
    "\n",
    "# Processing parameters\n",
    "MAX_CHUNK_TOKENS = 1000          # Maximum tokens per chunk (GPT-4o tokenization)\n",
    "MIN_CHUNK_CHARS = 50             # Minimum characters per chunk\n",
    "COALESCE_THRESHOLD = 50          # Minimum size for chunk coalescing\n",
    "IMPORT_LINES_THRESHOLD = 10      # Threshold for creating separate import chunks\n",
    "MAX_RECURSION_DEPTH = 3          # Maximum depth for recursive sub-chunking\n",
    "\n",
    "# File processing\n",
    "SUPPORTED_EXTENSIONS = ['.tsx', '.ts', '.js', '.jsx', '.html', '.css', '.scss']\n",
    "ADDITIONAL_EXCLUSIONS = [\n",
    "    'node_modules',              # Node.js dependencies\n",
    "    'dist',                      # Distribution/build folder\n",
    "    'build',                     # Build output\n",
    "    '.next',                     # Next.js build cache\n",
    "    '.git',                      # Git repository\n",
    "    '*.min.js',                  # Minified JavaScript\n",
    "    '*.min.css',                 # Minified CSS\n",
    "    'coverage',                  # Test coverage reports\n",
    "    '.vscode',                   # VS Code settings\n",
    "    '.idea',                     # JetBrains IDE settings\n",
    "]\n",
    "\n",
    "# Fallback chunker settings (Sweep's algorithm)\n",
    "FALLBACK_MAX_CHARS = 512 * 3     # 1536 characters for fallback chunking\n",
    "FALLBACK_COALESCE = 50           # Coalescing threshold for fallback\n",
    "\n",
    "# Token encoding\n",
    "TIKTOKEN_ENCODING = \"cl100k_base\"  # GPT-4o compatible encoding\n",
    "\n",
    "# Logging configuration\n",
    "LOG_LEVEL = logging.INFO\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "\n",
    "print(\"âœ… Configuration loaded successfully\")\n",
    "print(f\"ðŸ“ Target Directory: {TARGET_DIRECTORY}\")\n",
    "print(f\"ðŸ“ Output Directory: {OUTPUT_DIRECTORY}\")\n",
    "print(f\"ðŸŽ¯ Max Chunk Tokens: {MAX_CHUNK_TOKENS}\")\n",
    "print(f\"ðŸ“ Supported Extensions: {', '.join(SUPPORTED_EXTENSIONS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9acfcc2",
   "metadata": {},
   "source": [
    "Core Imports and Dependencies (Markdown)\n",
    "This cell imports all required libraries and modules for the chunking system. The imports are organized by functionality:\n",
    "Core Libraries:\n",
    "\n",
    "Standard Python libraries for file operations, logging, and data structures\n",
    "pathlib for cross-platform path handling\n",
    "dataclasses and enum for structured data types\n",
    "\n",
    "External Dependencies:\n",
    "\n",
    "tree-sitter-language-pack for AST parsing\n",
    "tiktoken for GPT-4o token counting\n",
    "pathspec for .gitignore pattern matching\n",
    "pyyaml for YAML frontmatter generation\n",
    "\n",
    "Error Handling:\n",
    "\n",
    "Graceful handling of missing dependencies with helpful error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30cc068c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tree-sitter dependencies loaded\n",
      "âœ… Tiktoken loaded\n",
      "âœ… Pathspec loaded\n",
      "âœ… PyYAML loaded\n",
      "\n",
      "ðŸŽ‰ All dependencies loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CORE IMPORTS AND DEPENDENCIES\n",
    "# =============================================================================\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import logging\n",
    "import hashlib\n",
    "import secrets\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple, Union, Any\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "\n",
    "# Check and import external dependencies\n",
    "try:\n",
    "    # Tree-sitter imports\n",
    "    from tree_sitter_language_pack import get_binding, get_language, get_parser\n",
    "    from tree_sitter import Tree, Node\n",
    "    print(\"âœ… Tree-sitter dependencies loaded\")\n",
    "except ImportError as e:\n",
    "    print(\"âŒ Error importing tree-sitter dependencies:\")\n",
    "    print(\"   Please install: pip install tree-sitter-language-pack\")\n",
    "    raise e\n",
    "\n",
    "try:\n",
    "    # Token counting\n",
    "    import tiktoken\n",
    "    print(\"âœ… Tiktoken loaded\")\n",
    "except ImportError as e:\n",
    "    print(\"âŒ Error importing tiktoken:\")\n",
    "    print(\"   Please install: pip install tiktoken\")\n",
    "    raise e\n",
    "\n",
    "try:\n",
    "    # .gitignore parsing\n",
    "    import pathspec\n",
    "    print(\"âœ… Pathspec loaded\")\n",
    "except ImportError as e:\n",
    "    print(\"âŒ Error importing pathspec:\")\n",
    "    print(\"   Please install: pip install pathspec\")\n",
    "    raise e\n",
    "\n",
    "# Verify PyYAML is available\n",
    "try:\n",
    "    import yaml\n",
    "    print(\"âœ… PyYAML loaded\")\n",
    "except ImportError as e:\n",
    "    print(\"âŒ Error importing PyYAML:\")\n",
    "    print(\"   Please install: pip install pyyaml\")\n",
    "    raise e\n",
    "\n",
    "print(\"\\nðŸŽ‰ All dependencies loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e71da4",
   "metadata": {},
   "source": [
    "Data Structures and Enums (Markdown)\n",
    "This cell defines the core data structures used throughout the chunking system. These classes provide type safety and structured data handling:\n",
    "Enums:\n",
    "\n",
    "ChunkType: Categorizes chunks as code, imports, or fallback\n",
    "ChunkMethod: Tracks whether AST parsing or fallback chunking was used\n",
    "\n",
    "Data Classes:\n",
    "\n",
    "FileMetadata: Stores information about each source file\n",
    "ImportInfo: Manages import classification and counting\n",
    "ChunkSpan: Represents the position and content of a chunk\n",
    "CodeChunk: Complete chunk information with all metadata\n",
    "ProcessingStats: Tracks processing results and performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88e4a448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data structures and enums defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA STRUCTURES AND ENUMS\n",
    "# =============================================================================\n",
    "\n",
    "class ChunkType(Enum):\n",
    "    \"\"\"Types of chunks that can be created\"\"\"\n",
    "    CODE = \"code\"           # Regular code chunks\n",
    "    IMPORTS = \"imports\"     # Dedicated import chunks\n",
    "    FALLBACK = \"fallback\"   # Chunks created using fallback strategy\n",
    "\n",
    "class ChunkMethod(Enum):\n",
    "    \"\"\"Methods used for chunking\"\"\"\n",
    "    AST = \"ast\"             # AST-based chunking (preferred)\n",
    "    FALLBACK = \"fallback\"   # Fallback chunking (when AST fails)\n",
    "\n",
    "@dataclass\n",
    "class FileMetadata:\n",
    "    \"\"\"Metadata for a source file\"\"\"\n",
    "    file_path: Path\n",
    "    relative_path: str\n",
    "    extension: str\n",
    "    size_bytes: int\n",
    "    total_lines: int\n",
    "    import_lines: int\n",
    "    has_package_json: bool = False\n",
    "    \n",
    "@dataclass\n",
    "class ImportInfo:\n",
    "    \"\"\"Information about imports in a file\"\"\"\n",
    "    external_imports: List[str] = field(default_factory=list)\n",
    "    local_imports: List[str] = field(default_factory=list)\n",
    "    all_imports: List[str] = field(default_factory=list)\n",
    "    import_line_count: int = 0\n",
    "\n",
    "@dataclass\n",
    "class ChunkSpan:\n",
    "    \"\"\"Represents a chunk's position in source code\"\"\"\n",
    "    start_byte: int\n",
    "    end_byte: int\n",
    "    start_line: int\n",
    "    end_line: int\n",
    "    content: str\n",
    "\n",
    "@dataclass\n",
    "class CodeChunk:\n",
    "    \"\"\"Complete chunk information with all metadata\"\"\"\n",
    "    source_file: str\n",
    "    chunk_index: int\n",
    "    total_chunks: int\n",
    "    chunk_type: ChunkType\n",
    "    chunk_method: ChunkMethod\n",
    "    content: str\n",
    "    imports_used: List[str]\n",
    "    structure: str\n",
    "    summary: str\n",
    "    file_id: str\n",
    "    span: ChunkSpan\n",
    "\n",
    "@dataclass\n",
    "class ProcessingStats:\n",
    "    \"\"\"Statistics for processing results\"\"\"\n",
    "    total_files: int = 0\n",
    "    successfully_parsed: int = 0\n",
    "    fallback_chunked: int = 0\n",
    "    total_chunks: int = 0\n",
    "    files_with_import_chunks: int = 0\n",
    "    processing_time: float = 0.0\n",
    "    error_files: List[str] = field(default_factory=list)\n",
    "\n",
    "print(\"âœ… Data structures and enums defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5d0244",
   "metadata": {},
   "source": [
    "Logger Setup (Markdown)\n",
    "This cell configures the logging system for the chunking process. The logger provides:\n",
    "Features:\n",
    "\n",
    "Configurable log levels (DEBUG, INFO, WARNING, ERROR)\n",
    "Timestamped log messages with clear formatting\n",
    "Console output for real-time monitoring\n",
    "Proper error tracking and reporting\n",
    "\n",
    "Log Levels:\n",
    "\n",
    "INFO: General processing information, progress updates\n",
    "WARNING: Non-critical issues, fallback usage\n",
    "ERROR: Critical errors, file processing failures\n",
    "DEBUG: Detailed debugging information (verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "428ba6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 19:28:53,910 - INFO - ðŸ”§ Logging system initialized\n",
      "2025-08-02 19:28:53,911 - INFO - ðŸ“Š Log level set to: INFO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Logger setup complete\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOGGER SETUP\n",
    "# =============================================================================\n",
    "\n",
    "def setup_logging(log_level: int = logging.INFO) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Set up logging configuration for the chunking system.\n",
    "    \n",
    "    Args:\n",
    "        log_level: Logging level (default: INFO)\n",
    "        \n",
    "    Returns:\n",
    "        Configured logger instance\n",
    "    \"\"\"\n",
    "    # Create logger\n",
    "    logger = logging.getLogger('code_chunker')\n",
    "    logger.setLevel(log_level)\n",
    "    \n",
    "    # Remove existing handlers to avoid duplicate logs\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "    \n",
    "    # Create console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(log_level)\n",
    "    \n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter(LOG_FORMAT)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Add handler to logger\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    # Prevent propagation to root logger\n",
    "    logger.propagate = False\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Initialize the logger\n",
    "logger = setup_logging(LOG_LEVEL)\n",
    "logger.info(\"ðŸ”§ Logging system initialized\")\n",
    "logger.info(f\"ðŸ“Š Log level set to: {logging.getLevelName(LOG_LEVEL)}\")\n",
    "\n",
    "print(\"âœ… Logger setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8541d9",
   "metadata": {},
   "source": [
    "Token Counter Initialization (Markdown)\n",
    "This cell sets up the token counting system using OpenAI's tiktoken library. Token counting is crucial for:\n",
    "Purpose:\n",
    "\n",
    "Ensuring chunks stay within the 1000 token limit\n",
    "Accurate token counting for GPT-4o compatibility\n",
    "Determining when chunks need sub-chunking or coalescing\n",
    "\n",
    "Token Encoding:\n",
    "\n",
    "Uses cl100k_base encoding (GPT-4o compatible)\n",
    "Provides accurate token counts for modern language models\n",
    "Handles special characters and Unicode properly\n",
    "\n",
    "Functions:\n",
    "\n",
    "initialize_token_counter(): Sets up the tiktoken encoder\n",
    "count_tokens(): Counts tokens in any text string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67ecb31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 19:28:53,920 - INFO - ðŸŽ¯ Token encoder initialized with cl100k_base\n",
      "2025-08-02 19:28:53,938 - INFO - âœ… Token counting test: 'function helloWorld() { console.log('Hello, World!'); }' = 14 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Token counter initialized and tested\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TOKEN COUNTER INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def initialize_token_counter(encoding_name: str = \"cl100k_base\") -> tiktoken.Encoding:\n",
    "    \"\"\"\n",
    "    Initialize tiktoken encoder for token counting.\n",
    "    \n",
    "    Args:\n",
    "        encoding_name: Name of the tiktoken encoding to use\n",
    "        \n",
    "    Returns:\n",
    "        Initialized tiktoken encoding instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoder = tiktoken.get_encoding(encoding_name)\n",
    "        logger.info(f\"ðŸŽ¯ Token encoder initialized with {encoding_name}\")\n",
    "        return encoder\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Failed to initialize token encoder: {e}\")\n",
    "        raise\n",
    "\n",
    "def count_tokens(text: str, encoder: tiktoken.Encoding) -> int:\n",
    "    \"\"\"\n",
    "    Count tokens in text using tiktoken.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to count tokens for\n",
    "        encoder: Tiktoken encoder instance\n",
    "        \n",
    "    Returns:\n",
    "        Number of tokens in the text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not text:\n",
    "            return 0\n",
    "        return len(encoder.encode(text))\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"âš ï¸ Error counting tokens: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Initialize the global token encoder\n",
    "token_encoder = initialize_token_counter(TIKTOKEN_ENCODING)\n",
    "\n",
    "# Test token counting with a sample\n",
    "test_text = \"function helloWorld() { console.log('Hello, World!'); }\"\n",
    "test_tokens = count_tokens(test_text, token_encoder)\n",
    "logger.info(f\"âœ… Token counting test: '{test_text}' = {test_tokens} tokens\")\n",
    "\n",
    "print(\"âœ… Token counter initialized and tested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdaafb4",
   "metadata": {},
   "source": [
    "Tree-sitter Parser Manager (Markdown)\n",
    "This cell implements the ParserManager class, which handles all Tree-sitter AST parsing operations. The parser manager:\n",
    "Responsibilities:\n",
    "\n",
    "Initializes parsers for different file types (TypeScript, HTML, CSS)\n",
    "Maps file extensions to appropriate parsers\n",
    "Handles parsing failures gracefully\n",
    "Provides a unified interface for AST parsing\n",
    "\n",
    "Supported Languages:\n",
    "\n",
    "TypeScript/TSX: Handles .tsx, .ts, .js, .jsx files\n",
    "HTML: Handles .html files\n",
    "CSS: Handles .css, .scss files\n",
    "\n",
    "Error Handling:\n",
    "\n",
    "Graceful fallback when parsers fail to initialize\n",
    "Detailed error logging for debugging\n",
    "Returns None for unparseable files (triggers fallback chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7f8375f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 19:28:53,953 - INFO - âœ… Initialized tsx parser for ['.tsx', '.ts', '.js', '.jsx']\n",
      "2025-08-02 19:28:53,955 - INFO - âœ… Initialized html parser for ['.html']\n",
      "2025-08-02 19:28:53,958 - INFO - âœ… Initialized css parser for ['.css', '.scss']\n",
      "2025-08-02 19:28:53,959 - INFO - ðŸŽ¯ Parser manager initialized\n",
      "2025-08-02 19:28:53,959 - INFO - ðŸ“ Supported extensions: .css, .html, .js, .jsx, .scss, .ts, .tsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tree-sitter parser manager initialized\n",
      "ðŸ“ Supported file types: .css, .html, .js, .jsx, .scss, .ts, .tsx\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TREE-SITTER PARSER MANAGER\n",
    "# =============================================================================\n",
    "\n",
    "class ParserManager:\n",
    "    \"\"\"Manages Tree-sitter parsers for different file types\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parsers = {}\n",
    "        self.languages = {}\n",
    "        self._initialize_parsers()\n",
    "    \n",
    "    def _initialize_parsers(self) -> None:\n",
    "        \"\"\"Initialize all required Tree-sitter parsers\"\"\"\n",
    "        # Parser configurations: (language_name, file_extensions)\n",
    "        parser_configs = [\n",
    "            ('tsx', ['.tsx', '.ts', '.js', '.jsx']),\n",
    "            ('html', ['.html']),\n",
    "            ('css', ['.css', '.scss'])\n",
    "        ]\n",
    "        \n",
    "        for language_name, extensions in parser_configs:\n",
    "            try:\n",
    "                # Get language and parser\n",
    "                language = get_language(language_name)\n",
    "                parser = get_parser(language_name)\n",
    "                \n",
    "                # Store language and parser for each extension\n",
    "                for ext in extensions:\n",
    "                    self.languages[ext] = language\n",
    "                    self.parsers[ext] = parser\n",
    "                \n",
    "                logger.info(f\"âœ… Initialized {language_name} parser for {extensions}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed to initialize {language_name} parser: {e}\")\n",
    "                # Continue with other parsers even if one fails\n",
    "                continue\n",
    "    \n",
    "    def get_parser(self, file_extension: str) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Get appropriate parser for file extension.\n",
    "        \n",
    "        Args:\n",
    "            file_extension: File extension (e.g., '.tsx', '.css')\n",
    "            \n",
    "        Returns:\n",
    "            Tree-sitter parser instance or None if unsupported\n",
    "        \"\"\"\n",
    "        parser = self.parsers.get(file_extension.lower())\n",
    "        if parser is None:\n",
    "            logger.warning(f\"âš ï¸ No parser available for extension: {file_extension}\")\n",
    "        return parser\n",
    "    \n",
    "    def get_language(self, file_extension: str) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Get appropriate language for file extension.\n",
    "        \n",
    "        Args:\n",
    "            file_extension: File extension (e.g., '.tsx', '.css')\n",
    "            \n",
    "        Returns:\n",
    "            Tree-sitter language instance or None if unsupported\n",
    "        \"\"\"\n",
    "        return self.languages.get(file_extension.lower())\n",
    "    \n",
    "    def parse_file(self, file_path: Path, content: bytes) -> Optional[Tree]:\n",
    "        \"\"\"\n",
    "        Parse file content using appropriate parser.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the file being parsed\n",
    "            content: File content as bytes\n",
    "            \n",
    "        Returns:\n",
    "            Parsed Tree object or None if parsing failed\n",
    "        \"\"\"\n",
    "        file_extension = file_path.suffix.lower()\n",
    "        parser = self.get_parser(file_extension)\n",
    "        \n",
    "        if parser is None:\n",
    "            logger.warning(f\"âš ï¸ No parser for {file_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            tree = parser.parse(content)\n",
    "            if tree.root_node.has_error:\n",
    "                logger.warning(f\"âš ï¸ Parse errors in {file_path}\")\n",
    "                return None\n",
    "            \n",
    "            logger.debug(f\"âœ… Successfully parsed {file_path}\")\n",
    "            return tree\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Failed to parse {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def list_supported_extensions(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get list of supported file extensions.\n",
    "        \n",
    "        Returns:\n",
    "            List of supported file extensions\n",
    "        \"\"\"\n",
    "        return list(self.parsers.keys())\n",
    "\n",
    "# Initialize the global parser manager\n",
    "parser_manager = ParserManager()\n",
    "\n",
    "# Display initialization results\n",
    "supported_extensions = parser_manager.list_supported_extensions()\n",
    "logger.info(f\"ðŸŽ¯ Parser manager initialized\")\n",
    "logger.info(f\"ðŸ“ Supported extensions: {', '.join(sorted(supported_extensions))}\")\n",
    "\n",
    "print(\"âœ… Tree-sitter parser manager initialized\")\n",
    "print(f\"ðŸ“ Supported file types: {', '.join(sorted(supported_extensions))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d57113",
   "metadata": {},
   "source": [
    "ST Node Type Mappings (Markdown)\n",
    "This cell defines the AST node types that serve as chunk boundaries for different file types. These mappings are based on semantic code structures:\n",
    "Purpose:\n",
    "\n",
    "Define which AST nodes should create new chunks\n",
    "Organize mappings by file type for targeted chunking\n",
    "Provide sub-chunking strategies for oversized chunks\n",
    "\n",
    "Chunk Boundary Types:\n",
    "\n",
    "Functions: Function declarations, arrow functions, methods\n",
    "Classes: Class declarations and interfaces\n",
    "Components: JSX elements and React components\n",
    "Declarations: Variable declarations, type aliases, enums\n",
    "Styles: CSS rules, at-rules, keyframes\n",
    "\n",
    "Sub-chunking Strategy:\n",
    "\n",
    "Used when base chunks exceed token limits\n",
    "Provides finer-grained boundaries within large constructs\n",
    "Includes control flow statements and logic blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d5cdb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 19:28:53,976 - INFO - ðŸŽ¯ AST node type mappings configured\n",
      "2025-08-02 19:28:53,977 - INFO - ðŸ“ Configured for 7 file types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AST node type mappings configured\n",
      "ðŸ“ File types configured: .tsx, .ts, .js, .jsx, .html, .css, .scss\n",
      "ðŸŽ¯ Example boundaries for .tsx: function_declaration, arrow_function, function_expression, method_definition, class_declaration...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# AST NODE TYPE MAPPINGS\n",
    "# =============================================================================\n",
    "\n",
    "# Primary chunk boundaries for different file types\n",
    "AST_CHUNK_BOUNDARIES = {\n",
    "    '.tsx': [\n",
    "        'function_declaration', 'arrow_function', 'function_expression',\n",
    "        'method_definition', 'class_declaration', 'interface_declaration',\n",
    "        'type_alias_declaration', 'enum_declaration', 'variable_declaration',\n",
    "        'export_statement', 'jsx_element', 'jsx_fragment', 'namespace_declaration'\n",
    "    ],\n",
    "    '.ts': [\n",
    "        'function_declaration', 'arrow_function', 'function_expression',\n",
    "        'method_definition', 'class_declaration', 'interface_declaration',\n",
    "        'type_alias_declaration', 'enum_declaration', 'variable_declaration',\n",
    "        'export_statement', 'namespace_declaration'\n",
    "    ],\n",
    "    '.js': [\n",
    "        'function_declaration', 'arrow_function', 'function_expression',\n",
    "        'method_definition', 'class_declaration', 'variable_declaration',\n",
    "        'export_statement'\n",
    "    ],\n",
    "    '.jsx': [\n",
    "        'function_declaration', 'arrow_function', 'function_expression',\n",
    "        'method_definition', 'class_declaration', 'variable_declaration',\n",
    "        'export_statement', 'jsx_element', 'jsx_fragment'\n",
    "    ],\n",
    "    '.html': [\n",
    "        'element', 'script_element', 'style_element'\n",
    "    ],\n",
    "    '.css': [\n",
    "        'rule_set', 'at_rule', 'keyframes_statement'\n",
    "    ],\n",
    "    '.scss': [\n",
    "        'rule_set', 'at_rule', 'keyframes_statement'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Sub-chunking node types for when chunks are too large\n",
    "AST_SUB_CHUNK_BOUNDARIES = {\n",
    "    'common': [\n",
    "        'for_statement', 'while_statement', 'if_statement', \n",
    "        'switch_statement', 'try_statement', 'block'\n",
    "    ],\n",
    "    'jsx': ['jsx_element', 'jsx_expression'],\n",
    "    'css': ['declaration', 'property'],\n",
    "    'functions': ['parameter', 'argument']\n",
    "}\n",
    "\n",
    "# Import-related node types for different languages\n",
    "AST_IMPORT_NODES = {\n",
    "    '.tsx': ['import_statement', 'import_declaration'],\n",
    "    '.ts': ['import_statement', 'import_declaration'],\n",
    "    '.js': ['import_statement', 'import_declaration'],\n",
    "    '.jsx': ['import_statement', 'import_declaration'],\n",
    "    '.css': ['import_statement'],\n",
    "    '.scss': ['import_statement']\n",
    "}\n",
    "\n",
    "def get_chunk_boundaries_for_extension(extension: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get chunk boundary node types for a file extension.\n",
    "    \n",
    "    Args:\n",
    "        extension: File extension (e.g., '.tsx')\n",
    "        \n",
    "    Returns:\n",
    "        List of AST node types that should create chunk boundaries\n",
    "    \"\"\"\n",
    "    return AST_CHUNK_BOUNDARIES.get(extension.lower(), [])\n",
    "\n",
    "def get_sub_chunk_boundaries(extension: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get sub-chunk boundary node types for a file extension.\n",
    "    \n",
    "    Args:\n",
    "        extension: File extension\n",
    "        \n",
    "    Returns:\n",
    "        List of AST node types for sub-chunking\n",
    "    \"\"\"\n",
    "    boundaries = AST_SUB_CHUNK_BOUNDARIES['common'].copy()\n",
    "    \n",
    "    # Add extension-specific sub-boundaries\n",
    "    if extension.lower() in ['.tsx', '.jsx']:\n",
    "        boundaries.extend(AST_SUB_CHUNK_BOUNDARIES['jsx'])\n",
    "    elif extension.lower() in ['.css', '.scss']:\n",
    "        boundaries.extend(AST_SUB_CHUNK_BOUNDARIES['css'])\n",
    "    \n",
    "    return boundaries\n",
    "\n",
    "def get_import_nodes_for_extension(extension: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get import-related node types for a file extension.\n",
    "    \n",
    "    Args:\n",
    "        extension: File extension\n",
    "        \n",
    "    Returns:\n",
    "        List of AST node types related to imports\n",
    "    \"\"\"\n",
    "    return AST_IMPORT_NODES.get(extension.lower(), [])\n",
    "\n",
    "# Log the configuration\n",
    "logger.info(\"ðŸŽ¯ AST node type mappings configured\")\n",
    "logger.info(f\"ðŸ“ Configured for {len(AST_CHUNK_BOUNDARIES)} file types\")\n",
    "\n",
    "# Display summary\n",
    "print(\"âœ… AST node type mappings configured\")\n",
    "print(f\"ðŸ“ File types configured: {', '.join(AST_CHUNK_BOUNDARIES.keys())}\")\n",
    "print(f\"ðŸŽ¯ Example boundaries for .tsx: {', '.join(AST_CHUNK_BOUNDARIES['.tsx'][:5])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2ebf6",
   "metadata": {},
   "source": [
    "Initialization Summary and Validation (Markdown)\n",
    "This final cell provides a comprehensive summary of the initialization process and validates that all components are properly set up.\n",
    "Validation Checks:\n",
    "\n",
    "Configuration parameters are valid\n",
    "All required dependencies are loaded\n",
    "Parsers are initialized for all supported file types\n",
    "Token counter is working correctly\n",
    "Data structures are properly defined\n",
    "\n",
    "Status Report:\n",
    "\n",
    "Lists all initialized components\n",
    "Shows configuration summary\n",
    "Confirms system readiness for file processing\n",
    "Displays any warnings or issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13d373c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 19:28:53,991 - INFO - ðŸ”§ Phase 1 (Initialization) complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ” INITIALIZATION VALIDATION RESULTS\n",
      "============================================================\n",
      "âœ… Configuration: OK\n",
      "âœ… Token Counter: OK\n",
      "âœ… Parser Manager: OK\n",
      "âœ… Data Structures: OK\n",
      "âœ… AST Mappings: OK\n",
      "\n",
      "============================================================\n",
      "ðŸš€ CODE CHUNKING SYSTEM - INITIALIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "ðŸ“ DIRECTORIES:\n",
      "   Target: /Users/tiyadiashok/python-projects/code_chunker/rag_sources/code_sources/typescript\n",
      "   Output: /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/code_sources/typescript\n",
      "\n",
      "âš™ï¸ CONFIGURATION:\n",
      "   Max Chunk Tokens: 1000\n",
      "   Min Chunk Chars: 50\n",
      "   Import Threshold: 10 lines\n",
      "   Max Recursion Depth: 3\n",
      "\n",
      "ðŸ“ SUPPORTED FILE TYPES:\n",
      "   .tsx: âœ…\n",
      "   .ts: âœ…\n",
      "   .js: âœ…\n",
      "   .jsx: âœ…\n",
      "   .html: âœ…\n",
      "   .css: âœ…\n",
      "   .scss: âœ…\n",
      "\n",
      "ðŸŽ¯ COMPONENTS INITIALIZED:\n",
      "   âœ… Logger (INFO)\n",
      "   âœ… Token Counter (cl100k_base)\n",
      "   âœ… Parser Manager (7 parsers)\n",
      "   âœ… Data Structures\n",
      "   âœ… AST Mappings\n",
      "\n",
      "ðŸ”§ EXCLUSIONS:\n",
      "   â€¢ node_modules\n",
      "   â€¢ dist\n",
      "   â€¢ build\n",
      "   â€¢ .next\n",
      "   â€¢ .git\n",
      "   ... and 5 more\n",
      "\n",
      "ðŸŽ‰ SYSTEM READY FOR PROCESSING!\n",
      "   Next: Run Phase 2 - File Discovery\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# INITIALIZATION SUMMARY AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_initialization() -> bool:\n",
    "    \"\"\"\n",
    "    Validate that all components are properly initialized.\n",
    "    \n",
    "    Returns:\n",
    "        True if all components are ready, False otherwise\n",
    "    \"\"\"\n",
    "    validation_results = []\n",
    "    \n",
    "    # Check configuration\n",
    "    try:\n",
    "        config_valid = (\n",
    "            Path(TARGET_DIRECTORY).exists() and\n",
    "            MAX_CHUNK_TOKENS > 0 and\n",
    "            MIN_CHUNK_CHARS > 0 and\n",
    "            len(SUPPORTED_EXTENSIONS) > 0\n",
    "        )\n",
    "        validation_results.append((\"Configuration\", config_valid))\n",
    "    except:\n",
    "        validation_results.append((\"Configuration\", False))\n",
    "    \n",
    "    # Check token encoder\n",
    "    try:\n",
    "        test_tokens = count_tokens(\"test\", token_encoder)\n",
    "        token_valid = test_tokens > 0\n",
    "        validation_results.append((\"Token Counter\", token_valid))\n",
    "    except:\n",
    "        validation_results.append((\"Token Counter\", False))\n",
    "    \n",
    "    # Check parser manager\n",
    "    try:\n",
    "        parser_valid = len(parser_manager.list_supported_extensions()) > 0\n",
    "        validation_results.append((\"Parser Manager\", parser_valid))\n",
    "    except:\n",
    "        validation_results.append((\"Parser Manager\", False))\n",
    "    \n",
    "    # Check data structures\n",
    "    try:\n",
    "        test_chunk = CodeChunk(\n",
    "            source_file=\"test.tsx\",\n",
    "            chunk_index=1,\n",
    "            total_chunks=1,\n",
    "            chunk_type=ChunkType.CODE,\n",
    "            chunk_method=ChunkMethod.AST,\n",
    "            content=\"test\",\n",
    "            imports_used=[],\n",
    "            structure=\"\",\n",
    "            summary=\"test\",\n",
    "            file_id=\"test123\",\n",
    "            span=ChunkSpan(0, 4, 1, 1, \"test\")\n",
    "        )\n",
    "        data_valid = test_chunk.source_file == \"test.tsx\"\n",
    "        validation_results.append((\"Data Structures\", data_valid))\n",
    "    except:\n",
    "        validation_results.append((\"Data Structures\", False))\n",
    "    \n",
    "    # Check AST mappings\n",
    "    try:\n",
    "        tsx_boundaries = get_chunk_boundaries_for_extension('.tsx')\n",
    "        ast_valid = len(tsx_boundaries) > 0\n",
    "        validation_results.append((\"AST Mappings\", ast_valid))\n",
    "    except:\n",
    "        validation_results.append((\"AST Mappings\", False))\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ” INITIALIZATION VALIDATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_valid = True\n",
    "    for component, status in validation_results:\n",
    "        status_icon = \"âœ…\" if status else \"âŒ\"\n",
    "        print(f\"{status_icon} {component}: {'OK' if status else 'FAILED'}\")\n",
    "        if not status:\n",
    "            all_valid = False\n",
    "    \n",
    "    return all_valid\n",
    "\n",
    "def display_initialization_summary():\n",
    "    \"\"\"Display a comprehensive summary of the initialization.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸš€ CODE CHUNKING SYSTEM - INITIALIZATION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nðŸ“ DIRECTORIES:\")\n",
    "    print(f\"   Target: {TARGET_DIRECTORY}\")\n",
    "    print(f\"   Output: {OUTPUT_DIRECTORY}\")\n",
    "    \n",
    "    print(f\"\\nâš™ï¸ CONFIGURATION:\")\n",
    "    print(f\"   Max Chunk Tokens: {MAX_CHUNK_TOKENS}\")\n",
    "    print(f\"   Min Chunk Chars: {MIN_CHUNK_CHARS}\")\n",
    "    print(f\"   Import Threshold: {IMPORT_LINES_THRESHOLD} lines\")\n",
    "    print(f\"   Max Recursion Depth: {MAX_RECURSION_DEPTH}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ SUPPORTED FILE TYPES:\")\n",
    "    for ext in SUPPORTED_EXTENSIONS:\n",
    "        parser_status = \"âœ…\" if parser_manager.get_parser(ext) else \"âŒ\"\n",
    "        print(f\"   {ext}: {parser_status}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ COMPONENTS INITIALIZED:\")\n",
    "    print(f\"   âœ… Logger ({logging.getLevelName(LOG_LEVEL)})\")\n",
    "    print(f\"   âœ… Token Counter ({TIKTOKEN_ENCODING})\")\n",
    "    print(f\"   âœ… Parser Manager ({len(parser_manager.list_supported_extensions())} parsers)\")\n",
    "    print(f\"   âœ… Data Structures\")\n",
    "    print(f\"   âœ… AST Mappings\")\n",
    "    \n",
    "    print(f\"\\nðŸ”§ EXCLUSIONS:\")\n",
    "    for exclusion in ADDITIONAL_EXCLUSIONS[:5]:  # Show first 5\n",
    "        print(f\"   â€¢ {exclusion}\")\n",
    "    if len(ADDITIONAL_EXCLUSIONS) > 5:\n",
    "        print(f\"   ... and {len(ADDITIONAL_EXCLUSIONS) - 5} more\")\n",
    "\n",
    "# Run validation and display summary\n",
    "validation_passed = validate_initialization()\n",
    "\n",
    "if validation_passed:\n",
    "    display_initialization_summary()\n",
    "    print(f\"\\nðŸŽ‰ SYSTEM READY FOR PROCESSING!\")\n",
    "    print(\"   Next: Run Phase 2 - File Discovery\")\n",
    "else:\n",
    "    print(f\"\\nâŒ INITIALIZATION FAILED!\")\n",
    "    print(\"   Please check the error messages above and fix any issues.\")\n",
    "\n",
    "logger.info(\"ðŸ”§ Phase 1 (Initialization) complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e45efee",
   "metadata": {},
   "source": [
    "This completes Phase 1: Initialization and Setup. The implementation provides:\n",
    "\n",
    "Complete configuration system with validation\n",
    "Robust dependency management with error handling\n",
    "Comprehensive logging setup for debugging\n",
    "Token counting system for GPT-4o compatibility\n",
    "Tree-sitter parser management for all supported file types\n",
    "AST node type mappings for semantic chunking\n",
    "Validation and summary reporting for system readiness\n",
    "\n",
    "Each cell includes detailed markdown explanations and can be copied directly into a Jupyter notebook. The code is production-ready with proper error handling, logging, and validation checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea17c5",
   "metadata": {},
   "source": [
    "Phase 2: File Discovery and Filtering - Python Implementation\n",
    "Cell 17: Phase 2 Overview (Markdown)\n",
    "This phase implements the file discovery and filtering system that recursively scans the target directory to identify all processable code files. The system applies multiple filtering layers to ensure only relevant, accessible files are processed.\n",
    "Key Features:\n",
    "\n",
    "Recursive Directory Traversal: Scans all subdirectories while respecting exclusion patterns\n",
    ".gitignore Integration: Parses and applies .gitignore patterns from the project root\n",
    "Binary File Detection: Uses simple byte-based heuristic to skip binary files\n",
    "File Size Limits: Enforces 2MB maximum file size to prevent memory issues\n",
    "Progress Tracking: Provides real-time progress indicators for large codebases\n",
    "Error Recovery: Logs failures and continues processing remaining files\n",
    "\n",
    "Processing Flow:\n",
    "\n",
    "Load and parse .gitignore patterns\n",
    "Recursively discover all files in target directory\n",
    "Apply extension filtering (only supported file types)\n",
    "Apply exclusion patterns (.gitignore + additional exclusions)\n",
    "Validate file accessibility and size limits\n",
    "Generate FileMetadata objects for valid files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3efdc77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… .gitignore pattern loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2: FILE DISCOVERY AND FILTERING\n",
    "# =============================================================================\n",
    "\n",
    "def load_gitignore_patterns(project_root: Path) -> Optional[pathspec.PathSpec]:\n",
    "    \"\"\"\n",
    "    Load and parse .gitignore patterns from project root.\n",
    "    \n",
    "    Args:\n",
    "        project_root: Root directory of the project\n",
    "        \n",
    "    Returns:\n",
    "        PathSpec object for .gitignore patterns or None if not found\n",
    "    \"\"\"\n",
    "    gitignore_path = project_root / '.gitignore'\n",
    "    \n",
    "    if not gitignore_path.exists():\n",
    "        logger.info(\"ðŸ“‹ No .gitignore file found in project root\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(gitignore_path, 'r', encoding='utf-8') as f:\n",
    "            patterns = f.read().splitlines()\n",
    "        \n",
    "        # Filter out empty lines and comments\n",
    "        filtered_patterns = []\n",
    "        for pattern in patterns:\n",
    "            pattern = pattern.strip()\n",
    "            if pattern and not pattern.startswith('#'):\n",
    "                filtered_patterns.append(pattern)\n",
    "        \n",
    "        if filtered_patterns:\n",
    "            spec = pathspec.PathSpec.from_lines('gitwildmatch', filtered_patterns)\n",
    "            logger.info(f\"ðŸ“‹ Loaded {len(filtered_patterns)} .gitignore patterns\")\n",
    "            return spec\n",
    "        else:\n",
    "            logger.info(\"ðŸ“‹ .gitignore file found but contains no valid patterns\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Error reading .gitignore file: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_additional_exclusion_spec(additional_exclusions: List[str]) -> pathspec.PathSpec:\n",
    "    \"\"\"\n",
    "    Create PathSpec for additional exclusion patterns.\n",
    "    \n",
    "    Args:\n",
    "        additional_exclusions: List of additional patterns to exclude\n",
    "        \n",
    "    Returns:\n",
    "        PathSpec object for additional exclusions\n",
    "    \"\"\"\n",
    "    if not additional_exclusions:\n",
    "        return pathspec.PathSpec.from_lines('gitwildmatch', [])\n",
    "    \n",
    "    # Ensure node_modules is always excluded at any level\n",
    "    exclusion_patterns = additional_exclusions.copy()\n",
    "    if 'node_modules' not in exclusion_patterns:\n",
    "        exclusion_patterns.append('node_modules')\n",
    "    \n",
    "    # Add common exclusion patterns\n",
    "    exclusion_patterns.extend([\n",
    "        '**/node_modules',  # node_modules at any level\n",
    "        '**/node_modules/**',  # anything inside node_modules\n",
    "    ])\n",
    "    \n",
    "    logger.info(f\"ðŸ“‹ Created {len(exclusion_patterns)} additional exclusion patterns\")\n",
    "    return pathspec.PathSpec.from_lines('gitwildmatch', exclusion_patterns)\n",
    "\n",
    "print(\"âœ… .gitignore pattern loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e01fb3",
   "metadata": {},
   "source": [
    "File Validation Functions (Markdown)\n",
    "This cell implements core file validation functions that check whether discovered files should be processed. The validation includes:\n",
    "File Accessibility:\n",
    "\n",
    "Check if file exists and is readable\n",
    "Handle permission errors gracefully\n",
    "Skip files that can't be accessed\n",
    "\n",
    "Binary File Detection:\n",
    "\n",
    "Simple byte-based heuristic (presence of null bytes)\n",
    "Efficient early detection to avoid reading entire files\n",
    "Configurable sample size for detection\n",
    "\n",
    "File Size Validation:\n",
    "\n",
    "2MB maximum file size limit\n",
    "Prevents memory issues with extremely large files\n",
    "Logs oversized files for reference\n",
    "\n",
    "Empty File Handling:\n",
    "\n",
    "Skips files with 0 bytes\n",
    "Skips files containing only whitespace\n",
    "Optimizes processing by avoiding empty content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "792e6517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… File validation functions defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FILE VALIDATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def is_binary_file(file_path: Path, sample_size: int = 8192) -> bool:\n",
    "    \"\"\"\n",
    "    Check if file is binary using simple byte-based heuristic.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file to check\n",
    "        sample_size: Number of bytes to sample for detection\n",
    "        \n",
    "    Returns:\n",
    "        True if file appears to be binary, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            sample = f.read(sample_size)\n",
    "            \n",
    "        # Check for null bytes (common indicator of binary files)\n",
    "        if b'\\x00' in sample:\n",
    "            return True\n",
    "            \n",
    "        # Additional check: high ratio of non-printable characters\n",
    "        if len(sample) == 0:\n",
    "            return False\n",
    "            \n",
    "        # Count printable characters\n",
    "        printable_chars = 0\n",
    "        for byte in sample:\n",
    "            # ASCII printable range: 32-126, plus common whitespace: 9, 10, 13\n",
    "            if (32 <= byte <= 126) or byte in (9, 10, 13):\n",
    "                printable_chars += 1\n",
    "        \n",
    "        # If less than 85% printable characters, consider it binary\n",
    "        printable_ratio = printable_chars / len(sample)\n",
    "        return printable_ratio < 0.85\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"âš ï¸ Error checking if {file_path} is binary: {e}\")\n",
    "        return True  # Assume binary if can't read\n",
    "\n",
    "def validate_file_size(file_path: Path, max_size_mb: int = 2) -> bool:\n",
    "    \"\"\"\n",
    "    Validate file size is within acceptable limits.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file\n",
    "        max_size_mb: Maximum file size in megabytes\n",
    "        \n",
    "    Returns:\n",
    "        True if file size is acceptable, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        size_bytes = file_path.stat().st_size\n",
    "        size_mb = size_bytes / (1024 * 1024)\n",
    "        \n",
    "        if size_mb > max_size_mb:\n",
    "            logger.warning(f\"âš ï¸ File too large ({size_mb:.1f}MB): {file_path}\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Error checking file size for {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def is_empty_or_whitespace(file_path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Check if file is empty or contains only whitespace.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file\n",
    "        \n",
    "    Returns:\n",
    "        True if file is empty or whitespace-only, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read().strip()\n",
    "            return len(content) == 0\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"âš ï¸ Error checking if {file_path} is empty: {e}\")\n",
    "        return True  # Assume empty if can't read\n",
    "\n",
    "def validate_file_processable(file_path: Path) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Comprehensive validation if file can be processed.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_valid: bool, reason: str)\n",
    "    \"\"\"\n",
    "    # Check if file exists and is readable\n",
    "    if not file_path.exists():\n",
    "        return False, \"File does not exist\"\n",
    "    \n",
    "    if not file_path.is_file():\n",
    "        return False, \"Not a regular file\"\n",
    "    \n",
    "    # Check if it's a symbolic link (skip them)\n",
    "    if file_path.is_symlink():\n",
    "        return False, \"Symbolic link (skipped)\"\n",
    "    \n",
    "    # Check file permissions\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            pass  # Just try to open\n",
    "    except PermissionError:\n",
    "        return False, \"Permission denied\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Cannot access file: {e}\"\n",
    "    \n",
    "    # Check file size\n",
    "    if not validate_file_size(file_path):\n",
    "        return False, \"File too large (>2MB)\"\n",
    "    \n",
    "    # Check if binary\n",
    "    if is_binary_file(file_path):\n",
    "        return False, \"Binary file\"\n",
    "    \n",
    "    # Check if empty or whitespace only\n",
    "    if is_empty_or_whitespace(file_path):\n",
    "        return False, \"Empty or whitespace-only file\"\n",
    "    \n",
    "    return True, \"Valid\"\n",
    "\n",
    "print(\"âœ… File validation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea19d5",
   "metadata": {},
   "source": [
    "File Exclusion Logic (Markdown)\n",
    "This cell implements the exclusion logic that determines whether a discovered file should be skipped based on various patterns and rules.\n",
    "Exclusion Sources:\n",
    "\n",
    ".gitignore patterns: Loaded from project root\n",
    "Additional exclusions: User-specified patterns from configuration\n",
    "Extension filtering: Only process supported file extensions\n",
    "Special directories: Always exclude node_modules at any level\n",
    "\n",
    "Pattern Matching:\n",
    "\n",
    "Uses pathspec library for gitignore-compatible pattern matching\n",
    "Supports glob patterns, directory patterns, and negation patterns\n",
    "Applies patterns relative to project root for consistency\n",
    "\n",
    "Performance Optimization:\n",
    "\n",
    "Early exclusion checks to avoid unnecessary file operations\n",
    "Efficient pattern matching using compiled PathSpec objects\n",
    "Directory-level exclusion to skip entire subtrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cb09eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… File exclusion logic defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FILE EXCLUSION LOGIC\n",
    "# =============================================================================\n",
    "\n",
    "def is_file_excluded(file_path: Path, \n",
    "                    gitignore_spec: Optional[pathspec.PathSpec],\n",
    "                    additional_spec: pathspec.PathSpec,\n",
    "                    project_root: Path) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Check if file should be excluded from processing.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file\n",
    "        gitignore_spec: .gitignore PathSpec object\n",
    "        additional_spec: Additional exclusions PathSpec object\n",
    "        project_root: Project root directory\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_excluded: bool, reason: str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get relative path from project root\n",
    "        rel_path = file_path.relative_to(project_root)\n",
    "        rel_path_str = str(rel_path).replace('\\\\', '/')  # Use forward slashes\n",
    "        \n",
    "        # Check file extension first (most efficient)\n",
    "        if file_path.suffix.lower() not in SUPPORTED_EXTENSIONS:\n",
    "            return True, f\"Unsupported extension: {file_path.suffix}\"\n",
    "        \n",
    "        # Check additional exclusions (includes node_modules)\n",
    "        if additional_spec.match_file(rel_path_str):\n",
    "            return True, \"Matches additional exclusion pattern\"\n",
    "        \n",
    "        # Check if any parent directory is node_modules\n",
    "        for parent in file_path.parents:\n",
    "            if parent.name == 'node_modules':\n",
    "                return True, \"Inside node_modules directory\"\n",
    "        \n",
    "        # Check .gitignore patterns\n",
    "        if gitignore_spec and gitignore_spec.match_file(rel_path_str):\n",
    "            return True, \"Matches .gitignore pattern\"\n",
    "        \n",
    "        return False, \"Not excluded\"\n",
    "        \n",
    "    except ValueError:\n",
    "        # File is not under project root\n",
    "        return True, \"File not under project root\"\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"âš ï¸ Error checking exclusion for {file_path}: {e}\")\n",
    "        return True, f\"Error during exclusion check: {e}\"\n",
    "\n",
    "def is_directory_excluded(dir_path: Path, \n",
    "                         gitignore_spec: Optional[pathspec.PathSpec],\n",
    "                         additional_spec: pathspec.PathSpec,\n",
    "                         project_root: Path) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Check if entire directory should be excluded from traversal.\n",
    "    \n",
    "    Args:\n",
    "        dir_path: Path to the directory\n",
    "        gitignore_spec: .gitignore PathSpec object\n",
    "        additional_spec: Additional exclusions PathSpec object\n",
    "        project_root: Project root directory\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_excluded: bool, reason: str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Always exclude node_modules at any level\n",
    "        if dir_path.name == 'node_modules':\n",
    "            return True, \"node_modules directory\"\n",
    "        \n",
    "        # Get relative path from project root\n",
    "        rel_path = dir_path.relative_to(project_root)\n",
    "        rel_path_str = str(rel_path).replace('\\\\', '/')  # Use forward slashes\n",
    "        \n",
    "        # Check additional exclusions\n",
    "        if additional_spec.match_file(rel_path_str):\n",
    "            return True, \"Matches additional exclusion pattern\"\n",
    "        \n",
    "        # Check .gitignore patterns\n",
    "        if gitignore_spec and gitignore_spec.match_file(rel_path_str):\n",
    "            return True, \"Matches .gitignore pattern\"\n",
    "        \n",
    "        return False, \"Not excluded\"\n",
    "        \n",
    "    except ValueError:\n",
    "        # Directory is not under project root\n",
    "        return True, \"Directory not under project root\"\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"âš ï¸ Error checking directory exclusion for {dir_path}: {e}\")\n",
    "        return True, f\"Error during exclusion check: {e}\"\n",
    "\n",
    "print(\"âœ… File exclusion logic defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d29852",
   "metadata": {},
   "source": [
    "File Metadata Extraction (Markdown)\n",
    "This cell implements functions to extract metadata from discovered files. The metadata is essential for the chunking process and provides context about each file.\n",
    "Extracted Metadata:\n",
    "\n",
    "File Path Information: Absolute and relative paths\n",
    "File Properties: Extension, size in bytes, modification time\n",
    "Line Counting: Total lines and import line estimation\n",
    "Project Context: Relationship to package.json files\n",
    "\n",
    "Import Line Detection:\n",
    "\n",
    "Simple regex-based detection for common import patterns\n",
    "Handles ES6 imports, CommonJS requires, CSS imports\n",
    "Provides rough estimate for import chunking decisions\n",
    "\n",
    "Performance Considerations:\n",
    "\n",
    "Efficient line counting without loading entire files into memory\n",
    "Early termination for import counting after reasonable threshold\n",
    "Minimal file I/O operations to maintain speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6da16363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… File metadata extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FILE METADATA EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def count_file_lines(file_path: Path) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Count total lines and estimate import lines in a file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (total_lines: int, import_lines: int)\n",
    "    \"\"\"\n",
    "    # Import patterns for different file types\n",
    "    import_patterns = [\n",
    "        r'^\\s*import\\s+',           # ES6 imports\n",
    "        r'^\\s*from\\s+[\\'\"][^\\'\"]',  # ES6 from imports  \n",
    "        r'^\\s*const\\s+.*=\\s*require\\(',  # CommonJS require\n",
    "        r'^\\s*let\\s+.*=\\s*require\\(',    # CommonJS require\n",
    "        r'^\\s*var\\s+.*=\\s*require\\(',    # CommonJS require\n",
    "        r'^\\s*@import\\s+',          # CSS imports\n",
    "        r'^\\s*@use\\s+',             # SCSS use\n",
    "        r'^\\s*export\\s+.*from\\s+',  # Re-exports\n",
    "    ]\n",
    "    \n",
    "    compiled_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in import_patterns]\n",
    "    \n",
    "    try:\n",
    "        total_lines = 0\n",
    "        import_lines = 0\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                total_lines += 1\n",
    "                \n",
    "                # Check if line matches import patterns\n",
    "                line_stripped = line.strip()\n",
    "                if line_stripped and not line_stripped.startswith('//') and not line_stripped.startswith('/*'):\n",
    "                    for pattern in compiled_patterns:\n",
    "                        if pattern.match(line_stripped):\n",
    "                            import_lines += 1\n",
    "                            break\n",
    "        \n",
    "        return total_lines, import_lines\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"âš ï¸ Error counting lines in {file_path}: {e}\")\n",
    "        return 0, 0\n",
    "\n",
    "def extract_file_metadata(file_path: Path, project_root: Path) -> FileMetadata:\n",
    "    \"\"\"\n",
    "    Extract comprehensive metadata from a file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file\n",
    "        project_root: Project root directory\n",
    "        \n",
    "    Returns:\n",
    "        FileMetadata object with extracted information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Basic file information\n",
    "        stat_info = file_path.stat()\n",
    "        size_bytes = stat_info.st_size\n",
    "        \n",
    "        # Relative path calculation\n",
    "        relative_path = str(file_path.relative_to(project_root)).replace('\\\\', '/')\n",
    "        \n",
    "        # Extension\n",
    "        extension = file_path.suffix.lower()\n",
    "        \n",
    "        # Line counting\n",
    "        total_lines, import_lines = count_file_lines(file_path)\n",
    "        \n",
    "        # Check if there's a package.json in the project root\n",
    "        package_json_exists = (project_root / 'package.json').exists()\n",
    "        \n",
    "        metadata = FileMetadata(\n",
    "            file_path=file_path,\n",
    "            relative_path=relative_path,\n",
    "            extension=extension,\n",
    "            size_bytes=size_bytes,\n",
    "            total_lines=total_lines,\n",
    "            import_lines=import_lines,\n",
    "            has_package_json=package_json_exists\n",
    "        )\n",
    "        \n",
    "        logger.debug(f\"ðŸ“Š Extracted metadata for {relative_path}: {total_lines} lines, {import_lines} imports\")\n",
    "        return metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Error extracting metadata for {file_path}: {e}\")\n",
    "        # Return minimal metadata on error\n",
    "        return FileMetadata(\n",
    "            file_path=file_path,\n",
    "            relative_path=str(file_path.name),\n",
    "            extension=file_path.suffix.lower(),\n",
    "            size_bytes=0,\n",
    "            total_lines=0,\n",
    "            import_lines=0,\n",
    "            has_package_json=False\n",
    "        )\n",
    "\n",
    "print(\"âœ… File metadata extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8876a0",
   "metadata": {},
   "source": [
    "Main File Discovery Function (Markdown)\n",
    "This cell implements the main file discovery function that orchestrates the entire discovery process. This is the primary entry point for Phase 2.\n",
    "Process Flow:\n",
    "\n",
    "Initialize Patterns: Load .gitignore and create additional exclusion patterns\n",
    "Recursive Traversal: Walk through all directories and subdirectories\n",
    "Progressive Filtering: Apply exclusions at directory and file levels\n",
    "Validation Pipeline: Check file accessibility, size, and content type\n",
    "Metadata Generation: Extract comprehensive metadata for valid files\n",
    "Progress Reporting: Provide real-time feedback for large codebases\n",
    "\n",
    "Performance Features:\n",
    "\n",
    "Early Directory Exclusion: Skip entire subtrees when directories are excluded\n",
    "Batch Progress Updates: Update progress periodically to avoid log spam\n",
    "Memory Efficient: Process files one at a time without loading all into memory\n",
    "Error Recovery: Continue processing even when individual files fail\n",
    "\n",
    "Return Value:\n",
    "\n",
    "List of FileMetadata objects for all processable files\n",
    "Updated ProcessingStats with discovery metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b238412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Main file discovery function defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN FILE DISCOVERY FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def discover_files(target_directory: Path,\n",
    "                  supported_extensions: List[str],\n",
    "                  additional_exclusions: List[str],\n",
    "                  stats: ProcessingStats) -> List[FileMetadata]:\n",
    "    \"\"\"\n",
    "    Discover all processable files in target directory with comprehensive filtering.\n",
    "    \n",
    "    Args:\n",
    "        target_directory: Directory to scan recursively\n",
    "        supported_extensions: List of supported file extensions\n",
    "        additional_exclusions: Additional files/patterns to exclude\n",
    "        stats: ProcessingStats object to update\n",
    "        \n",
    "    Returns:\n",
    "        List of FileMetadata objects for discovered files\n",
    "    \"\"\"\n",
    "    logger.info(f\"ðŸ” Starting file discovery in: {target_directory}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Validate target directory\n",
    "    if not target_directory.exists():\n",
    "        logger.error(f\"âŒ Target directory does not exist: {target_directory}\")\n",
    "        return []\n",
    "    \n",
    "    if not target_directory.is_dir():\n",
    "        logger.error(f\"âŒ Target path is not a directory: {target_directory}\")\n",
    "        return []\n",
    "    \n",
    "    # Load exclusion patterns\n",
    "    logger.info(\"ðŸ“‹ Loading exclusion patterns...\")\n",
    "    gitignore_spec = load_gitignore_patterns(target_directory)\n",
    "    additional_spec = create_additional_exclusion_spec(additional_exclusions)\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    discovered_files = []\n",
    "    total_files_found = 0\n",
    "    excluded_files = 0\n",
    "    invalid_files = 0\n",
    "    last_progress_update = 0\n",
    "    \n",
    "    # Statistics tracking\n",
    "    exclusion_reasons = {}\n",
    "    validation_reasons = {}\n",
    "    \n",
    "    logger.info(\"ðŸš€ Beginning recursive file discovery...\")\n",
    "    \n",
    "    try:\n",
    "        # Walk through directory tree\n",
    "        for root_path in target_directory.rglob('*'):\n",
    "            \n",
    "            # Progress reporting (every 100 files)\n",
    "            total_files_found += 1\n",
    "            if total_files_found - last_progress_update >= 100:\n",
    "                logger.info(f\"ðŸ“Š Progress: {total_files_found} items scanned, {len(discovered_files)} valid files found\")\n",
    "                last_progress_update = total_files_found\n",
    "            \n",
    "            # Skip if it's a directory\n",
    "            if root_path.is_dir():\n",
    "                # Check if directory should be excluded (for optimization)\n",
    "                is_excluded, reason = is_directory_excluded(\n",
    "                    root_path, gitignore_spec, additional_spec, target_directory\n",
    "                )\n",
    "                if is_excluded:\n",
    "                    logger.debug(f\"ðŸ“ Excluding directory: {root_path.relative_to(target_directory)} ({reason})\")\n",
    "                continue\n",
    "            \n",
    "            # Only process files\n",
    "            if not root_path.is_file():\n",
    "                continue\n",
    "            \n",
    "            # Check file exclusions\n",
    "            is_excluded, exclusion_reason = is_file_excluded(\n",
    "                root_path, gitignore_spec, additional_spec, target_directory\n",
    "            )\n",
    "            \n",
    "            if is_excluded:\n",
    "                excluded_files += 1\n",
    "                exclusion_reasons[exclusion_reason] = exclusion_reasons.get(exclusion_reason, 0) + 1\n",
    "                logger.debug(f\"ðŸš« Excluded: {root_path.relative_to(target_directory)} ({exclusion_reason})\")\n",
    "                continue\n",
    "            \n",
    "            # Validate file is processable\n",
    "            is_valid, validation_reason = validate_file_processable(root_path)\n",
    "            \n",
    "            if not is_valid:\n",
    "                invalid_files += 1\n",
    "                validation_reasons[validation_reason] = validation_reasons.get(validation_reason, 0) + 1\n",
    "                logger.debug(f\"âŒ Invalid: {root_path.relative_to(target_directory)} ({validation_reason})\")\n",
    "                continue\n",
    "            \n",
    "            # Extract metadata for valid files\n",
    "            try:\n",
    "                metadata = extract_file_metadata(root_path, target_directory)\n",
    "                discovered_files.append(metadata)\n",
    "                logger.debug(f\"âœ… Added: {metadata.relative_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                invalid_files += 1\n",
    "                logger.error(f\"âŒ Error processing {root_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Error during file discovery: {e}\")\n",
    "        return discovered_files\n",
    "    \n",
    "    # Final statistics\n",
    "    discovery_time = time.time() - start_time\n",
    "    \n",
    "    # Update processing stats\n",
    "    stats.total_files = len(discovered_files)\n",
    "    \n",
    "    # Log comprehensive summary\n",
    "    logger.info(f\"\\n\" + \"=\"*60)\n",
    "    logger.info(f\"ðŸ“Š FILE DISCOVERY SUMMARY\")\n",
    "    logger.info(f\"=\"*60)\n",
    "    logger.info(f\"ðŸ“ Target Directory: {target_directory}\")\n",
    "    logger.info(f\"â±ï¸ Discovery Time: {discovery_time:.2f} seconds\")\n",
    "    logger.info(f\"ðŸ“„ Total Items Scanned: {total_files_found}\")\n",
    "    logger.info(f\"âœ… Valid Files Found: {len(discovered_files)}\")\n",
    "    logger.info(f\"ðŸš« Files Excluded: {excluded_files}\")\n",
    "    logger.info(f\"âŒ Invalid Files: {invalid_files}\")\n",
    "    \n",
    "    # Log exclusion reasons\n",
    "    if exclusion_reasons:\n",
    "        logger.info(f\"\\nðŸ“‹ EXCLUSION BREAKDOWN:\")\n",
    "        for reason, count in sorted(exclusion_reasons.items(), key=lambda x: x[1], reverse=True):\n",
    "            logger.info(f\"   â€¢ {reason}: {count} files\")\n",
    "    \n",
    "    # Log validation reasons\n",
    "    if validation_reasons:\n",
    "        logger.info(f\"\\nâŒ VALIDATION FAILURE BREAKDOWN:\")\n",
    "        for reason, count in sorted(validation_reasons.items(), key=lambda x: x[1], reverse=True):\n",
    "            logger.info(f\"   â€¢ {reason}: {count} files\")\n",
    "    \n",
    "    # Log file type breakdown\n",
    "    if discovered_files:\n",
    "        extension_counts = {}\n",
    "        total_size_mb = 0\n",
    "        total_lines = 0\n",
    "        \n",
    "        for metadata in discovered_files:\n",
    "            ext = metadata.extension\n",
    "            extension_counts[ext] = extension_counts.get(ext, 0) + 1\n",
    "            total_size_mb += metadata.size_bytes / (1024 * 1024)\n",
    "            total_lines += metadata.total_lines\n",
    "        \n",
    "        logger.info(f\"\\nðŸ“ FILE TYPE BREAKDOWN:\")\n",
    "        for ext, count in sorted(extension_counts.items()):\n",
    "            logger.info(f\"   â€¢ {ext}: {count} files\")\n",
    "        \n",
    "        logger.info(f\"\\nðŸ“Š CONTENT STATISTICS:\")\n",
    "        logger.info(f\"   â€¢ Total Size: {total_size_mb:.2f} MB\")\n",
    "        logger.info(f\"   â€¢ Total Lines: {total_lines:,}\")\n",
    "        logger.info(f\"   â€¢ Average File Size: {(total_size_mb * 1024) / len(discovered_files):.1f} KB\")\n",
    "        logger.info(f\"   â€¢ Average Lines per File: {total_lines / len(discovered_files):.0f}\")\n",
    "    \n",
    "    logger.info(f\"=\"*60)\n",
    "    logger.info(f\"ðŸŽ‰ File discovery complete! Found {len(discovered_files)} processable files\")\n",
    "    \n",
    "    return discovered_files\n",
    "\n",
    "print(\"âœ… Main file discovery function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e6034",
   "metadata": {},
   "source": [
    "Package.json Discovery and Analysis (Markdown)\n",
    "This cell implements the package.json discovery and dependency analysis system. Since the requirements specify handling nested package.json files differently, this system will:\n",
    "Package.json Discovery:\n",
    "\n",
    "Primary: Look for package.json in the project root (highest priority)\n",
    "Secondary: Discover package.json files in subdirectories\n",
    "Hierarchy: Maintain parent-child relationships between package.json files\n",
    "\n",
    "Dependency Extraction:\n",
    "\n",
    "Extract both dependencies and devDependencies\n",
    "Create lookup tables for import classification\n",
    "Handle nested dependencies with inheritance rules\n",
    "\n",
    "Multi-package Support:\n",
    "\n",
    "Monorepo support with multiple package.json files\n",
    "Workspace-aware dependency resolution\n",
    "Fallback to root-level dependencies when local ones are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ce0181b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Package.json discovery and analysis functions defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PACKAGE.JSON DISCOVERY AND ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class PackageInfo:\n",
    "    \"\"\"Information about a package.json file\"\"\"\n",
    "    path: Path\n",
    "    relative_path: str\n",
    "    dependencies: List[str] = field(default_factory=list)\n",
    "    dev_dependencies: List[str] = field(default_factory=list)\n",
    "    all_dependencies: set = field(default_factory=set)\n",
    "    is_root: bool = False\n",
    "\n",
    "def find_all_package_json_files(project_root: Path) -> List[PackageInfo]:\n",
    "    \"\"\"\n",
    "    Find all package.json files in the project with hierarchy information.\n",
    "    \n",
    "    Args:\n",
    "        project_root: Root directory to search in\n",
    "        \n",
    "    Returns:\n",
    "        List of PackageInfo objects sorted by hierarchy (root first)\n",
    "    \"\"\"\n",
    "    package_files = []\n",
    "    \n",
    "    logger.info(\"ðŸ“¦ Discovering package.json files...\")\n",
    "    \n",
    "    try:\n",
    "        # Search for all package.json files\n",
    "        for package_path in project_root.rglob('package.json'):\n",
    "            # Skip node_modules directories\n",
    "            if 'node_modules' in package_path.parts:\n",
    "                continue\n",
    "            \n",
    "            relative_path = str(package_path.relative_to(project_root)).replace('\\\\', '/')\n",
    "            is_root = package_path.parent == project_root\n",
    "            \n",
    "            package_info = PackageInfo(\n",
    "                path=package_path,\n",
    "                relative_path=relative_path,\n",
    "                is_root=is_root\n",
    "            )\n",
    "            \n",
    "            package_files.append(package_info)\n",
    "            logger.debug(f\"ðŸ“¦ Found package.json: {relative_path} (root: {is_root})\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Error discovering package.json files: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Sort by hierarchy: root first, then by depth\n",
    "    package_files.sort(key=lambda p: (not p.is_root, len(p.path.parts)))\n",
    "    \n",
    "    logger.info(f\"ðŸ“¦ Found {len(package_files)} package.json files\")\n",
    "    return package_files\n",
    "\n",
    "def extract_dependencies_from_package(package_path: Path) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Extract dependencies and devDependencies from a package.json file.\n",
    "    \n",
    "    Args:\n",
    "        package_path: Path to package.json file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (dependencies: List[str], devDependencies: List[str])\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(package_path, 'r', encoding='utf-8') as f:\n",
    "            package_data = json.load(f)\n",
    "        \n",
    "        # Extract dependencies\n",
    "        dependencies = list(package_data.get('dependencies', {}).keys())\n",
    "        dev_dependencies = list(package_data.get('devDependencies', {}).keys())\n",
    "        \n",
    "        logger.debug(f\"ðŸ“¦ {package_path.name}: {len(dependencies)} deps, {len(dev_dependencies)} devDeps\")\n",
    "        \n",
    "        return dependencies, dev_dependencies\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"âŒ Invalid JSON in {package_path}: {e}\")\n",
    "        return [], []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Error reading {package_path}: {e}\")\n",
    "        return [], []\n",
    "\n",
    "def analyze_all_package_files(package_files: List[PackageInfo]) -> Dict[str, set]:\n",
    "    \"\"\"\n",
    "    Analyze all package.json files and create dependency lookup.\n",
    "    \n",
    "    Args:\n",
    "        package_files: List of PackageInfo objects\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with combined dependency information\n",
    "    \"\"\"\n",
    "    logger.info(\"ðŸ“¦ Analyzing package.json dependencies...\")\n",
    "    \n",
    "    # Extract dependencies from each package file\n",
    "    for package_info in package_files:\n",
    "        deps, dev_deps = extract_dependencies_from_package(package_info.path)\n",
    "        package_info.dependencies = deps\n",
    "        package_info.dev_dependencies = dev_deps\n",
    "        package_info.all_dependencies = set(deps + dev_deps)\n",
    "    \n",
    "    # Create combined lookup tables\n",
    "    root_dependencies = set()\n",
    "    all_dependencies = set()\n",
    "    workspace_dependencies = {}\n",
    "    \n",
    "    for package_info in package_files:\n",
    "        if package_info.is_root:\n",
    "            root_dependencies = package_info.all_dependencies.copy()\n",
    "        \n",
    "        all_dependencies.update(package_info.all_dependencies)\n",
    "        \n",
    "        # Create workspace-specific lookup\n",
    "        workspace_dir = str(package_info.path.parent.relative_to(package_files[0].path.parent)).replace('\\\\', '/')\n",
    "        workspace_dependencies[workspace_dir] = package_info.all_dependencies\n",
    "    \n",
    "    # Log results\n",
    "    logger.info(f\"ðŸ“¦ Dependency analysis complete:\")\n",
    "    logger.info(f\"   â€¢Root dependencies: {len(root_dependencies)}\")\n",
    "    logger.info(f\"   â€¢ Total unique dependencies: {len(all_dependencies)}\")\n",
    "    logger.info(f\"   â€¢ Workspaces with dependencies: {len(workspace_dependencies)}\")\n",
    "    return {\n",
    "        'root': root_dependencies,\n",
    "        'all': all_dependencies,\n",
    "        'workspaces': workspace_dependencies\n",
    "    }\n",
    "\n",
    "def create_dependency_lookup(dependency_info: Dict[str, set]) -> set:\n",
    "    \"\"\"\n",
    "    Create optimized lookup set for external dependency classification.\n",
    "    Args:\n",
    "        dependency_info: Dictionary of dependency information\n",
    "        \n",
    "    Returns:\n",
    "        Set of external package names for quick lookup\n",
    "    \"\"\"\n",
    "    # Use all dependencies for comprehensive lookup\n",
    "    external_deps = dependency_info.get('all', set())\n",
    "\n",
    "    # Add common external packages that might not be in package.json\n",
    "    common_external = {\n",
    "        'react', 'react-dom', 'vue', 'angular', 'jquery', 'lodash', 'axios',\n",
    "        'express', 'next', 'nuxt', 'typescript', 'babel', 'webpack', 'vite'\n",
    "    }\n",
    "\n",
    "    external_deps.update(common_external)\n",
    "\n",
    "    logger.info(f\"ðŸŽ¯ Created dependency lookup with {len(external_deps)} packages\")\n",
    "    return external_deps\n",
    "print(\"âœ… Package.json discovery and analysis functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9939a2f",
   "metadata": {},
   "source": [
    "Phase 2 Pipeline Orchestrator (Markdown)\n",
    "\n",
    "This cell implements the main pipeline orchestrator for Phase 2 that coordinates all file discovery and filtering operations. This function serves as the primary entry point and integrates all the components defined in previous cells.\n",
    "\n",
    "**Pipeline Stages:**\n",
    "1. **Setup and Validation**: Validate target directory and configuration\n",
    "2. **Pattern Loading**: Load .gitignore and additional exclusion patterns\n",
    "3. **File Discovery**: Recursive discovery with filtering and validation\n",
    "4. **Package Analysis**: Discover and analyze package.json files\n",
    "5. **Dependency Lookup**: Create optimized dependency classification system\n",
    "6. **Results Compilation**: Compile and return comprehensive results\n",
    "\n",
    "**Error Handling:**\n",
    "- Graceful handling of missing directories or permission issues\n",
    "- Detailed error logging with context\n",
    "- Partial results return even if some operations fail\n",
    "- Recovery strategies for common failure scenarios\n",
    "\n",
    "**Integration Points:**\n",
    "- Updates global `ProcessingStats` object from Phase 1\n",
    "- Returns data structures needed for Phase 3 (AST Analysis)\n",
    "- Maintains consistency with Phase 1 configuration and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93a543c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Phase 2 pipeline orchestrator defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2 PIPELINE ORCHESTRATOR\n",
    "# =============================================================================\n",
    "\n",
    "def run_file_discovery_pipeline(target_directory: Path,\n",
    "                               supported_extensions: List[str],\n",
    "                               additional_exclusions: List[str],\n",
    "                               stats: ProcessingStats) -> Tuple[List[FileMetadata], set]:\n",
    "    \"\"\"\n",
    "    Execute the complete file discovery and filtering pipeline.\n",
    "    \n",
    "    Args:\n",
    "        target_directory: Directory to scan for files\n",
    "        supported_extensions: List of file extensions to process\n",
    "        additional_exclusions: Additional exclusion patterns\n",
    "        stats: ProcessingStats object to update\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (discovered_files: List[FileMetadata], external_deps: set)\n",
    "    \"\"\"\n",
    "    logger.info(\"ðŸš€ Starting Phase 2: File Discovery and Filtering\")\n",
    "    phase_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: Validate target directory\n",
    "        logger.info(\"ðŸ” Stage 1: Validating target directory\")\n",
    "        if not target_directory.exists():\n",
    "            logger.error(f\"âŒ Target directory does not exist: {target_directory}\")\n",
    "            return [], set()\n",
    "        \n",
    "        if not target_directory.is_dir():\n",
    "            logger.error(f\"âŒ Target path is not a directory: {target_directory}\")\n",
    "            return [], set()\n",
    "        \n",
    "        logger.info(f\"âœ… Target directory validated: {target_directory}\")\n",
    "        \n",
    "        # Stage 2: Discover and analyze package.json files\n",
    "        logger.info(\"ðŸ” Stage 2: Package.json discovery and analysis\")\n",
    "        package_files = find_all_package_json_files(target_directory)\n",
    "        \n",
    "        if package_files:\n",
    "            dependency_info = analyze_all_package_files(package_files)\n",
    "            external_deps = create_dependency_lookup(dependency_info)\n",
    "            \n",
    "            # Log package.json summary\n",
    "            root_packages = [p for p in package_files if p.is_root]\n",
    "            workspace_packages = [p for p in package_files if not p.is_root]\n",
    "            \n",
    "            logger.info(f\"ðŸ“¦ Package.json summary:\")\n",
    "            logger.info(f\"   â€¢ Root package.json: {'Yes' if root_packages else 'No'}\")\n",
    "            logger.info(f\"   â€¢ Workspace packages: {len(workspace_packages)}\")\n",
    "            logger.info(f\"   â€¢ Total external dependencies: {len(external_deps)}\")\n",
    "            \n",
    "        else:\n",
    "            logger.warning(\"âš ï¸ No package.json files found - all imports will be treated as local\")\n",
    "            external_deps = set()\n",
    "        \n",
    "        # Stage 3: File discovery\n",
    "        logger.info(\"ðŸ” Stage 3: Recursive file discovery\")\n",
    "        discovered_files = discover_files(\n",
    "            target_directory=target_directory,\n",
    "            supported_extensions=supported_extensions,\n",
    "            additional_exclusions=additional_exclusions,\n",
    "            stats=stats\n",
    "        )\n",
    "        \n",
    "        # Stage 4: Final validation and statistics\n",
    "        logger.info(\"ðŸ” Stage 4: Final validation and statistics\")\n",
    "        \n",
    "        if not discovered_files:\n",
    "            logger.warning(\"âš ï¸ No processable files found in target directory\")\n",
    "            return [], external_deps\n",
    "        \n",
    "        # Calculate additional statistics\n",
    "        files_with_imports = sum(1 for f in discovered_files if f.import_lines > 0)\n",
    "        large_files = sum(1 for f in discovered_files if f.size_bytes > 1024 * 1024)  # > 1MB\n",
    "        \n",
    "        # Update stats\n",
    "        stats.total_files = len(discovered_files)\n",
    "        \n",
    "        # Phase completion summary\n",
    "        phase_duration = time.time() - phase_start_time\n",
    "        \n",
    "        logger.info(f\"\\n\" + \"=\"*60)\n",
    "        logger.info(f\"ðŸŽ‰ PHASE 2 COMPLETE - FILE DISCOVERY SUCCESS\")\n",
    "        logger.info(f\"=\"*60)\n",
    "        logger.info(f\"â±ï¸ Phase Duration: {phase_duration:.2f} seconds\")\n",
    "        logger.info(f\"ðŸ“ Target Directory: {target_directory}\")\n",
    "        logger.info(f\"âœ… Files Discovered: {len(discovered_files)}\")\n",
    "        logger.info(f\"ðŸ“¦ Package.json Files: {len(package_files) if package_files else 0}\")\n",
    "        logger.info(f\"ðŸ”— External Dependencies: {len(external_deps)}\")\n",
    "        logger.info(f\"ðŸ“ Files with Imports: {files_with_imports}\")\n",
    "        logger.info(f\"ðŸ“Š Large Files (>1MB): {large_files}\")\n",
    "        \n",
    "        # File type summary\n",
    "        ext_summary = {}\n",
    "        for file_meta in discovered_files:\n",
    "            ext = file_meta.extension\n",
    "            ext_summary[ext] = ext_summary.get(ext, 0) + 1\n",
    "        \n",
    "        logger.info(f\"ðŸ“ File Types:\")\n",
    "        for ext, count in sorted(ext_summary.items()):\n",
    "            logger.info(f\"   â€¢ {ext}: {count} files\")\n",
    "        \n",
    "        logger.info(f\"=\"*60)\n",
    "        logger.info(f\"âž¡ï¸ Ready for Phase 3: AST Analysis and Structure Extraction\")\n",
    "        \n",
    "        return discovered_files, external_deps\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Phase 2 failed with error: {e}\")\n",
    "        logger.error(f\"ðŸ“Š Partial results: {len(discovered_files) if 'discovered_files' in locals() else 0} files\")\n",
    "        \n",
    "        # Return partial results if available\n",
    "        partial_files = locals().get('discovered_files', [])\n",
    "        partial_deps = locals().get('external_deps', set())\n",
    "        \n",
    "        return partial_files, partial_deps\n",
    "\n",
    "print(\"âœ… Phase 2 pipeline orchestrator defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596af1d8",
   "metadata": {},
   "source": [
    "Phase 2 Execution and Results (Markdown)\n",
    "This final cell executes the Phase 2 pipeline using the configuration from Phase 1 and displays the results. It serves as the integration point between Phase 1 configuration and Phase 2 execution.\n",
    "Execution Flow:\n",
    "\n",
    "Load Configuration: Use global variables from Phase 1\n",
    "Execute Pipeline: Run the complete file discovery pipeline\n",
    "Store Results: Save results in global variables for Phase 3\n",
    "Display Summary: Show comprehensive results and next steps\n",
    "Validation: Ensure results are ready for Phase 3 processing\n",
    "\n",
    "Global Variables Created:\n",
    "\n",
    "discovered_files: List of FileMetadata objects for Phase 3\n",
    "external_dependencies: Set of external package names for import classification\n",
    "phase2_stats: Updated ProcessingStats with discovery metrics\n",
    "\n",
    "Error Handling:\n",
    "\n",
    "Graceful handling of pipeline failures\n",
    "Clear error messages with troubleshooting suggestions\n",
    "Partial results preservation when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f82593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 20:04:55,239 - INFO - ðŸŽ¬ Executing Phase 2: File Discovery and Filtering\n",
      "2025-08-02 20:04:55,245 - INFO - ðŸ“ Using configuration from Phase 1:\n",
      "2025-08-02 20:04:55,247 - INFO -    â€¢ Target: /Users/tiyadiashok/python-projects/code_chunker/rag_sources/code_sources/typescript\n",
      "2025-08-02 20:04:55,249 - INFO -    â€¢ Output: /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/code_sources/typescript\n",
      "2025-08-02 20:04:55,250 - INFO -    â€¢ Extensions: ['.tsx', '.ts', '.js', '.jsx', '.html', '.css', '.scss']\n",
      "2025-08-02 20:04:55,250 - INFO -    â€¢ Exclusions: 10 patterns\n",
      "2025-08-02 20:04:55,251 - INFO - ðŸš€ Starting Phase 2: File Discovery and Filtering\n",
      "2025-08-02 20:04:55,252 - INFO - ðŸ” Stage 1: Validating target directory\n",
      "2025-08-02 20:04:55,253 - INFO - âœ… Target directory validated: /Users/tiyadiashok/python-projects/code_chunker/rag_sources/code_sources/typescript\n",
      "2025-08-02 20:04:55,254 - INFO - ðŸ” Stage 2: Package.json discovery and analysis\n",
      "2025-08-02 20:04:55,255 - INFO - ðŸ“¦ Discovering package.json files...\n",
      "2025-08-02 20:04:55,263 - INFO - ðŸ“¦ Found 0 package.json files\n",
      "2025-08-02 20:04:55,264 - WARNING - âš ï¸ No package.json files found - all imports will be treated as local\n",
      "2025-08-02 20:04:55,264 - INFO - ðŸ” Stage 3: Recursive file discovery\n",
      "2025-08-02 20:04:55,264 - INFO - ðŸ” Starting file discovery in: /Users/tiyadiashok/python-projects/code_chunker/rag_sources/code_sources/typescript\n",
      "2025-08-02 20:04:55,265 - INFO - ðŸ“‹ Loading exclusion patterns...\n",
      "2025-08-02 20:04:55,265 - INFO - ðŸ“‹ No .gitignore file found in project root\n",
      "2025-08-02 20:04:55,266 - INFO - ðŸ“‹ Created 12 additional exclusion patterns\n",
      "2025-08-02 20:04:55,272 - INFO - ðŸš€ Beginning recursive file discovery...\n",
      "2025-08-02 20:04:55,272 - INFO - \n",
      "============================================================\n",
      "2025-08-02 20:04:55,272 - INFO - ðŸ“Š FILE DISCOVERY SUMMARY\n",
      "2025-08-02 20:04:55,273 - INFO - ============================================================\n",
      "2025-08-02 20:04:55,273 - INFO - ðŸ“ Target Directory: /Users/tiyadiashok/python-projects/code_chunker/rag_sources/code_sources/typescript\n",
      "2025-08-02 20:04:55,273 - INFO - â±ï¸ Discovery Time: 0.01 seconds\n",
      "2025-08-02 20:04:55,274 - INFO - ðŸ“„ Total Items Scanned: 0\n",
      "2025-08-02 20:04:55,274 - INFO - âœ… Valid Files Found: 0\n",
      "2025-08-02 20:04:55,274 - INFO - ðŸš« Files Excluded: 0\n",
      "2025-08-02 20:04:55,274 - INFO - âŒ Invalid Files: 0\n",
      "2025-08-02 20:04:55,275 - INFO - ============================================================\n",
      "2025-08-02 20:04:55,276 - INFO - ðŸŽ‰ File discovery complete! Found 0 processable files\n",
      "2025-08-02 20:04:55,276 - INFO - ðŸ” Stage 4: Final validation and statistics\n",
      "2025-08-02 20:04:55,276 - WARNING - âš ï¸ No processable files found in target directory\n",
      "2025-08-02 20:04:55,276 - WARNING - âš ï¸ No files discovered - check target directory and exclusion patterns\n",
      "2025-08-02 20:04:55,277 - INFO - ðŸ”§ Phase 2 (File Discovery and Filtering) complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Phase 2 execution...\n",
      "âŒ Phase 2 execution failed - please check the error messages above\n",
      "ðŸ’¡ Troubleshooting suggestions:\n",
      "   â€¢ Verify TARGET_DIRECTORY exists and is accessible\n",
      "   â€¢ Check file permissions in target directory\n",
      "   â€¢ Review exclusion patterns for overly restrictive rules\n",
      "   â€¢ Ensure supported file extensions match your project\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2 EXECUTION AND RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "def execute_phase2():\n",
    "    \"\"\"Execute Phase 2 with configuration from Phase 1\"\"\"\n",
    "    global discovered_files, external_dependencies, phase2_stats\n",
    "    \n",
    "    logger.info(\"ðŸŽ¬ Executing Phase 2: File Discovery and Filtering\")\n",
    "    \n",
    "    # Initialize stats object if not already created\n",
    "    if 'phase2_stats' not in globals():\n",
    "        phase2_stats = ProcessingStats()\n",
    "    \n",
    "    # Validate configuration from Phase 1\n",
    "    target_path = Path(TARGET_DIRECTORY)\n",
    "    output_path = Path(OUTPUT_DIRECTORY)\n",
    "    \n",
    "    logger.info(f\"ðŸ“ Using configuration from Phase 1:\")\n",
    "    logger.info(f\"   â€¢ Target: {target_path}\")\n",
    "    logger.info(f\"   â€¢ Output: {output_path}\")\n",
    "    logger.info(f\"   â€¢ Extensions: {SUPPORTED_EXTENSIONS}\")\n",
    "    logger.info(f\"   â€¢ Exclusions: {len(ADDITIONAL_EXCLUSIONS)} patterns\")\n",
    "    \n",
    "    # Execute the pipeline\n",
    "    try:\n",
    "        files, deps = run_file_discovery_pipeline(\n",
    "            target_directory=target_path,\n",
    "            supported_extensions=SUPPORTED_EXTENSIONS,\n",
    "            additional_exclusions=ADDITIONAL_EXCLUSIONS,\n",
    "            stats=phase2_stats\n",
    "        )\n",
    "        \n",
    "        # Store results globally for Phase 3\n",
    "        discovered_files = files\n",
    "        external_dependencies = deps\n",
    "        \n",
    "        # Success summary\n",
    "        if discovered_files:\n",
    "            logger.info(f\"\\nðŸŽ¯ PHASE 2 RESULTS STORED:\")\n",
    "            logger.info(f\"   â€¢ discovered_files: {len(discovered_files)} files\")\n",
    "            logger.info(f\"   â€¢ external_dependencies: {len(external_dependencies)} packages\")\n",
    "            logger.info(f\"   â€¢ phase2_stats: Updated with discovery metrics\")\n",
    "            \n",
    "            # Sample files preview\n",
    "            logger.info(f\"\\nðŸ“ SAMPLE DISCOVERED FILES (first 5):\")\n",
    "            for i, file_meta in enumerate(discovered_files[:5]):\n",
    "                size_kb = file_meta.size_bytes / 1024\n",
    "                logger.info(f\"   {i+1}. {file_meta.relative_path} ({size_kb:.1f}KB, {file_meta.total_lines} lines)\")\n",
    "            \n",
    "            if len(discovered_files) > 5:\n",
    "                logger.info(f\"   ... and {len(discovered_files) - 5} more files\")\n",
    "            \n",
    "            # Sample dependencies preview\n",
    "            if external_dependencies:\n",
    "                logger.info(f\"\\nðŸ“¦ SAMPLE EXTERNAL DEPENDENCIES (first 10):\")\n",
    "                deps_list = sorted(list(external_dependencies))\n",
    "                for i, dep in enumerate(deps_list[:10]):\n",
    "                    logger.info(f\"   â€¢ {dep}\")\n",
    "                \n",
    "                if len(external_dependencies) > 10:\n",
    "                    logger.info(f\"   ... and {len(external_dependencies) - 10} more dependencies\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        else:\n",
    "            logger.warning(\"âš ï¸ No files discovered - check target directory and exclusion patterns\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Phase 2 execution failed: {e}\")\n",
    "        \n",
    "        # Initialize empty results on failure\n",
    "        discovered_files = []\n",
    "        external_dependencies = set()\n",
    "        \n",
    "        return False\n",
    "\n",
    "def validate_phase2_results():\n",
    "    \"\"\"Validate Phase 2 results are ready for Phase 3\"\"\"\n",
    "    logger.info(\"ðŸ” Validating Phase 2 results for Phase 3 readiness...\")\n",
    "    \n",
    "    validation_issues = []\n",
    "    \n",
    "    # Check discovered_files\n",
    "    if 'discovered_files' not in globals():\n",
    "        validation_issues.append(\"discovered_files variable not found\")\n",
    "    elif not isinstance(discovered_files, list):\n",
    "        validation_issues.append(\"discovered_files is not a list\")\n",
    "    elif len(discovered_files) == 0:\n",
    "        validation_issues.append(\"No files were discovered\")\n",
    "    \n",
    "    # Check external_dependencies\n",
    "    if 'external_dependencies' not in globals():\n",
    "        validation_issues.append(\"external_dependencies variable not found\")\n",
    "    elif not isinstance(external_dependencies, set):\n",
    "        validation_issues.append(\"external_dependencies is not a set\")\n",
    "    \n",
    "    # Check file metadata structure\n",
    "    if 'discovered_files' in globals() and discovered_files:\n",
    "        sample_file = discovered_files[0]\n",
    "        required_attrs = ['file_path', 'relative_path', 'extension', 'size_bytes', 'total_lines', 'import_lines']\n",
    "        for attr in required_attrs:\n",
    "            if not hasattr(sample_file, attr):\n",
    "                validation_issues.append(f\"FileMetadata missing attribute: {attr}\")\n",
    "    \n",
    "    # Report results\n",
    "    if validation_issues:\n",
    "        logger.error(\"âŒ Phase 2 validation failed:\")\n",
    "        for issue in validation_issues:\n",
    "            logger.error(f\"   â€¢ {issue}\")\n",
    "        return False\n",
    "    else:\n",
    "        logger.info(\"âœ… Phase 2 validation passed - ready for Phase 3\")\n",
    "        return True\n",
    "\n",
    "# Execute Phase 2\n",
    "print(\"ðŸš€ Starting Phase 2 execution...\")\n",
    "phase2_success = execute_phase2()\n",
    "\n",
    "if phase2_success:\n",
    "    print(\"âœ… Phase 2 completed successfully!\")\n",
    "    \n",
    "    # Validate results\n",
    "    validation_success = validate_phase2_results()\n",
    "    \n",
    "    if validation_success:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ðŸŽ‰ PHASE 2 COMPLETE - SYSTEM READY FOR PHASE 3\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"ðŸ“ Files Ready for Processing: {len(discovered_files)}\")\n",
    "        print(f\"ðŸ“¦ External Dependencies Identified: {len(external_dependencies)}\")\n",
    "        print(f\"â±ï¸ Total Discovery Time: {phase2_stats.processing_time:.2f}s\")\n",
    "        print(\"âž¡ï¸ Next Step: Execute Phase 3 - AST Analysis and Structure Extraction\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"âŒ Phase 2 validation failed - please check the issues above\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ Phase 2 execution failed - please check the error messages above\")\n",
    "    print(\"ðŸ’¡ Troubleshooting suggestions:\")\n",
    "    print(\"   â€¢ Verify TARGET_DIRECTORY exists and is accessible\")\n",
    "    print(\"   â€¢ Check file permissions in target directory\")\n",
    "    print(\"   â€¢ Review exclusion patterns for overly restrictive rules\")\n",
    "    print(\"   â€¢ Ensure supported file extensions match your project\")\n",
    "\n",
    "logger.info(\"ðŸ”§ Phase 2 (File Discovery and Filtering) complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a574b",
   "metadata": {},
   "source": [
    "This completes Phase 2: File Discovery and Filtering. The implementation provides:\n",
    "\n",
    "Complete .gitignore Integration - Parses and applies .gitignore patterns correctly\n",
    "Robust File Validation - Binary detection, size limits, accessibility checks\n",
    "Recursive Directory Traversal - Efficient scanning with early exclusion optimization\n",
    "Package.json Analysis - Comprehensive dependency extraction with nested package support\n",
    "Progress Reporting - Real-time feedback for large codebases\n",
    "Error Recovery - Graceful handling of failures with detailed logging\n",
    "Performance Optimization - Memory-efficient processing and batch operations\n",
    "Comprehensive Statistics - Detailed metrics and breakdowns for analysis\n",
    "\n",
    "The phase integrates seamlessly with Phase 1 configuration and prepares all necessary data structures for Phase 3 (AST Analysis and Structure Extraction). Each cell includes detailed markdown explanations and can be copied directly into your existing Jupyter notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
