{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405ef96b",
   "metadata": {},
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ff9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883da7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "RST (reStructuredText) AST Chunker\n",
    "\n",
    "This module implements semantic chunking for RST files using docutils for parsing.\n",
    "It follows the same patterns as the existing Python and TypeScript AST chunkers.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import secrets\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from docutils import core, nodes\n",
    "from docutils.frontend import OptionParser\n",
    "from docutils.utils import new_document\n",
    "from docutils.parsers.rst import Parser\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION AND CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "MAX_CHUNK_TOKENS = 1000\n",
    "TARGET_TOKENS = 600\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA STRUCTURES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class RSTChunk:\n",
    "    \"\"\"Represents a semantic chunk of RST content\"\"\"\n",
    "    start_line: int\n",
    "    end_line: int\n",
    "    content: str\n",
    "    node_type: str\n",
    "    name: str\n",
    "    depth: int\n",
    "    level: int = 0  # For section hierarchy (0=title, 1=section, 2=subsection, etc.)\n",
    "    token_count: int = 0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.token_count == 0:\n",
    "            self.token_count = count_tokens(self.content)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RSTReference:\n",
    "    \"\"\"Represents a reference/include in RST (similar to imports)\"\"\"\n",
    "    reference_type: str  # 'include', 'image', 'figure', 'literalinclude', etc.\n",
    "    target: str\n",
    "    line_number: int\n",
    "    directive: str\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TOKEN COUNTING\n",
    "# =============================================================================\n",
    "\n",
    "def count_tokens(content: str) -> int:\n",
    "    \"\"\"Count tokens using tiktoken for GPT-4\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "        return len(encoding.encode(content))\n",
    "    except Exception:\n",
    "        # Fallback: rough estimation (1 token ≈ 4 characters)\n",
    "        return len(content) // 4\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RST PARSING AND ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def remove_include_directives(rst_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove or comment out include directives to prevent file resolution errors.\n",
    "    This allows parsing to proceed without trying to resolve include files.\n",
    "    \"\"\"\n",
    "    lines = rst_content.split('\\n')\n",
    "    processed_lines = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        \n",
    "        # Check for include directive\n",
    "        if line.strip().startswith('.. include::'):\n",
    "            # Comment out the include directive\n",
    "            processed_lines.append(f\".. # INCLUDE DISABLED: {line.strip()}\")\n",
    "            i += 1\n",
    "            \n",
    "            # Also comment out any options that follow\n",
    "            while i < len(lines) and lines[i].startswith('   :'):\n",
    "                processed_lines.append(f\".. # INCLUDE OPTION: {lines[i].strip()}\")\n",
    "                i += 1\n",
    "        else:\n",
    "            processed_lines.append(line)\n",
    "            i += 1\n",
    "    \n",
    "    return '\\n'.join(processed_lines)\n",
    "\n",
    "\n",
    "def parse_rst_file(file_path: Path) -> Tuple[nodes.document, str]:\n",
    "    \"\"\"Parse RST file using docutils with proper working directory and include handling\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            rst_content = f.read()\n",
    "        \n",
    "        # Method 1: Try parsing in the file's directory context\n",
    "        original_cwd = Path.cwd()\n",
    "        file_dir = file_path.parent\n",
    "        \n",
    "        try:\n",
    "            # Change to file's directory so relative includes can be resolved\n",
    "            import os\n",
    "            os.chdir(file_dir)\n",
    "            \n",
    "            # Use the newer API (fixing deprecation warnings)\n",
    "            from docutils.frontend import get_default_settings\n",
    "            \n",
    "            settings = get_default_settings(Parser)\n",
    "            # Configure settings to be more permissive\n",
    "            settings.report_level = 4  # Only show errors, not warnings\n",
    "            settings.halt_level = 5    # Don't halt on warnings/errors\n",
    "            settings.warning_stream = None  # Suppress warning output\n",
    "            \n",
    "            # Use file path as source path for better error context\n",
    "            source_path = str(file_path)\n",
    "            document = new_document(source_path, settings=settings)\n",
    "            \n",
    "            parser = Parser()\n",
    "            parser.parse(rst_content, document)\n",
    "            \n",
    "            print(f\"✅ Successfully parsed {file_path.name} with includes\")\n",
    "            return document, rst_content\n",
    "            \n",
    "        except Exception as include_error:\n",
    "            print(f\"⚠️ Include resolution failed for {file_path.name}: {include_error}\")\n",
    "            \n",
    "            # Method 2: Try with includes disabled\n",
    "            try:\n",
    "                rst_content_no_includes = remove_include_directives(rst_content)\n",
    "                \n",
    "                settings = get_default_settings(Parser)\n",
    "                settings.report_level = 5  # Suppress all warnings\n",
    "                settings.halt_level = 5    # Don't halt on errors\n",
    "                settings.warning_stream = None\n",
    "                \n",
    "                document = new_document(str(file_path), settings=settings)\n",
    "                parser = Parser()\n",
    "                parser.parse(rst_content_no_includes, document)\n",
    "                \n",
    "                print(f\"✅ Successfully parsed {file_path.name} with includes disabled\")\n",
    "                return document, rst_content  # Return original content, not modified\n",
    "                \n",
    "            except Exception as parse_error:\n",
    "                print(f\"⚠️ Full parsing failed for {file_path.name}: {parse_error}\")\n",
    "                # Return minimal document for manual parsing fallback\n",
    "                settings = get_default_settings(Parser)\n",
    "                document = new_document(str(file_path), settings=settings)\n",
    "                return document, rst_content\n",
    "                \n",
    "        finally:\n",
    "            # Always restore original working directory\n",
    "            os.chdir(original_cwd)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def filter_raw_html(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove raw HTML blocks and directives that are meant for web view.\n",
    "    These are not useful for chunking and RAG purposes.\n",
    "    \"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    processed_lines = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        line_stripped = line.strip()\n",
    "        \n",
    "        # Check for raw HTML directive\n",
    "        if line_stripped.startswith('.. raw:: html'):\n",
    "            # Skip the directive line\n",
    "            i += 1\n",
    "            \n",
    "            # Skip any options (lines starting with spaces and colons)\n",
    "            while i < len(lines) and lines[i].startswith('   :'):\n",
    "                i += 1\n",
    "            \n",
    "            # Skip empty line after options\n",
    "            if i < len(lines) and lines[i].strip() == '':\n",
    "                i += 1\n",
    "            \n",
    "            # Skip the HTML content (indented lines)\n",
    "            while i < len(lines):\n",
    "                if lines[i].strip() == '':\n",
    "                    i += 1\n",
    "                    continue\n",
    "                    \n",
    "                # Check if line is indented (part of the raw HTML block)\n",
    "                if lines[i].startswith('   ') and lines[i].strip():\n",
    "                    i += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    # End of HTML block\n",
    "                    break\n",
    "            \n",
    "            # Add a comment indicating HTML was removed\n",
    "            processed_lines.append(\".. # Raw HTML content removed for chunking\")\n",
    "            continue\n",
    "            \n",
    "        # Check for HTML tags in regular content\n",
    "        elif '<' in line and '>' in line:\n",
    "            # Basic HTML tag detection and removal\n",
    "            import re\n",
    "            # Remove common HTML tags but keep the text content\n",
    "            cleaned_line = re.sub(r'<[^>]+>', '', line)\n",
    "            if cleaned_line.strip():\n",
    "                processed_lines.append(cleaned_line)\n",
    "            else:\n",
    "                processed_lines.append(line)  # Keep original if cleaning removed everything\n",
    "            i += 1\n",
    "        else:\n",
    "            processed_lines.append(line)\n",
    "            i += 1\n",
    "    \n",
    "    return '\\n'.join(processed_lines)\n",
    "def process_content_for_images(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Process content to replace image/figure references with descriptive text\n",
    "    and remove raw HTML content.\n",
    "    \"\"\"\n",
    "    # First remove raw HTML content\n",
    "    content = filter_raw_html(content)\n",
    "    \n",
    "    lines = content.split('\\n')\n",
    "    processed_lines = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        \n",
    "        # Check for image directive\n",
    "        if line.strip().startswith('.. image::'):\n",
    "            # Extract image path\n",
    "            image_path = line.split('::', 1)[1].strip()\n",
    "            \n",
    "            # Look for alt text in following lines\n",
    "            alt_text = None\n",
    "            j = i + 1\n",
    "            while j < len(lines) and (lines[j].startswith('   ') or lines[j].strip() == ''):\n",
    "                if lines[j].strip().startswith(':alt:'):\n",
    "                    alt_text = lines[j].split(':alt:', 1)[1].strip()\n",
    "                    break\n",
    "                elif lines[j].strip().startswith(':alt '):\n",
    "                    alt_text = lines[j].split(':alt ', 1)[1].strip()\n",
    "                    break\n",
    "                j += 1\n",
    "            \n",
    "            # Replace with descriptive text\n",
    "            if alt_text:\n",
    "                processed_lines.append(f\"Image: {alt_text}\")\n",
    "            else:\n",
    "                # Use filename if no alt text\n",
    "                filename = Path(image_path).stem.replace('-', ' ').replace('_', ' ')\n",
    "                processed_lines.append(f\"Image: {filename}\")\n",
    "            \n",
    "            # Skip the image directive and its options\n",
    "            i = j if alt_text else i + 1\n",
    "            continue\n",
    "            \n",
    "        # Check for figure directive\n",
    "        elif line.strip().startswith('.. figure::'):\n",
    "            # Extract figure path \n",
    "            figure_path = line.split('::', 1)[1].strip()\n",
    "            \n",
    "            # Look for alt text and caption in following lines\n",
    "            alt_text = None\n",
    "            caption = None\n",
    "            j = i + 1\n",
    "            \n",
    "            # Skip options (lines starting with spaces and colons)\n",
    "            while j < len(lines) and lines[j].startswith('   :'):\n",
    "                if lines[j].strip().startswith(':alt:'):\n",
    "                    alt_text = lines[j].split(':alt:', 1)[1].strip()\n",
    "                j += 1\n",
    "            \n",
    "            # Caption is the next non-empty, indented line after options\n",
    "            if j < len(lines) and lines[j].startswith('   ') and lines[j].strip():\n",
    "                caption = lines[j].strip()\n",
    "            \n",
    "            # Replace with descriptive text\n",
    "            if caption:\n",
    "                processed_lines.append(f\"Figure: {caption}\")\n",
    "            elif alt_text:\n",
    "                processed_lines.append(f\"Figure: {alt_text}\")\n",
    "            else:\n",
    "                # Use filename if no caption or alt text\n",
    "                filename = Path(figure_path).stem.replace('-', ' ').replace('_', ' ')\n",
    "                processed_lines.append(f\"Figure: {filename}\")\n",
    "            \n",
    "            # Skip the figure directive, options, and caption\n",
    "            i = j + 1 if caption else j\n",
    "            continue\n",
    "        else:\n",
    "            processed_lines.append(line)\n",
    "            i += 1\n",
    "    \n",
    "    return '\\n'.join(processed_lines)\n",
    "def extract_references_from_source(rst_content: str) -> List[RSTReference]:\n",
    "    \"\"\"\n",
    "    Extract references directly from source text instead of relying on AST.\n",
    "    This is more reliable and doesn't depend on successful parsing.\n",
    "    \"\"\"\n",
    "    references = []\n",
    "    lines = rst_content.split('\\n')\n",
    "    \n",
    "    for line_num, line in enumerate(lines, 1):\n",
    "        line_content = line.strip()\n",
    "        \n",
    "        # Detect RST directives that reference external files\n",
    "        if line_content.startswith('.. '):\n",
    "            for ref_type in ['include', 'literalinclude', 'csv-table']:\n",
    "                if f'.. {ref_type}::' in line_content:\n",
    "                    # Extract target from the directive\n",
    "                    parts = line_content.split('::', 1)\n",
    "                    if len(parts) > 1:\n",
    "                        target = parts[1].strip()\n",
    "                        references.append(RSTReference(\n",
    "                            reference_type=ref_type,\n",
    "                            target=target,\n",
    "                            line_number=line_num,\n",
    "                            directive=line_content\n",
    "                        ))\n",
    "                    break\n",
    "    \n",
    "    return references\n",
    "\n",
    "\n",
    "def extract_references(document: nodes.document, source_lines: List[str]) -> List[RSTReference]:\n",
    "    \"\"\"Extract references/includes from RST document (but not images/figures)\"\"\"\n",
    "    # First try AST-based extraction\n",
    "    references = []\n",
    "    \n",
    "    try:\n",
    "        # Use the newer findall method instead of deprecated traverse\n",
    "        for node in document.findall():\n",
    "            line_num = getattr(node, 'line', None)\n",
    "            \n",
    "            # Check for various reference types in the source (excluding images/figures)\n",
    "            if line_num and line_num <= len(source_lines):\n",
    "                line_content = source_lines[line_num - 1].strip()\n",
    "                \n",
    "                # Detect common RST directives that reference external files\n",
    "                # but exclude image and figure directives\n",
    "                if line_content.startswith('.. '):\n",
    "                    for ref_type in ['include', 'literalinclude', 'csv-table']:\n",
    "                        if f'.. {ref_type}::' in line_content:\n",
    "                            # Extract target from the directive\n",
    "                            parts = line_content.split('::', 1)\n",
    "                            if len(parts) > 1:\n",
    "                                target = parts[1].strip()\n",
    "                                references.append(RSTReference(\n",
    "                                    reference_type=ref_type,\n",
    "                                    target=target,\n",
    "                                    line_number=line_num,\n",
    "                                    directive=line_content\n",
    "                                ))\n",
    "                            break\n",
    "    except Exception:\n",
    "        # Fallback to source-based extraction if AST approach fails\n",
    "        pass\n",
    "    \n",
    "    # If AST extraction failed or found nothing, use source-based extraction\n",
    "    if not references:\n",
    "        references = extract_references_from_source('\\n'.join(source_lines))\n",
    "    \n",
    "    return references\n",
    "\n",
    "\n",
    "def get_section_level(node: nodes.section, title_levels: Dict[str, int]) -> int:\n",
    "    \"\"\"Determine section level based on title decoration\"\"\"\n",
    "    title = node[0]  # First child should be title\n",
    "    if isinstance(title, nodes.title):\n",
    "        # Get the raw text and try to find its decoration in source\n",
    "        return len(list(node.traverse(nodes.section, include_self=False)))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def extract_semantic_chunks(document: nodes.document, rst_content: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract semantic chunks from RST document\"\"\"\n",
    "    chunks = []\n",
    "    source_lines = rst_content.split('\\n')\n",
    "    \n",
    "    # Extract document title if present\n",
    "    if document.children and isinstance(document.children[0], nodes.title):\n",
    "        title_node = document.children[0]\n",
    "        title_line = getattr(title_node, 'line', 1)\n",
    "        title_end = title_line + 2  # Usually title + decoration\n",
    "        \n",
    "        title_content = '\\n'.join(source_lines[:title_end])\n",
    "        title_content = process_content_for_images(title_content)\n",
    "        \n",
    "        chunks.append({\n",
    "            'type': 'document_title',\n",
    "            'name': str(title_node.astext()),\n",
    "            'start_line': 1,\n",
    "            'end_line': title_end,\n",
    "            'content': title_content,\n",
    "            'level': 0,\n",
    "            'depth': 0\n",
    "        })\n",
    "    \n",
    "    # Process sections hierarchically\n",
    "    def process_section(section: nodes.section, parent_depth: int = 0):\n",
    "        if not isinstance(section, nodes.section):\n",
    "            return\n",
    "            \n",
    "        title = section[0] if section.children and isinstance(section[0], nodes.title) else None\n",
    "        if not title:\n",
    "            return\n",
    "            \n",
    "        section_start = getattr(title, 'line', 1)\n",
    "        section_name = title.astext()\n",
    "        \n",
    "        # Find section end by looking for next sibling or parent end\n",
    "        section_end = len(source_lines)\n",
    "        for sibling in section.parent.children[section.parent.children.index(section) + 1:]:\n",
    "            if hasattr(sibling, 'line') and sibling.line:\n",
    "                section_end = sibling.line - 1\n",
    "                break\n",
    "        \n",
    "        # Count subsections to determine actual end\n",
    "        subsections = list(section.traverse(nodes.section, include_self=False))\n",
    "        if subsections:\n",
    "            # Section ends where first subsection starts\n",
    "            first_subsection = subsections[0]\n",
    "            if hasattr(first_subsection[0], 'line'):\n",
    "                section_content_end = first_subsection[0].line - 1\n",
    "            else:\n",
    "                section_content_end = section_end\n",
    "        else:\n",
    "            section_content_end = section_end\n",
    "        \n",
    "        # Extract section content (without subsections)\n",
    "        section_content_lines = []\n",
    "        current_line = section_start - 1\n",
    "        \n",
    "        # Add title and content until first subsection\n",
    "        while current_line < min(section_content_end, len(source_lines)):\n",
    "            section_content_lines.append(source_lines[current_line])\n",
    "            current_line += 1\n",
    "        \n",
    "        section_content = '\\n'.join(section_content_lines)\n",
    "        \n",
    "        # Process content to replace images with descriptions\n",
    "        section_content = process_content_for_images(section_content)\n",
    "        \n",
    "        if section_content.strip():\n",
    "            chunks.append({\n",
    "                'type': 'section',\n",
    "                'name': section_name,\n",
    "                'start_line': section_start,\n",
    "                'end_line': section_content_end,\n",
    "                'content': section_content,\n",
    "                'level': parent_depth + 1,\n",
    "                'depth': parent_depth\n",
    "            })\n",
    "        \n",
    "        # Process subsections\n",
    "        for subsection in section.traverse(nodes.section, include_self=False):\n",
    "            if subsection.parent == section:  # Direct child only\n",
    "                process_section(subsection, parent_depth + 1)\n",
    "    \n",
    "    # Process all top-level sections\n",
    "    for section in document.traverse(nodes.section):\n",
    "        if section.parent == document:  # Top-level sections only\n",
    "            process_section(section)\n",
    "    \n",
    "            # Note: Code blocks are NOT extracted separately - they stay within their sections\n",
    "        # This ensures code remains intact within the section context\n",
    "        \n",
    "        # Extract only directives (notes, warnings, etc.) that are standalone\n",
    "        for node in document.traverse():\n",
    "            line_num = getattr(node, 'line', None)\n",
    "            if not line_num:\n",
    "                continue\n",
    "                \n",
    "            # Directives (notes, warnings, etc.) - only if not within a section\n",
    "            if isinstance(node, nodes.Admonition):\n",
    "                # Check if this admonition is within a section\n",
    "                parent_section = None\n",
    "                for ancestor in node.traverse(include_self=False, descend=False):\n",
    "                    if isinstance(ancestor, nodes.section):\n",
    "                        parent_section = ancestor\n",
    "                        break\n",
    "                \n",
    "                # Only create separate chunk if not within a section\n",
    "                if not parent_section:\n",
    "                    admonition_text = node.astext()\n",
    "                    admonition_type = node.tagname if hasattr(node, 'tagname') else 'admonition'\n",
    "                    chunks.append({\n",
    "                        'type': f'{admonition_type}_directive',\n",
    "                        'name': f'{admonition_type}_line_{line_num}',\n",
    "                        'start_line': line_num,\n",
    "                        'end_line': line_num + admonition_text.count('\\n'),\n",
    "                        'content': admonition_text,\n",
    "                        'level': 8,\n",
    "                        'depth': 0\n",
    "                    })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CHUNK CREATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_rst_chunks(semantic_nodes: List[Dict[str, Any]]) -> List[RSTChunk]:\n",
    "    \"\"\"Create RSTChunk objects from semantic nodes\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    for node_info in semantic_nodes:\n",
    "        chunk = RSTChunk(\n",
    "            start_line=node_info['start_line'],\n",
    "            end_line=node_info['end_line'],\n",
    "            content=node_info['content'],\n",
    "            node_type=node_info['type'],\n",
    "            name=node_info['name'],\n",
    "            depth=node_info.get('depth', 0),\n",
    "            level=node_info.get('level', 0)\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def group_small_chunks(chunks: List[RSTChunk], target_tokens: int = TARGET_TOKENS) -> List[RSTChunk]:\n",
    "    \"\"\"Group small chunks together aggressively to reach reasonable size\"\"\"\n",
    "    if not chunks:\n",
    "        return chunks\n",
    "    \n",
    "    print(f\"🔄 Grouping {len(chunks)} chunks (target: {target_tokens} tokens)\")\n",
    "    \n",
    "    # Check if everything together is under the max limit\n",
    "    total_tokens = sum(c.token_count for c in chunks)\n",
    "    if total_tokens <= MAX_CHUNK_TOKENS:\n",
    "        # Combine everything into one chunk\n",
    "        combined_content = '\\n\\n'.join(c.content for c in chunks)\n",
    "        combined_chunk = RSTChunk(\n",
    "            start_line=chunks[0].start_line,\n",
    "            end_line=chunks[-1].end_line,\n",
    "            content=combined_content,\n",
    "            node_type='complete_document',\n",
    "            name=f\"complete_document_{len(chunks)}_parts\",\n",
    "            depth=0,\n",
    "            level=0\n",
    "        )\n",
    "        print(f\"✅ Combined all {len(chunks)} chunks into 1 complete document ({total_tokens} tokens)\")\n",
    "        return [combined_chunk]\n",
    "    \n",
    "    # Group chunks more aggressively - aim for larger chunks\n",
    "    grouped_chunks = []\n",
    "    current_group = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    # Sort chunks by level to group similar hierarchy levels together\n",
    "    sorted_chunks = sorted(chunks, key=lambda c: (c.level, c.start_line))\n",
    "    \n",
    "    for chunk in sorted_chunks:\n",
    "        # More aggressive grouping - use higher threshold\n",
    "        can_add = (current_tokens + chunk.token_count <= MAX_CHUNK_TOKENS)\n",
    "        should_group = (current_tokens + chunk.token_count <= target_tokens * 1.5)  # 1.5x target\n",
    "        \n",
    "        if can_add and (not current_group or should_group):\n",
    "            current_group.append(chunk)\n",
    "            current_tokens += chunk.token_count\n",
    "        else:\n",
    "            # Finalize current group if it has content\n",
    "            if current_group:\n",
    "                if len(current_group) == 1:\n",
    "                    grouped_chunks.append(current_group[0])\n",
    "                else:\n",
    "                    # Create grouped chunk\n",
    "                    group_content = '\\n\\n'.join(c.content for c in current_group)\n",
    "                    \n",
    "                    # Better naming based on content types\n",
    "                    group_types = list(set(c.node_type for c in current_group))\n",
    "                    if len(group_types) == 1:\n",
    "                        group_name = f\"{group_types[0]}_group_{len(current_group)}_parts\"\n",
    "                    else:\n",
    "                        group_name = f\"mixed_content_{len(current_group)}_parts\"\n",
    "                    \n",
    "                    # Use the earliest chunk's position info\n",
    "                    earliest_chunk = min(current_group, key=lambda c: c.start_line)\n",
    "                    latest_chunk = max(current_group, key=lambda c: c.end_line)\n",
    "                    \n",
    "                    grouped_chunk = RSTChunk(\n",
    "                        start_line=earliest_chunk.start_line,\n",
    "                        end_line=latest_chunk.end_line,\n",
    "                        content=group_content,\n",
    "                        node_type='grouped_content',\n",
    "                        name=group_name,\n",
    "                        depth=min(c.depth for c in current_group),  # Use minimum depth\n",
    "                        level=min(c.level for c in current_group)   # Use minimum level\n",
    "                    )\n",
    "                    grouped_chunks.append(grouped_chunk)\n",
    "                \n",
    "                print(f\"  📦 Grouped {len(current_group)} chunks → {current_tokens} tokens\")\n",
    "            \n",
    "            # Start new group with current chunk\n",
    "            current_group = [chunk]\n",
    "            current_tokens = chunk.token_count\n",
    "    \n",
    "    # Add final group\n",
    "    if current_group:\n",
    "        if len(current_group) == 1:\n",
    "            grouped_chunks.append(current_group[0])\n",
    "        else:\n",
    "            group_content = '\\n\\n'.join(c.content for c in current_group)\n",
    "            \n",
    "            # Better naming\n",
    "            group_types = list(set(c.node_type for c in current_group))\n",
    "            if len(group_types) == 1:\n",
    "                group_name = f\"{group_types[0]}_group_{len(current_group)}_parts\"\n",
    "            else:\n",
    "                group_name = f\"mixed_content_{len(current_group)}_parts\"\n",
    "            \n",
    "            earliest_chunk = min(current_group, key=lambda c: c.start_line)\n",
    "            latest_chunk = max(current_group, key=lambda c: c.end_line)\n",
    "            \n",
    "            grouped_chunk = RSTChunk(\n",
    "                start_line=earliest_chunk.start_line,\n",
    "                end_line=latest_chunk.end_line,\n",
    "                content=group_content,\n",
    "                node_type='grouped_content',\n",
    "                name=group_name,\n",
    "                depth=min(c.depth for c in current_group),\n",
    "                level=min(c.level for c in current_group)\n",
    "            )\n",
    "            grouped_chunks.append(grouped_chunk)\n",
    "            \n",
    "        print(f\"  📦 Final group: {len(current_group)} chunks → {current_tokens} tokens\")\n",
    "    \n",
    "    print(f\"✅ Grouping result: {len(chunks)} → {len(grouped_chunks)} chunks\")\n",
    "    \n",
    "    # Show final chunk sizes\n",
    "    for i, chunk in enumerate(grouped_chunks):\n",
    "        print(f\"    Chunk {i+1}: {chunk.token_count} tokens ({chunk.name})\")\n",
    "    \n",
    "    return grouped_chunks\n",
    "\n",
    "\n",
    "def analyze_code_blocks_in_content(content: str) -> List[Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Analyze code blocks in content to identify their boundaries.\n",
    "    Returns list of code block locations with start/end line numbers.\n",
    "    \"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    code_blocks = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        # Check for code-block directive\n",
    "        if line.startswith('.. code-block::') or line.startswith('.. literalinclude::'):\n",
    "            start_line = i\n",
    "            i += 1\n",
    "            \n",
    "            # Skip options (lines starting with spaces and colons)\n",
    "            while i < len(lines) and lines[i].startswith('   :'):\n",
    "                i += 1\n",
    "            \n",
    "            # Skip empty line after options\n",
    "            if i < len(lines) and lines[i].strip() == '':\n",
    "                i += 1\n",
    "            \n",
    "            # Find end of code block (when indentation decreases)\n",
    "            block_indent = None\n",
    "            end_line = i\n",
    "            \n",
    "            while i < len(lines):\n",
    "                if lines[i].strip() == '':\n",
    "                    i += 1\n",
    "                    continue\n",
    "                    \n",
    "                # Check indentation\n",
    "                current_indent = len(lines[i]) - len(lines[i].lstrip())\n",
    "                \n",
    "                if block_indent is None:\n",
    "                    if current_indent > 0:\n",
    "                        block_indent = current_indent\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    if current_indent < block_indent and lines[i].strip():\n",
    "                        break\n",
    "                    i += 1\n",
    "            \n",
    "            end_line = i - 1\n",
    "            code_blocks.append({\n",
    "                'start': start_line,\n",
    "                'end': end_line,\n",
    "                'type': 'directive'\n",
    "            })\n",
    "            continue\n",
    "            \n",
    "        # Check for literal blocks (double colon)\n",
    "        elif line.endswith('::') and not line.startswith('.. '):\n",
    "            start_line = i\n",
    "            i += 1\n",
    "            \n",
    "            # Skip empty line after ::\n",
    "            if i < len(lines) and lines[i].strip() == '':\n",
    "                i += 1\n",
    "            \n",
    "            # Find end of literal block\n",
    "            block_indent = None\n",
    "            end_line = i\n",
    "            \n",
    "            while i < len(lines):\n",
    "                if lines[i].strip() == '':\n",
    "                    i += 1\n",
    "                    continue\n",
    "                    \n",
    "                current_indent = len(lines[i]) - len(lines[i].lstrip())\n",
    "                \n",
    "                if block_indent is None:\n",
    "                    if current_indent > 0:\n",
    "                        block_indent = current_indent\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    if current_indent < block_indent and lines[i].strip():\n",
    "                        break\n",
    "                    i += 1\n",
    "            \n",
    "            end_line = i - 1\n",
    "            code_blocks.append({\n",
    "                'start': start_line,\n",
    "                'end': end_line,\n",
    "                'type': 'literal'\n",
    "            })\n",
    "            continue\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "    return code_blocks\n",
    "\n",
    "\n",
    "def split_content_preserving_code_blocks(content: str, max_tokens: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split content into chunks while preserving code block integrity.\n",
    "    If a code block is too large, it gets its own chunk.\n",
    "    \"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    code_blocks = analyze_code_blocks_in_content(content)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk_lines = []\n",
    "    current_tokens = 0\n",
    "    line_idx = 0\n",
    "    \n",
    "    while line_idx < len(lines):\n",
    "        # Check if current line is start of a code block\n",
    "        current_code_block = None\n",
    "        for cb in code_blocks:\n",
    "            if cb['start'] == line_idx:\n",
    "                current_code_block = cb\n",
    "                break\n",
    "        \n",
    "        if current_code_block:\n",
    "            # We're at the start of a code block\n",
    "            code_block_lines = lines[current_code_block['start']:current_code_block['end'] + 1]\n",
    "            code_block_content = '\\n'.join(code_block_lines)\n",
    "            code_block_tokens = count_tokens(code_block_content)\n",
    "            \n",
    "            # If code block + current chunk would exceed limit, finalize current chunk\n",
    "            if current_tokens + code_block_tokens > max_tokens and current_chunk_lines:\n",
    "                chunks.append('\\n'.join(current_chunk_lines))\n",
    "                current_chunk_lines = []\n",
    "                current_tokens = 0\n",
    "            \n",
    "            # If code block itself is too large, give it its own chunk\n",
    "            if code_block_tokens > max_tokens:\n",
    "                # Finalize current chunk if it has content\n",
    "                if current_chunk_lines:\n",
    "                    chunks.append('\\n'.join(current_chunk_lines))\n",
    "                    current_chunk_lines = []\n",
    "                    current_tokens = 0\n",
    "                \n",
    "                # Code block gets its own chunk\n",
    "                chunks.append(code_block_content)\n",
    "            else:\n",
    "                # Add code block to current chunk\n",
    "                current_chunk_lines.extend(code_block_lines)\n",
    "                current_tokens += code_block_tokens\n",
    "            \n",
    "            # Move past the code block\n",
    "            line_idx = current_code_block['end'] + 1\n",
    "        else:\n",
    "            # Regular line - add if it fits\n",
    "            line = lines[line_idx]\n",
    "            line_tokens = count_tokens(line)\n",
    "            \n",
    "            if current_tokens + line_tokens > max_tokens and current_chunk_lines:\n",
    "                # Finalize current chunk\n",
    "                chunks.append('\\n'.join(current_chunk_lines))\n",
    "                current_chunk_lines = [line]\n",
    "                current_tokens = line_tokens\n",
    "            else:\n",
    "                current_chunk_lines.append(line)\n",
    "                current_tokens += line_tokens\n",
    "            \n",
    "            line_idx += 1\n",
    "    \n",
    "    # Add final chunk\n",
    "    if current_chunk_lines:\n",
    "        chunks.append('\\n'.join(current_chunk_lines))\n",
    "    \n",
    "    return chunks\n",
    "def sub_chunk_by_lines(chunk: RSTChunk, rst_content: str) -> List[RSTChunk]:\n",
    "    \"\"\"Sub-chunk oversized chunks while preserving code block integrity\"\"\"\n",
    "    if chunk.token_count <= MAX_CHUNK_TOKENS:\n",
    "        return [chunk]\n",
    "    \n",
    "    # Extract the chunk's content from the full document\n",
    "    lines = rst_content.split('\\n')\n",
    "    chunk_lines = lines[chunk.start_line-1:chunk.end_line]\n",
    "    chunk_content = '\\n'.join(chunk_lines)\n",
    "    \n",
    "    # Split content preserving code blocks\n",
    "    sub_contents = split_content_preserving_code_blocks(chunk_content, MAX_CHUNK_TOKENS)\n",
    "    \n",
    "    if len(sub_contents) <= 1:\n",
    "        return [chunk]  # Couldn't split effectively\n",
    "    \n",
    "    sub_chunks = []\n",
    "    lines_processed = 0\n",
    "    \n",
    "    for i, sub_content in enumerate(sub_contents):\n",
    "        sub_lines = sub_content.split('\\n')\n",
    "        sub_start_line = chunk.start_line + lines_processed\n",
    "        sub_end_line = sub_start_line + len(sub_lines) - 1\n",
    "        \n",
    "        sub_chunk = RSTChunk(\n",
    "            start_line=sub_start_line,\n",
    "            end_line=sub_end_line,\n",
    "            content=sub_content,\n",
    "            node_type=f\"{chunk.node_type}_part\",\n",
    "            name=f\"{chunk.name}_part_{i+1}\",\n",
    "            depth=chunk.depth + 1,\n",
    "            level=chunk.level\n",
    "        )\n",
    "        sub_chunks.append(sub_chunk)\n",
    "        \n",
    "        lines_processed += len(sub_lines)\n",
    "    \n",
    "    return sub_chunks\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FILE PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def process_rst_file(file_path: Path) -> List[RSTChunk]:\n",
    "    \"\"\"Process a single RST file and return chunks\"\"\"\n",
    "    print(f\"\\n🔍 Processing: {file_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Parse RST file\n",
    "        document, rst_content = parse_rst_file(file_path)\n",
    "        \n",
    "        if document.children:\n",
    "            print(f\"✅ Successfully parsed RST structure\")\n",
    "        else:\n",
    "            print(\"⚠️ Empty document\")\n",
    "            return []\n",
    "        \n",
    "        # Extract references (similar to imports)\n",
    "        references = extract_references(document, rst_content.split('\\n'))\n",
    "        if references:\n",
    "            print(f\"📎 Found {len(references)} references/includes\")\n",
    "            for ref in references:\n",
    "                print(f\"  - {ref.reference_type}: {ref.target}\")\n",
    "        \n",
    "        # Find semantic chunks\n",
    "        semantic_nodes = extract_semantic_chunks(document, rst_content)\n",
    "        print(f\"Found {len(semantic_nodes)} semantic units\")\n",
    "        \n",
    "        # Show what we found\n",
    "        for node in semantic_nodes:\n",
    "            preview = node['content'][:100].replace('\\n', ' ').strip()\n",
    "            print(f\"  - {node['type']}: {node['name']} ({count_tokens(node['content'])} tokens)\")\n",
    "            print(f\"    Preview: {preview}...\")\n",
    "        \n",
    "        # Create chunks\n",
    "        base_chunks = create_rst_chunks(semantic_nodes)\n",
    "        \n",
    "        # Group small chunks\n",
    "        base_chunks = group_small_chunks(base_chunks, target_tokens=TARGET_TOKENS)\n",
    "        print(f\"Created {len(base_chunks)} semantic chunks\")\n",
    "        \n",
    "        # Apply sub-chunking for oversized chunks\n",
    "        final_chunks = []\n",
    "        oversized_count = 0\n",
    "        \n",
    "        for chunk in base_chunks:\n",
    "            if chunk.token_count > MAX_CHUNK_TOKENS:\n",
    "                print(f\"  Sub-chunking {chunk.name} ({chunk.token_count} tokens)\")\n",
    "                sub_chunks = sub_chunk_by_lines(chunk, rst_content)\n",
    "                final_chunks.extend(sub_chunks)\n",
    "                oversized_count += 1\n",
    "            else:\n",
    "                final_chunks.append(chunk)\n",
    "        \n",
    "        if oversized_count > 0:\n",
    "            print(f\"Sub-chunked {oversized_count} oversized chunks\")\n",
    "        print(f\"Final result: {len(final_chunks)} total chunks\")\n",
    "        \n",
    "        return final_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_unique_id(length: int = 6) -> str:\n",
    "    \"\"\"Generate a random unique ID\"\"\"\n",
    "    alphabet = string.ascii_lowercase + string.digits\n",
    "    return ''.join(secrets.choice(alphabet) for _ in range(length))\n",
    "\n",
    "\n",
    "def create_chunk_filename(original_filename: str, chunk_number: int, unique_id: str) -> str:\n",
    "    \"\"\"Create chunk filename: index.rst_chunk_001_a1s2d3.md\"\"\"\n",
    "    return f\"{original_filename}_chunk_{chunk_number:03d}_{unique_id}.md\"\n",
    "\n",
    "\n",
    "def create_chunk_markdown(chunk: RSTChunk, source_file_path: str, references: List[RSTReference]) -> str:\n",
    "    \"\"\"Create markdown content with YAML frontmatter\"\"\"\n",
    "    unique_id = generate_unique_id()\n",
    "    \n",
    "    # Filter references that might apply to this chunk\n",
    "    chunk_references = []\n",
    "    for ref in references:\n",
    "        if chunk.start_line <= ref.line_number <= chunk.end_line:\n",
    "            chunk_references.append(f\"{ref.reference_type}: {ref.target}\")\n",
    "    \n",
    "    frontmatter = f\"\"\"---\n",
    "file_path: \"{source_file_path}\"\n",
    "chunk_id: \"{unique_id}\"\n",
    "chunk_type: \"{chunk.node_type}\"\n",
    "chunk_name: \"{chunk.name}\"\n",
    "start_line: {chunk.start_line}\n",
    "end_line: {chunk.end_line}\n",
    "token_count: {chunk.token_count}\n",
    "depth: {chunk.depth}\n",
    "level: {chunk.level}\n",
    "language: \"rst\"\n",
    "references: {chunk_references}\n",
    "---\n",
    "\n",
    "# {chunk.name}\n",
    "\n",
    "**Type:** {chunk.node_type}  \n",
    "**Tokens:** {chunk.token_count}  \n",
    "**Depth:** {chunk.depth}  \n",
    "**Level:** {chunk.level}\n",
    "\n",
    "```rst\n",
    "{chunk.content}\n",
    "```\n",
    "\"\"\"\n",
    "    return frontmatter\n",
    "\n",
    "\n",
    "def save_chunks_to_files(chunks: List[RSTChunk], \n",
    "                        original_file_path: Path, \n",
    "                        input_directory: Path,\n",
    "                        output_base: Path,\n",
    "                        references: List[RSTReference]) -> List[str]:\n",
    "    \"\"\"Save chunks as markdown files maintaining directory structure\"\"\"\n",
    "    if not chunks:\n",
    "        return []\n",
    "    \n",
    "    # Calculate relative path from input directory\n",
    "    try:\n",
    "        rel_path = original_file_path.relative_to(input_directory)\n",
    "    except ValueError:\n",
    "        # If file is not under input directory, use just the filename\n",
    "        rel_path = original_file_path.name\n",
    "    \n",
    "    # Create output directory structure\n",
    "    output_dir = output_base / rel_path.parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate unique ID for this file\n",
    "    file_unique_id = generate_unique_id()\n",
    "    \n",
    "    saved_files = []\n",
    "    \n",
    "    # Add chunk index to each chunk and save\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        # Create chunk filename\n",
    "        chunk_filename = create_chunk_filename(\n",
    "            original_file_path.name, \n",
    "            i, \n",
    "            file_unique_id\n",
    "        )\n",
    "        \n",
    "        # Create markdown content\n",
    "        markdown_content = create_chunk_markdown(\n",
    "            chunk, \n",
    "            str(rel_path), \n",
    "            references\n",
    "        )\n",
    "        \n",
    "        # Write to file\n",
    "        chunk_file_path = output_dir / chunk_filename\n",
    "        try:\n",
    "            with open(chunk_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "            \n",
    "            saved_files.append(str(chunk_file_path))\n",
    "            print(f\"  ✅ Saved: {chunk_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error saving {chunk_filename}: {e}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "\n",
    "def print_chunk_summary(chunks: List[RSTChunk], file_name: str):\n",
    "    \"\"\"Print detailed summary of chunks\"\"\"\n",
    "    print(f\"\\n--- RST Chunk Summary for {file_name} ---\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        indent = \"  \" * chunk.depth\n",
    "        content_lines = len(chunk.content.split('\\n'))\n",
    "        \n",
    "        print(f\"{indent}{i}. {chunk.name}\")\n",
    "        print(f\"{indent}   Type: {chunk.node_type} | Level: {chunk.level} | Lines: {content_lines} | Tokens: {chunk.token_count}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# JUPYTER NOTEBOOK FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def process_rst_content(rst_content: str, file_name: str = \"content.rst\") -> List[RSTChunk]:\n",
    "    \"\"\"\n",
    "    Process RST content directly (for Jupyter notebooks).\n",
    "    \n",
    "    Args:\n",
    "        rst_content: Raw RST content as string\n",
    "        file_name: Name to use for the content (for display purposes)\n",
    "    \n",
    "    Returns:\n",
    "        List of RSTChunk objects\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the approach you suggested: pre-process to remove includes\n",
    "        rst_input = remove_include_directives(rst_content)\n",
    "        \n",
    "        # Parse with docutils using the pattern you mentioned\n",
    "        from docutils.frontend import get_default_settings\n",
    "        \n",
    "        settings = get_default_settings(Parser)\n",
    "        settings.report_level = 4  # Show errors but not warnings\n",
    "        settings.halt_level = 5    # Don't halt on errors\n",
    "        settings.warning_stream = None  # Suppress warnings\n",
    "        \n",
    "        document = new_document(file_name, settings=settings)\n",
    "        parser = Parser()\n",
    "        \n",
    "        try:\n",
    "            parser.parse(rst_input, document)\n",
    "            print(f\"✅ Successfully parsed RST structure for {file_name}\")\n",
    "        except Exception as parse_error:\n",
    "            print(f\"⚠️ Parse error in {file_name}: {parse_error}\")\n",
    "            # Continue with whatever was parsed\n",
    "        \n",
    "        if not document.children:\n",
    "            print(f\"⚠️ Empty document: {file_name}\")\n",
    "            return []\n",
    "        \n",
    "        # Extract references from original content (not the modified one)\n",
    "        references = extract_references_from_source(rst_content)\n",
    "        if references:\n",
    "            print(f\"📎 Found {len(references)} references/includes\")\n",
    "            for ref in references:\n",
    "                print(f\"  - {ref.reference_type}: {ref.target}\")\n",
    "        \n",
    "        # Find semantic chunks\n",
    "        semantic_nodes = extract_semantic_chunks(document, rst_content)\n",
    "        print(f\"Found {len(semantic_nodes)} semantic units\")\n",
    "        \n",
    "        # Show what we found\n",
    "        for node in semantic_nodes:\n",
    "            preview = node['content'][:100].replace('\\n', ' ').strip()\n",
    "            print(f\"  - {node['type']}: {node['name']} ({count_tokens(node['content'])} tokens)\")\n",
    "            print(f\"    Preview: {preview}...\")\n",
    "        \n",
    "        # Create chunks\n",
    "        base_chunks = create_rst_chunks(semantic_nodes)\n",
    "        \n",
    "        # Group small chunks\n",
    "        base_chunks = group_small_chunks(base_chunks, target_tokens=TARGET_TOKENS)\n",
    "        print(f\"Created {len(base_chunks)} semantic chunks\")\n",
    "        \n",
    "        # Apply sub-chunking for oversized chunks\n",
    "        final_chunks = []\n",
    "        oversized_count = 0\n",
    "        \n",
    "        for chunk in base_chunks:\n",
    "            if chunk.token_count > MAX_CHUNK_TOKENS:\n",
    "                print(f\"  Sub-chunking {chunk.name} ({chunk.token_count} tokens)\")\n",
    "                sub_chunks = sub_chunk_by_lines(chunk, rst_content)\n",
    "                final_chunks.extend(sub_chunks)\n",
    "                oversized_count += 1\n",
    "            else:\n",
    "                final_chunks.append(chunk)\n",
    "        \n",
    "        if oversized_count > 0:\n",
    "            print(f\"Sub-chunked {oversized_count} oversized chunks\")\n",
    "        print(f\"Final result: {len(final_chunks)} total chunks\")\n",
    "        \n",
    "        return final_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing RST content: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def display_chunks(chunks: List[RSTChunk]) -> None:\n",
    "    \"\"\"Display chunks in a notebook-friendly format\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"📄 RST CHUNKS SUMMARY ({len(chunks)} chunks)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    total_tokens = sum(chunk.token_count for chunk in chunks)\n",
    "    print(f\"📊 Total tokens: {total_tokens:,}\")\n",
    "    print(f\"📊 Average tokens per chunk: {total_tokens/len(chunks):.1f}\" if chunks else \"No chunks\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        indent = \"  \" * chunk.depth\n",
    "        content_lines = len(chunk.content.split('\\n'))\n",
    "        \n",
    "        print(f\"\\n{indent}📝 Chunk {i}: {chunk.name}\")\n",
    "        print(f\"{indent}   Type: {chunk.node_type} | Level: {chunk.level} | Lines: {content_lines} | Tokens: {chunk.token_count}\")\n",
    "        \n",
    "        # Show content preview\n",
    "        preview = chunk.content[:200].replace('\\n', ' ').strip()\n",
    "        if len(chunk.content) > 200:\n",
    "            preview += \"...\"\n",
    "        print(f\"{indent}   Preview: {preview}\")\n",
    "\n",
    "\n",
    "def save_chunks_as_markdown(chunks: List[RSTChunk], output_dir: str = \"rst_chunks\") -> None:\n",
    "    \"\"\"Save chunks as markdown files (notebook version)\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    file_unique_id = generate_unique_id()\n",
    "    saved_count = 0\n",
    "    \n",
    "    print(f\"\\n💾 Saving {len(chunks)} chunks to {output_path}/\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        # Create chunk filename\n",
    "        chunk_filename = f\"chunk_{i:03d}_{file_unique_id}.md\"\n",
    "        \n",
    "        # Create markdown content\n",
    "        markdown_content = create_chunk_markdown(chunk, \"notebook_content.rst\", [])\n",
    "        \n",
    "        # Write to file\n",
    "        chunk_file_path = output_path / chunk_filename\n",
    "        try:\n",
    "            with open(chunk_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "            \n",
    "            print(f\"  ✅ Saved: {chunk_filename}\")\n",
    "            saved_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error saving {chunk_filename}: {e}\")\n",
    "    \n",
    "    print(f\"✅ Successfully saved {saved_count} chunk files\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for RST semantic chunking\"\"\"\n",
    "    print(\"🚀 RST (reStructuredText) Semantic Chunking\")\n",
    "    print(f\"Max chunk tokens: {MAX_CHUNK_TOKENS}\")\n",
    "    print(f\"Target tokens for grouping: {TARGET_TOKENS}\")\n",
    "    \n",
    "    # Get directory from user or use current directory\n",
    "    directory = input(\"\\nEnter source directory path (or press Enter for current directory): \").strip()\n",
    "    if not directory:\n",
    "        directory = \".\"\n",
    "    \n",
    "    target_dir = Path(directory).resolve()\n",
    "    if not target_dir.exists():\n",
    "        print(f\"❌ Directory not found: {directory}\")\n",
    "        return\n",
    "    \n",
    "    # Create output directory parallel to source directory\n",
    "    output_dir = target_dir.parent / f\"{target_dir.name}_rst_chunks\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    print(f\"📁 Output directory: {output_dir}\")\n",
    "    \n",
    "    target_path = target_dir\n",
    "    input_directory = target_dir\n",
    "    \n",
    "    \n",
    "    # Collect RST files\n",
    "    rst_files = []\n",
    "    for ext in ['*.rst', '*.txt']:\n",
    "        rst_files.extend(target_path.rglob(ext))\n",
    "    \n",
    "    # Filter to actual RST files by checking content\n",
    "    actual_rst_files = []\n",
    "    for file in rst_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read(1000)  # Check first 1000 chars\n",
    "                # Simple heuristic: look for RST-like content\n",
    "                if any(marker in content for marker in ['===', '---', '~~~', '^^^', '.. ', '::']):\n",
    "                    actual_rst_files.append(file)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    rst_files = actual_rst_files\n",
    "    \n",
    "    if not rst_files:\n",
    "        print(f\"❌ No RST files found in {directory}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"🔍 Found {len(rst_files)} RST file(s)\")\n",
    "    \n",
    "    # Group by directory for display\n",
    "    by_dir = {}\n",
    "    for f in rst_files:\n",
    "        dir_path = str(f.parent.relative_to(input_directory)) if f.parent != input_directory else '.'\n",
    "        by_dir[dir_path] = by_dir.get(dir_path, []) + [f.name]\n",
    "    \n",
    "    for dir_path, files in sorted(by_dir.items()):\n",
    "        print(f\"  📂 {dir_path}: {len(files)} files\")\n",
    "        for file_name in sorted(files)[:3]:  # Show first 3 files\n",
    "            print(f\"    📄 {file_name}\")\n",
    "        if len(files) > 3:\n",
    "            print(f\"    ... and {len(files) - 3} more\")\n",
    "    \n",
    "    # Process all files automatically\n",
    "    print(f\"\\n🔄 Processing all {len(rst_files)} file(s)...\")\n",
    "    all_chunks = {}\n",
    "    all_references = {}\n",
    "    \n",
    "    for file_path in rst_files:\n",
    "        try:\n",
    "            chunks = process_rst_file(file_path)\n",
    "            all_chunks[file_path] = chunks\n",
    "            \n",
    "            # Extract references for this file\n",
    "            document, rst_content = parse_rst_file(file_path)\n",
    "            references = extract_references(document, rst_content.split('\\n'))\n",
    "            all_references[file_path] = references\n",
    "            \n",
    "            print_chunk_summary(chunks, file_path.name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Summary\n",
    "    total_chunks = sum(len(chunks) for chunks in all_chunks.values())\n",
    "    total_tokens = sum(chunk.token_count for chunks in all_chunks.values() for chunk in chunks)\n",
    "    \n",
    "    print(f\"\\n📊 Processing Summary:\")\n",
    "    print(f\"   Files processed: {len(all_chunks)}\")\n",
    "    print(f\"   Total chunks: {total_chunks}\")\n",
    "    print(f\"   Total tokens: {total_tokens:,}\")\n",
    "    print(f\"   Average tokens per chunk: {total_tokens/total_chunks:.1f}\" if total_chunks > 0 else \"   No chunks created\")\n",
    "    \n",
    "    # Save chunks automatically with parallel directory structure\n",
    "    print(f\"\\n💾 Saving chunks to markdown files...\")\n",
    "    saved_count = 0\n",
    "    \n",
    "    for file_path, chunks in all_chunks.items():\n",
    "        references = all_references.get(file_path, [])\n",
    "        saved_files = save_chunks_to_files(chunks, file_path, input_directory, output_dir, references)\n",
    "        saved_count += len(saved_files)\n",
    "    \n",
    "    print(f\"✅ Saved {saved_count} chunk files to {output_dir}\")\n",
    "    print(f\"📁 Directory structure preserved in output\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
