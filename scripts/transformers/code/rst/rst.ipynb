{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405ef96b",
   "metadata": {},
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ff9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883da7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "RST (reStructuredText) AST Chunker\n",
    "\n",
    "This module implements semantic chunking for RST files using docutils for parsing.\n",
    "It follows the same patterns as the existing Python and TypeScript AST chunkers.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import secrets\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from docutils import core, nodes\n",
    "from docutils.frontend import OptionParser\n",
    "from docutils.utils import new_document\n",
    "from docutils.parsers.rst import Parser\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION AND CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "MAX_CHUNK_TOKENS = 1000\n",
    "TARGET_TOKENS = 600\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA STRUCTURES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class RSTChunk:\n",
    "    \"\"\"Represents a semantic chunk of RST content\"\"\"\n",
    "    start_line: int\n",
    "    end_line: int\n",
    "    content: str\n",
    "    node_type: str\n",
    "    name: str\n",
    "    depth: int\n",
    "    level: int = 0  # For section hierarchy (0=title, 1=section, 2=subsection, etc.)\n",
    "    token_count: int = 0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.token_count == 0:\n",
    "            self.token_count = count_tokens(self.content)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RSTReference:\n",
    "    \"\"\"Represents a reference/include in RST (similar to imports)\"\"\"\n",
    "    reference_type: str  # 'include', 'image', 'figure', 'literalinclude', etc.\n",
    "    target: str\n",
    "    line_number: int\n",
    "    directive: str\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TOKEN COUNTING\n",
    "# =============================================================================\n",
    "\n",
    "def count_tokens(content: str) -> int:\n",
    "    \"\"\"Count tokens using tiktoken for GPT-4\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "        return len(encoding.encode(content))\n",
    "    except Exception:\n",
    "        # Fallback: rough estimation (1 token ≈ 4 characters)\n",
    "        return len(content) // 4\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RST PARSING AND ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def remove_include_directives(rst_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove or comment out include directives to prevent file resolution errors.\n",
    "    This allows parsing to proceed without trying to resolve include files.\n",
    "    \"\"\"\n",
    "    lines = rst_content.split('\\n')\n",
    "    processed_lines = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        \n",
    "        # Check for include directive\n",
    "        if line.strip().startswith('.. include::'):\n",
    "            # Comment out the include directive\n",
    "            processed_lines.append(f\".. # INCLUDE DISABLED: {line.strip()}\")\n",
    "            i += 1\n",
    "            \n",
    "            # Also comment out any options that follow\n",
    "            while i < len(lines) and lines[i].startswith('   :'):\n",
    "                processed_lines.append(f\".. # INCLUDE OPTION: {lines[i].strip()}\")\n",
    "                i += 1\n",
    "        else:\n",
    "            processed_lines.append(line)\n",
    "            i += 1\n",
    "    \n",
    "    return '\\n'.join(processed_lines)\n",
    "\n",
    "\n",
    "def parse_rst_file(file_path: Path) -> Tuple[nodes.document, str]:\n",
    "    \"\"\"Parse RST file using docutils with proper working directory and include handling\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            rst_content = f.read()\n",
    "        \n",
    "        # Method 1: Try parsing in the file's directory context\n",
    "        original_cwd = Path.cwd()\n",
    "        file_dir = file_path.parent\n",
    "        \n",
    "        try:\n",
    "            # Change to file's directory so relative includes can be resolved\n",
    "            import os\n",
    "            os.chdir(file_dir)\n",
    "            \n",
    "            # Use the newer API (fixing deprecation warnings)\n",
    "            from docutils.frontend import get_default_settings\n",
    "            \n",
    "            settings = get_default_settings(Parser)\n",
    "            # Configure settings to be more permissive\n",
    "            settings.report_level = 4  # Only show errors, not warnings\n",
    "            settings.halt_level = 5    # Don't halt on warnings/errors\n",
    "            settings.warning_stream = None  # Suppress warning output\n",
    "            \n",
    "            # Use file path as source path for better error context\n",
    "            source_path = str(file_path)\n",
    "            document = new_document(source_path, settings=settings)\n",
    "            \n",
    "            parser = Parser()\n",
    "            parser.parse(rst_content, document)\n",
    "            \n",
    "            print(f\"✅ Successfully parsed {file_path.name} with includes\")\n",
    "            return document, rst_content\n",
    "            \n",
    "        except Exception as include_error:\n",
    "            print(f\"⚠️ Include resolution failed for {file_path.name}: {include_error}\")\n",
    "            \n",
    "            # Method 2: Try with includes disabled\n",
    "            try:\n",
    "                rst_content_no_includes = remove_include_directives(rst_content)\n",
    "                \n",
    "                settings = get_default_settings(Parser)\n",
    "                settings.report_level = 5  # Suppress all warnings\n",
    "                settings.halt_level = 5    # Don't halt on errors\n",
    "                settings.warning_stream = None\n",
    "                \n",
    "                document = new_document(str(file_path), settings=settings)\n",
    "                parser = Parser()\n",
    "                parser.parse(rst_content_no_includes, document)\n",
    "                \n",
    "                print(f\"✅ Successfully parsed {file_path.name} with includes disabled\")\n",
    "                return document, rst_content  # Return original content, not modified\n",
    "                \n",
    "            except Exception as parse_error:\n",
    "                print(f\"⚠️ Full parsing failed for {file_path.name}: {parse_error}\")\n",
    "                # Return minimal document for manual parsing fallback\n",
    "                settings = get_default_settings(Parser)\n",
    "                document = new_document(str(file_path), settings=settings)\n",
    "                return document, rst_content\n",
    "                \n",
    "        finally:\n",
    "            # Always restore original working directory\n",
    "            os.chdir(original_cwd)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def filter_raw_html(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove raw HTML blocks and directives that are meant for web view.\n",
    "    These are not useful for chunking and RAG purposes.\n",
    "    \"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    processed_lines = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        line_stripped = line.strip()\n",
    "        \n",
    "        # Check for raw HTML directive\n",
    "        if line_stripped.startswith('.. raw:: html'):\n",
    "            # Skip the directive line\n",
    "            i += 1\n",
    "            \n",
    "            # Skip any options (lines starting with spaces and colons)\n",
    "            while i < len(lines) and lines[i].startswith('   :'):\n",
    "                i += 1\n",
    "            \n",
    "            # Skip empty line after options\n",
    "            if i < len(lines) and lines[i].strip() == '':\n",
    "                i += 1\n",
    "            \n",
    "            # Skip the HTML content (indented lines)\n",
    "            while i < len(lines):\n",
    "                if lines[i].strip() == '':\n",
    "                    i += 1\n",
    "                    continue\n",
    "                    \n",
    "                # Check if line is indented (part of the raw HTML block)\n",
    "                if lines[i].startswith('   ') and lines[i].strip():\n",
    "                    i += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    # End of HTML block\n",
    "                    break\n",
    "            \n",
    "            # Add a comment indicating HTML was removed\n",
    "            processed_lines.append(\".. # Raw HTML content removed for chunking\")\n",
    "            continue\n",
    "            \n",
    "        # Check for HTML tags in regular content\n",
    "        elif '<' in line and '>' in line:\n",
    "            # Basic HTML tag detection and removal\n",
    "            import re\n",
    "            # Remove common HTML tags but keep the text content\n",
    "            cleaned_line = re.sub(r'<[^>]+>', '', line)\n",
    "            if cleaned_line.strip():\n",
    "                processed_lines.append(cleaned_line)\n",
    "            else:\n",
    "                processed_lines.append(line)  # Keep original if cleaning removed everything\n",
    "            i += 1\n",
    "        else:\n",
    "            processed_lines.append(line)\n",
    "            i += 1\n",
    "    \n",
    "    return '\\n'.join(processed_lines)\n",
    "def process_content_for_images(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Process content to replace image/figure references with descriptive text\n",
    "    and remove raw HTML content.\n",
    "    \"\"\"\n",
    "    # First remove raw HTML content\n",
    "    content = filter_raw_html(content)\n",
    "    \n",
    "    lines = content.split('\\n')\n",
    "    processed_lines = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        \n",
    "        # Check for image directive\n",
    "        if line.strip().startswith('.. image::'):\n",
    "            # Extract image path\n",
    "            image_path = line.split('::', 1)[1].strip()\n",
    "            \n",
    "            # Look for alt text in following lines\n",
    "            alt_text = None\n",
    "            j = i + 1\n",
    "            while j < len(lines) and (lines[j].startswith('   ') or lines[j].strip() == ''):\n",
    "                if lines[j].strip().startswith(':alt:'):\n",
    "                    alt_text = lines[j].split(':alt:', 1)[1].strip()\n",
    "                    break\n",
    "                elif lines[j].strip().startswith(':alt '):\n",
    "                    alt_text = lines[j].split(':alt ', 1)[1].strip()\n",
    "                    break\n",
    "                j += 1\n",
    "            \n",
    "            # Replace with descriptive text\n",
    "            if alt_text:\n",
    "                processed_lines.append(f\"Image: {alt_text}\")\n",
    "            else:\n",
    "                # Use filename if no alt text\n",
    "                filename = Path(image_path).stem.replace('-', ' ').replace('_', ' ')\n",
    "                processed_lines.append(f\"Image: {filename}\")\n",
    "            \n",
    "            # Skip the image directive and its options\n",
    "            i = j if alt_text else i + 1\n",
    "            continue\n",
    "            \n",
    "        # Check for figure directive\n",
    "        elif line.strip().startswith('.. figure::'):\n",
    "            # Extract figure path \n",
    "            figure_path = line.split('::', 1)[1].strip()\n",
    "            \n",
    "            # Look for alt text and caption in following lines\n",
    "            alt_text = None\n",
    "            caption = None\n",
    "            j = i + 1\n",
    "            \n",
    "            # Skip options (lines starting with spaces and colons)\n",
    "            while j < len(lines) and lines[j].startswith('   :'):\n",
    "                if lines[j].strip().startswith(':alt:'):\n",
    "                    alt_text = lines[j].split(':alt:', 1)[1].strip()\n",
    "                j += 1\n",
    "            \n",
    "            # Caption is the next non-empty, indented line after options\n",
    "            if j < len(lines) and lines[j].startswith('   ') and lines[j].strip():\n",
    "                caption = lines[j].strip()\n",
    "            \n",
    "            # Replace with descriptive text\n",
    "            if caption:\n",
    "                processed_lines.append(f\"Figure: {caption}\")\n",
    "            elif alt_text:\n",
    "                processed_lines.append(f\"Figure: {alt_text}\")\n",
    "            else:\n",
    "                # Use filename if no caption or alt text\n",
    "                filename = Path(figure_path).stem.replace('-', ' ').replace('_', ' ')\n",
    "                processed_lines.append(f\"Figure: {filename}\")\n",
    "            \n",
    "            # Skip the figure directive, options, and caption\n",
    "            i = j + 1 if caption else j\n",
    "            continue\n",
    "        else:\n",
    "            processed_lines.append(line)\n",
    "            i += 1\n",
    "    \n",
    "    return '\\n'.join(processed_lines)\n",
    "def extract_references_from_source(rst_content: str) -> List[RSTReference]:\n",
    "    \"\"\"\n",
    "    Extract references directly from source text instead of relying on AST.\n",
    "    This is more reliable and doesn't depend on successful parsing.\n",
    "    \"\"\"\n",
    "    references = []\n",
    "    lines = rst_content.split('\\n')\n",
    "    \n",
    "    for line_num, line in enumerate(lines, 1):\n",
    "        line_content = line.strip()\n",
    "        \n",
    "        # Detect RST directives that reference external files\n",
    "        if line_content.startswith('.. '):\n",
    "            for ref_type in ['include', 'literalinclude', 'csv-table']:\n",
    "                if f'.. {ref_type}::' in line_content:\n",
    "                    # Extract target from the directive\n",
    "                    parts = line_content.split('::', 1)\n",
    "                    if len(parts) > 1:\n",
    "                        target = parts[1].strip()\n",
    "                        references.append(RSTReference(\n",
    "                            reference_type=ref_type,\n",
    "                            target=target,\n",
    "                            line_number=line_num,\n",
    "                            directive=line_content\n",
    "                        ))\n",
    "                    break\n",
    "    \n",
    "    return references\n",
    "\n",
    "\n",
    "def extract_references(document: nodes.document, source_lines: List[str]) -> List[RSTReference]:\n",
    "    \"\"\"Extract references/includes from RST document (but not images/figures)\"\"\"\n",
    "    # First try AST-based extraction\n",
    "    references = []\n",
    "    \n",
    "    try:\n",
    "        # Use the newer findall method instead of deprecated traverse\n",
    "        for node in document.findall():\n",
    "            line_num = getattr(node, 'line', None)\n",
    "            \n",
    "            # Check for various reference types in the source (excluding images/figures)\n",
    "            if line_num and line_num <= len(source_lines):\n",
    "                line_content = source_lines[line_num - 1].strip()\n",
    "                \n",
    "                # Detect common RST directives that reference external files\n",
    "                # but exclude image and figure directives\n",
    "                if line_content.startswith('.. '):\n",
    "                    for ref_type in ['include', 'literalinclude', 'csv-table']:\n",
    "                        if f'.. {ref_type}::' in line_content:\n",
    "                            # Extract target from the directive\n",
    "                            parts = line_content.split('::', 1)\n",
    "                            if len(parts) > 1:\n",
    "                                target = parts[1].strip()\n",
    "                                references.append(RSTReference(\n",
    "                                    reference_type=ref_type,\n",
    "                                    target=target,\n",
    "                                    line_number=line_num,\n",
    "                                    directive=line_content\n",
    "                                ))\n",
    "                            break\n",
    "    except Exception:\n",
    "        # Fallback to source-based extraction if AST approach fails\n",
    "        pass\n",
    "    \n",
    "    # If AST extraction failed or found nothing, use source-based extraction\n",
    "    if not references:\n",
    "        references = extract_references_from_source('\\n'.join(source_lines))\n",
    "    \n",
    "    return references\n",
    "\n",
    "\n",
    "def get_section_level(node: nodes.section, title_levels: Dict[str, int]) -> int:\n",
    "    \"\"\"Determine section level based on title decoration\"\"\"\n",
    "    title = node[0]  # First child should be title\n",
    "    if isinstance(title, nodes.title):\n",
    "        # Get the raw text and try to find its decoration in source\n",
    "        return len(list(node.traverse(nodes.section, include_self=False)))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def extract_semantic_chunks(document: nodes.document, rst_content: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract semantic chunks from RST document\"\"\"\n",
    "    chunks = []\n",
    "    source_lines = rst_content.split('\\n')\n",
    "    \n",
    "    # Extract document title if present\n",
    "    if document.children and isinstance(document.children[0], nodes.title):\n",
    "        title_node = document.children[0]\n",
    "        title_line = getattr(title_node, 'line', 1)\n",
    "        title_end = title_line + 2  # Usually title + decoration\n",
    "        \n",
    "        title_content = '\\n'.join(source_lines[:title_end])\n",
    "        title_content = process_content_for_images(title_content)\n",
    "        \n",
    "        chunks.append({\n",
    "            'type': 'document_title',\n",
    "            'name': str(title_node.astext()),\n",
    "            'start_line': 1,\n",
    "            'end_line': title_end,\n",
    "            'content': title_content,\n",
    "            'level': 0,\n",
    "            'depth': 0\n",
    "        })\n",
    "    \n",
    "    # Process sections hierarchically\n",
    "    def process_section(section: nodes.section, parent_depth: int = 0):\n",
    "        if not isinstance(section, nodes.section):\n",
    "            return\n",
    "            \n",
    "        title = section[0] if section.children and isinstance(section[0], nodes.title) else None\n",
    "        if not title:\n",
    "            return\n",
    "            \n",
    "        section_start = getattr(title, 'line', 1)\n",
    "        section_name = title.astext()\n",
    "        \n",
    "        # Find section end by looking for next sibling or parent end\n",
    "        section_end = len(source_lines)\n",
    "        for sibling in section.parent.children[section.parent.children.index(section) + 1:]:\n",
    "            if hasattr(sibling, 'line') and sibling.line:\n",
    "                section_end = sibling.line - 1\n",
    "                break\n",
    "        \n",
    "        # Count subsections to determine actual end\n",
    "        subsections = list(section.traverse(nodes.section, include_self=False))\n",
    "        if subsections:\n",
    "            # Section ends where first subsection starts\n",
    "            first_subsection = subsections[0]\n",
    "            if hasattr(first_subsection[0], 'line'):\n",
    "                section_content_end = first_subsection[0].line - 1\n",
    "            else:\n",
    "                section_content_end = section_end\n",
    "        else:\n",
    "            section_content_end = section_end\n",
    "        \n",
    "        # Extract section content (without subsections)\n",
    "        section_content_lines = []\n",
    "        current_line = section_start - 1\n",
    "        \n",
    "        # Add title and content until first subsection\n",
    "        while current_line < min(section_content_end, len(source_lines)):\n",
    "            section_content_lines.append(source_lines[current_line])\n",
    "            current_line += 1\n",
    "        \n",
    "        section_content = '\\n'.join(section_content_lines)\n",
    "        \n",
    "        # Process content to replace images with descriptions\n",
    "        section_content = process_content_for_images(section_content)\n",
    "        \n",
    "        if section_content.strip():\n",
    "            chunks.append({\n",
    "                'type': 'section',\n",
    "                'name': section_name,\n",
    "                'start_line': section_start,\n",
    "                'end_line': section_content_end,\n",
    "                'content': section_content,\n",
    "                'level': parent_depth + 1,\n",
    "                'depth': parent_depth\n",
    "            })\n",
    "        \n",
    "        # Process subsections\n",
    "        for subsection in section.traverse(nodes.section, include_self=False):\n",
    "            if subsection.parent == section:  # Direct child only\n",
    "                process_section(subsection, parent_depth + 1)\n",
    "    \n",
    "    # Process all top-level sections\n",
    "    for section in document.traverse(nodes.section):\n",
    "        if section.parent == document:  # Top-level sections only\n",
    "            process_section(section)\n",
    "    \n",
    "            # Note: Code blocks are NOT extracted separately - they stay within their sections\n",
    "        # This ensures code remains intact within the section context\n",
    "        \n",
    "        # Extract only directives (notes, warnings, etc.) that are standalone\n",
    "        for node in document.traverse():\n",
    "            line_num = getattr(node, 'line', None)\n",
    "            if not line_num:\n",
    "                continue\n",
    "                \n",
    "            # Directives (notes, warnings, etc.) - only if not within a section\n",
    "            if isinstance(node, nodes.Admonition):\n",
    "                # Check if this admonition is within a section\n",
    "                parent_section = None\n",
    "                for ancestor in node.traverse(include_self=False, descend=False):\n",
    "                    if isinstance(ancestor, nodes.section):\n",
    "                        parent_section = ancestor\n",
    "                        break\n",
    "                \n",
    "                # Only create separate chunk if not within a section\n",
    "                if not parent_section:\n",
    "                    admonition_text = node.astext()\n",
    "                    admonition_type = node.tagname if hasattr(node, 'tagname') else 'admonition'\n",
    "                    chunks.append({\n",
    "                        'type': f'{admonition_type}_directive',\n",
    "                        'name': f'{admonition_type}_line_{line_num}',\n",
    "                        'start_line': line_num,\n",
    "                        'end_line': line_num + admonition_text.count('\\n'),\n",
    "                        'content': admonition_text,\n",
    "                        'level': 8,\n",
    "                        'depth': 0\n",
    "                    })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CHUNK CREATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_rst_chunks(semantic_nodes: List[Dict[str, Any]]) -> List[RSTChunk]:\n",
    "    \"\"\"Create RSTChunk objects from semantic nodes\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    for node_info in semantic_nodes:\n",
    "        chunk = RSTChunk(\n",
    "            start_line=node_info['start_line'],\n",
    "            end_line=node_info['end_line'],\n",
    "            content=node_info['content'],\n",
    "            node_type=node_info['type'],\n",
    "            name=node_info['name'],\n",
    "            depth=node_info.get('depth', 0),\n",
    "            level=node_info.get('level', 0)\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def group_small_chunks(chunks: List[RSTChunk], target_tokens: int = TARGET_TOKENS) -> List[RSTChunk]:\n",
    "    \"\"\"Group small chunks together aggressively to reach reasonable size\"\"\"\n",
    "    if not chunks:\n",
    "        return chunks\n",
    "    \n",
    "    print(f\"🔄 Grouping {len(chunks)} chunks (target: {target_tokens} tokens)\")\n",
    "    \n",
    "    # Check if everything together is under the max limit\n",
    "    total_tokens = sum(c.token_count for c in chunks)\n",
    "    if total_tokens <= MAX_CHUNK_TOKENS:\n",
    "        # Combine everything into one chunk\n",
    "        combined_content = '\\n\\n'.join(c.content for c in chunks)\n",
    "        combined_chunk = RSTChunk(\n",
    "            start_line=chunks[0].start_line,\n",
    "            end_line=chunks[-1].end_line,\n",
    "            content=combined_content,\n",
    "            node_type='complete_document',\n",
    "            name=f\"complete_document_{len(chunks)}_parts\",\n",
    "            depth=0,\n",
    "            level=0\n",
    "        )\n",
    "        print(f\"✅ Combined all {len(chunks)} chunks into 1 complete document ({total_tokens} tokens)\")\n",
    "        return [combined_chunk]\n",
    "    \n",
    "    # Group chunks more aggressively - aim for larger chunks\n",
    "    grouped_chunks = []\n",
    "    current_group = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    # Sort chunks by level to group similar hierarchy levels together\n",
    "    sorted_chunks = sorted(chunks, key=lambda c: (c.level, c.start_line))\n",
    "    \n",
    "    for chunk in sorted_chunks:\n",
    "        # More aggressive grouping - use higher threshold\n",
    "        can_add = (current_tokens + chunk.token_count <= MAX_CHUNK_TOKENS)\n",
    "        should_group = (current_tokens + chunk.token_count <= target_tokens * 1.5)  # 1.5x target\n",
    "        \n",
    "        if can_add and (not current_group or should_group):\n",
    "            current_group.append(chunk)\n",
    "            current_tokens += chunk.token_count\n",
    "        else:\n",
    "            # Finalize current group if it has content\n",
    "            if current_group:\n",
    "                if len(current_group) == 1:\n",
    "                    grouped_chunks.append(current_group[0])\n",
    "                else:\n",
    "                    # Create grouped chunk\n",
    "                    group_content = '\\n\\n'.join(c.content for c in current_group)\n",
    "                    \n",
    "                    # Better naming based on content types\n",
    "                    group_types = list(set(c.node_type for c in current_group))\n",
    "                    if len(group_types) == 1:\n",
    "                        group_name = f\"{group_types[0]}_group_{len(current_group)}_parts\"\n",
    "                    else:\n",
    "                        group_name = f\"mixed_content_{len(current_group)}_parts\"\n",
    "                    \n",
    "                    # Use the earliest chunk's position info\n",
    "                    earliest_chunk = min(current_group, key=lambda c: c.start_line)\n",
    "                    latest_chunk = max(current_group, key=lambda c: c.end_line)\n",
    "                    \n",
    "                    grouped_chunk = RSTChunk(\n",
    "                        start_line=earliest_chunk.start_line,\n",
    "                        end_line=latest_chunk.end_line,\n",
    "                        content=group_content,\n",
    "                        node_type='grouped_content',\n",
    "                        name=group_name,\n",
    "                        depth=min(c.depth for c in current_group),  # Use minimum depth\n",
    "                        level=min(c.level for c in current_group)   # Use minimum level\n",
    "                    )\n",
    "                    grouped_chunks.append(grouped_chunk)\n",
    "                \n",
    "                print(f\"  📦 Grouped {len(current_group)} chunks → {current_tokens} tokens\")\n",
    "            \n",
    "            # Start new group with current chunk\n",
    "            current_group = [chunk]\n",
    "            current_tokens = chunk.token_count\n",
    "    \n",
    "    # Add final group\n",
    "    if current_group:\n",
    "        if len(current_group) == 1:\n",
    "            grouped_chunks.append(current_group[0])\n",
    "        else:\n",
    "            group_content = '\\n\\n'.join(c.content for c in current_group)\n",
    "            \n",
    "            # Better naming\n",
    "            group_types = list(set(c.node_type for c in current_group))\n",
    "            if len(group_types) == 1:\n",
    "                group_name = f\"{group_types[0]}_group_{len(current_group)}_parts\"\n",
    "            else:\n",
    "                group_name = f\"mixed_content_{len(current_group)}_parts\"\n",
    "            \n",
    "            earliest_chunk = min(current_group, key=lambda c: c.start_line)\n",
    "            latest_chunk = max(current_group, key=lambda c: c.end_line)\n",
    "            \n",
    "            grouped_chunk = RSTChunk(\n",
    "                start_line=earliest_chunk.start_line,\n",
    "                end_line=latest_chunk.end_line,\n",
    "                content=group_content,\n",
    "                node_type='grouped_content',\n",
    "                name=group_name,\n",
    "                depth=min(c.depth for c in current_group),\n",
    "                level=min(c.level for c in current_group)\n",
    "            )\n",
    "            grouped_chunks.append(grouped_chunk)\n",
    "            \n",
    "        print(f\"  📦 Final group: {len(current_group)} chunks → {current_tokens} tokens\")\n",
    "    \n",
    "    print(f\"✅ Grouping result: {len(chunks)} → {len(grouped_chunks)} chunks\")\n",
    "    \n",
    "    # Show final chunk sizes\n",
    "    for i, chunk in enumerate(grouped_chunks):\n",
    "        print(f\"    Chunk {i+1}: {chunk.token_count} tokens ({chunk.name})\")\n",
    "    \n",
    "    return grouped_chunks\n",
    "\n",
    "\n",
    "def analyze_code_blocks_in_content(content: str) -> List[Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Analyze code blocks in content to identify their boundaries.\n",
    "    Returns list of code block locations with start/end line numbers.\n",
    "    \"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    code_blocks = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        # Check for code-block directive\n",
    "        if line.startswith('.. code-block::') or line.startswith('.. literalinclude::'):\n",
    "            start_line = i\n",
    "            i += 1\n",
    "            \n",
    "            # Skip options (lines starting with spaces and colons)\n",
    "            while i < len(lines) and lines[i].startswith('   :'):\n",
    "                i += 1\n",
    "            \n",
    "            # Skip empty line after options\n",
    "            if i < len(lines) and lines[i].strip() == '':\n",
    "                i += 1\n",
    "            \n",
    "            # Find end of code block (when indentation decreases)\n",
    "            block_indent = None\n",
    "            end_line = i\n",
    "            \n",
    "            while i < len(lines):\n",
    "                if lines[i].strip() == '':\n",
    "                    i += 1\n",
    "                    continue\n",
    "                    \n",
    "                # Check indentation\n",
    "                current_indent = len(lines[i]) - len(lines[i].lstrip())\n",
    "                \n",
    "                if block_indent is None:\n",
    "                    if current_indent > 0:\n",
    "                        block_indent = current_indent\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    if current_indent < block_indent and lines[i].strip():\n",
    "                        break\n",
    "                    i += 1\n",
    "            \n",
    "            end_line = i - 1\n",
    "            code_blocks.append({\n",
    "                'start': start_line,\n",
    "                'end': end_line,\n",
    "                'type': 'directive'\n",
    "            })\n",
    "            continue\n",
    "            \n",
    "        # Check for literal blocks (double colon)\n",
    "        elif line.endswith('::') and not line.startswith('.. '):\n",
    "            start_line = i\n",
    "            i += 1\n",
    "            \n",
    "            # Skip empty line after ::\n",
    "            if i < len(lines) and lines[i].strip() == '':\n",
    "                i += 1\n",
    "            \n",
    "            # Find end of literal block\n",
    "            block_indent = None\n",
    "            end_line = i\n",
    "            \n",
    "            while i < len(lines):\n",
    "                if lines[i].strip() == '':\n",
    "                    i += 1\n",
    "                    continue\n",
    "                    \n",
    "                current_indent = len(lines[i]) - len(lines[i].lstrip())\n",
    "                \n",
    "                if block_indent is None:\n",
    "                    if current_indent > 0:\n",
    "                        block_indent = current_indent\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    if current_indent < block_indent and lines[i].strip():\n",
    "                        break\n",
    "                    i += 1\n",
    "            \n",
    "            end_line = i - 1\n",
    "            code_blocks.append({\n",
    "                'start': start_line,\n",
    "                'end': end_line,\n",
    "                'type': 'literal'\n",
    "            })\n",
    "            continue\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "    return code_blocks\n",
    "\n",
    "\n",
    "def split_content_preserving_code_blocks(content: str, max_tokens: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split content into chunks while preserving code block integrity.\n",
    "    If a code block is too large, it gets its own chunk.\n",
    "    \"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    code_blocks = analyze_code_blocks_in_content(content)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk_lines = []\n",
    "    current_tokens = 0\n",
    "    line_idx = 0\n",
    "    \n",
    "    while line_idx < len(lines):\n",
    "        # Check if current line is start of a code block\n",
    "        current_code_block = None\n",
    "        for cb in code_blocks:\n",
    "            if cb['start'] == line_idx:\n",
    "                current_code_block = cb\n",
    "                break\n",
    "        \n",
    "        if current_code_block:\n",
    "            # We're at the start of a code block\n",
    "            code_block_lines = lines[current_code_block['start']:current_code_block['end'] + 1]\n",
    "            code_block_content = '\\n'.join(code_block_lines)\n",
    "            code_block_tokens = count_tokens(code_block_content)\n",
    "            \n",
    "            # If code block + current chunk would exceed limit, finalize current chunk\n",
    "            if current_tokens + code_block_tokens > max_tokens and current_chunk_lines:\n",
    "                chunks.append('\\n'.join(current_chunk_lines))\n",
    "                current_chunk_lines = []\n",
    "                current_tokens = 0\n",
    "            \n",
    "            # If code block itself is too large, give it its own chunk\n",
    "            if code_block_tokens > max_tokens:\n",
    "                # Finalize current chunk if it has content\n",
    "                if current_chunk_lines:\n",
    "                    chunks.append('\\n'.join(current_chunk_lines))\n",
    "                    current_chunk_lines = []\n",
    "                    current_tokens = 0\n",
    "                \n",
    "                # Code block gets its own chunk\n",
    "                chunks.append(code_block_content)\n",
    "            else:\n",
    "                # Add code block to current chunk\n",
    "                current_chunk_lines.extend(code_block_lines)\n",
    "                current_tokens += code_block_tokens\n",
    "            \n",
    "            # Move past the code block\n",
    "            line_idx = current_code_block['end'] + 1\n",
    "        else:\n",
    "            # Regular line - add if it fits\n",
    "            line = lines[line_idx]\n",
    "            line_tokens = count_tokens(line)\n",
    "            \n",
    "            if current_tokens + line_tokens > max_tokens and current_chunk_lines:\n",
    "                # Finalize current chunk\n",
    "                chunks.append('\\n'.join(current_chunk_lines))\n",
    "                current_chunk_lines = [line]\n",
    "                current_tokens = line_tokens\n",
    "            else:\n",
    "                current_chunk_lines.append(line)\n",
    "                current_tokens += line_tokens\n",
    "            \n",
    "            line_idx += 1\n",
    "    \n",
    "    # Add final chunk\n",
    "    if current_chunk_lines:\n",
    "        chunks.append('\\n'.join(current_chunk_lines))\n",
    "    \n",
    "    return chunks\n",
    "def sub_chunk_by_lines(chunk: RSTChunk, rst_content: str) -> List[RSTChunk]:\n",
    "    \"\"\"Sub-chunk oversized chunks while preserving code block integrity\"\"\"\n",
    "    if chunk.token_count <= MAX_CHUNK_TOKENS:\n",
    "        return [chunk]\n",
    "    \n",
    "    # Extract the chunk's content from the full document\n",
    "    lines = rst_content.split('\\n')\n",
    "    chunk_lines = lines[chunk.start_line-1:chunk.end_line]\n",
    "    chunk_content = '\\n'.join(chunk_lines)\n",
    "    \n",
    "    # Split content preserving code blocks\n",
    "    sub_contents = split_content_preserving_code_blocks(chunk_content, MAX_CHUNK_TOKENS)\n",
    "    \n",
    "    if len(sub_contents) <= 1:\n",
    "        return [chunk]  # Couldn't split effectively\n",
    "    \n",
    "    sub_chunks = []\n",
    "    lines_processed = 0\n",
    "    \n",
    "    for i, sub_content in enumerate(sub_contents):\n",
    "        sub_lines = sub_content.split('\\n')\n",
    "        sub_start_line = chunk.start_line + lines_processed\n",
    "        sub_end_line = sub_start_line + len(sub_lines) - 1\n",
    "        \n",
    "        sub_chunk = RSTChunk(\n",
    "            start_line=sub_start_line,\n",
    "            end_line=sub_end_line,\n",
    "            content=sub_content,\n",
    "            node_type=f\"{chunk.node_type}_part\",\n",
    "            name=f\"{chunk.name}_part_{i+1}\",\n",
    "            depth=chunk.depth + 1,\n",
    "            level=chunk.level\n",
    "        )\n",
    "        sub_chunks.append(sub_chunk)\n",
    "        \n",
    "        lines_processed += len(sub_lines)\n",
    "    \n",
    "    return sub_chunks\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FILE PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def process_rst_file(file_path: Path) -> List[RSTChunk]:\n",
    "    \"\"\"Process a single RST file and return chunks\"\"\"\n",
    "    print(f\"\\n🔍 Processing: {file_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Parse RST file\n",
    "        document, rst_content = parse_rst_file(file_path)\n",
    "        \n",
    "        if document.children:\n",
    "            print(f\"✅ Successfully parsed RST structure\")\n",
    "        else:\n",
    "            print(\"⚠️ Empty document\")\n",
    "            return []\n",
    "        \n",
    "        # Extract references (similar to imports)\n",
    "        references = extract_references(document, rst_content.split('\\n'))\n",
    "        if references:\n",
    "            print(f\"📎 Found {len(references)} references/includes\")\n",
    "            for ref in references:\n",
    "                print(f\"  - {ref.reference_type}: {ref.target}\")\n",
    "        \n",
    "        # Find semantic chunks\n",
    "        semantic_nodes = extract_semantic_chunks(document, rst_content)\n",
    "        print(f\"Found {len(semantic_nodes)} semantic units\")\n",
    "        \n",
    "        # Show what we found\n",
    "        for node in semantic_nodes:\n",
    "            preview = node['content'][:100].replace('\\n', ' ').strip()\n",
    "            print(f\"  - {node['type']}: {node['name']} ({count_tokens(node['content'])} tokens)\")\n",
    "            print(f\"    Preview: {preview}...\")\n",
    "        \n",
    "        # Create chunks\n",
    "        base_chunks = create_rst_chunks(semantic_nodes)\n",
    "        \n",
    "        # Group small chunks\n",
    "        base_chunks = group_small_chunks(base_chunks, target_tokens=TARGET_TOKENS)\n",
    "        print(f\"Created {len(base_chunks)} semantic chunks\")\n",
    "        \n",
    "        # Apply sub-chunking for oversized chunks\n",
    "        final_chunks = []\n",
    "        oversized_count = 0\n",
    "        \n",
    "        for chunk in base_chunks:\n",
    "            if chunk.token_count > MAX_CHUNK_TOKENS:\n",
    "                print(f\"  Sub-chunking {chunk.name} ({chunk.token_count} tokens)\")\n",
    "                sub_chunks = sub_chunk_by_lines(chunk, rst_content)\n",
    "                final_chunks.extend(sub_chunks)\n",
    "                oversized_count += 1\n",
    "            else:\n",
    "                final_chunks.append(chunk)\n",
    "        \n",
    "        if oversized_count > 0:\n",
    "            print(f\"Sub-chunked {oversized_count} oversized chunks\")\n",
    "        print(f\"Final result: {len(final_chunks)} total chunks\")\n",
    "        \n",
    "        return final_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_unique_id(length: int = 6) -> str:\n",
    "    \"\"\"Generate a random unique ID\"\"\"\n",
    "    alphabet = string.ascii_lowercase + string.digits\n",
    "    return ''.join(secrets.choice(alphabet) for _ in range(length))\n",
    "\n",
    "\n",
    "def create_chunk_filename(original_filename: str, chunk_number: int, unique_id: str) -> str:\n",
    "    \"\"\"Create chunk filename: index.rst_chunk_001_a1s2d3.md\"\"\"\n",
    "    return f\"{original_filename}_chunk_{chunk_number:03d}_{unique_id}.md\"\n",
    "\n",
    "\n",
    "def create_chunk_markdown(chunk: RSTChunk, source_file_path: str, references: List[RSTReference]) -> str:\n",
    "    \"\"\"Create markdown content with YAML frontmatter\"\"\"\n",
    "    unique_id = generate_unique_id()\n",
    "    \n",
    "    # Filter references that might apply to this chunk\n",
    "    chunk_references = []\n",
    "    for ref in references:\n",
    "        if chunk.start_line <= ref.line_number <= chunk.end_line:\n",
    "            chunk_references.append(f\"{ref.reference_type}: {ref.target}\")\n",
    "    \n",
    "    frontmatter = f\"\"\"---\n",
    "file_path: \"{source_file_path}\"\n",
    "chunk_id: \"{unique_id}\"\n",
    "chunk_type: \"{chunk.node_type}\"\n",
    "chunk_name: \"{chunk.name}\"\n",
    "start_line: {chunk.start_line}\n",
    "end_line: {chunk.end_line}\n",
    "token_count: {chunk.token_count}\n",
    "depth: {chunk.depth}\n",
    "level: {chunk.level}\n",
    "language: \"rst\"\n",
    "references: {chunk_references}\n",
    "---\n",
    "\n",
    "# {chunk.name}\n",
    "\n",
    "**Type:** {chunk.node_type}  \n",
    "**Tokens:** {chunk.token_count}  \n",
    "**Depth:** {chunk.depth}  \n",
    "**Level:** {chunk.level}\n",
    "\n",
    "```rst\n",
    "{chunk.content}\n",
    "```\n",
    "\"\"\"\n",
    "    return frontmatter\n",
    "\n",
    "\n",
    "def save_chunks_to_files(chunks: List[RSTChunk], \n",
    "                        original_file_path: Path, \n",
    "                        input_directory: Path,\n",
    "                        output_base: Path,\n",
    "                        references: List[RSTReference]) -> List[str]:\n",
    "    \"\"\"Save chunks as markdown files maintaining directory structure\"\"\"\n",
    "    if not chunks:\n",
    "        return []\n",
    "    \n",
    "    # Calculate relative path from input directory\n",
    "    try:\n",
    "        rel_path = original_file_path.relative_to(input_directory)\n",
    "    except ValueError:\n",
    "        # If file is not under input directory, use just the filename\n",
    "        rel_path = original_file_path.name\n",
    "    \n",
    "    # Create output directory structure\n",
    "    output_dir = output_base / rel_path.parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate unique ID for this file\n",
    "    file_unique_id = generate_unique_id()\n",
    "    \n",
    "    saved_files = []\n",
    "    \n",
    "    # Add chunk index to each chunk and save\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        # Create chunk filename\n",
    "        chunk_filename = create_chunk_filename(\n",
    "            original_file_path.name, \n",
    "            i, \n",
    "            file_unique_id\n",
    "        )\n",
    "        \n",
    "        # Create markdown content\n",
    "        markdown_content = create_chunk_markdown(\n",
    "            chunk, \n",
    "            str(rel_path), \n",
    "            references\n",
    "        )\n",
    "        \n",
    "        # Write to file\n",
    "        chunk_file_path = output_dir / chunk_filename\n",
    "        try:\n",
    "            with open(chunk_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "            \n",
    "            saved_files.append(str(chunk_file_path))\n",
    "            print(f\"  ✅ Saved: {chunk_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error saving {chunk_filename}: {e}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "\n",
    "def print_chunk_summary(chunks: List[RSTChunk], file_name: str):\n",
    "    \"\"\"Print detailed summary of chunks\"\"\"\n",
    "    print(f\"\\n--- RST Chunk Summary for {file_name} ---\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        indent = \"  \" * chunk.depth\n",
    "        content_lines = len(chunk.content.split('\\n'))\n",
    "        \n",
    "        print(f\"{indent}{i}. {chunk.name}\")\n",
    "        print(f\"{indent}   Type: {chunk.node_type} | Level: {chunk.level} | Lines: {content_lines} | Tokens: {chunk.token_count}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# JUPYTER NOTEBOOK FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def process_rst_content(rst_content: str, file_name: str = \"content.rst\") -> List[RSTChunk]:\n",
    "    \"\"\"\n",
    "    Process RST content directly (for Jupyter notebooks).\n",
    "    \n",
    "    Args:\n",
    "        rst_content: Raw RST content as string\n",
    "        file_name: Name to use for the content (for display purposes)\n",
    "    \n",
    "    Returns:\n",
    "        List of RSTChunk objects\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the approach you suggested: pre-process to remove includes\n",
    "        rst_input = remove_include_directives(rst_content)\n",
    "        \n",
    "        # Parse with docutils using the pattern you mentioned\n",
    "        from docutils.frontend import get_default_settings\n",
    "        \n",
    "        settings = get_default_settings(Parser)\n",
    "        settings.report_level = 4  # Show errors but not warnings\n",
    "        settings.halt_level = 5    # Don't halt on errors\n",
    "        settings.warning_stream = None  # Suppress warnings\n",
    "        \n",
    "        document = new_document(file_name, settings=settings)\n",
    "        parser = Parser()\n",
    "        \n",
    "        try:\n",
    "            parser.parse(rst_input, document)\n",
    "            print(f\"✅ Successfully parsed RST structure for {file_name}\")\n",
    "        except Exception as parse_error:\n",
    "            print(f\"⚠️ Parse error in {file_name}: {parse_error}\")\n",
    "            # Continue with whatever was parsed\n",
    "        \n",
    "        if not document.children:\n",
    "            print(f\"⚠️ Empty document: {file_name}\")\n",
    "            return []\n",
    "        \n",
    "        # Extract references from original content (not the modified one)\n",
    "        references = extract_references_from_source(rst_content)\n",
    "        if references:\n",
    "            print(f\"📎 Found {len(references)} references/includes\")\n",
    "            for ref in references:\n",
    "                print(f\"  - {ref.reference_type}: {ref.target}\")\n",
    "        \n",
    "        # Find semantic chunks\n",
    "        semantic_nodes = extract_semantic_chunks(document, rst_content)\n",
    "        print(f\"Found {len(semantic_nodes)} semantic units\")\n",
    "        \n",
    "        # Show what we found\n",
    "        for node in semantic_nodes:\n",
    "            preview = node['content'][:100].replace('\\n', ' ').strip()\n",
    "            print(f\"  - {node['type']}: {node['name']} ({count_tokens(node['content'])} tokens)\")\n",
    "            print(f\"    Preview: {preview}...\")\n",
    "        \n",
    "        # Create chunks\n",
    "        base_chunks = create_rst_chunks(semantic_nodes)\n",
    "        \n",
    "        # Group small chunks\n",
    "        base_chunks = group_small_chunks(base_chunks, target_tokens=TARGET_TOKENS)\n",
    "        print(f\"Created {len(base_chunks)} semantic chunks\")\n",
    "        \n",
    "        # Apply sub-chunking for oversized chunks\n",
    "        final_chunks = []\n",
    "        oversized_count = 0\n",
    "        \n",
    "        for chunk in base_chunks:\n",
    "            if chunk.token_count > MAX_CHUNK_TOKENS:\n",
    "                print(f\"  Sub-chunking {chunk.name} ({chunk.token_count} tokens)\")\n",
    "                sub_chunks = sub_chunk_by_lines(chunk, rst_content)\n",
    "                final_chunks.extend(sub_chunks)\n",
    "                oversized_count += 1\n",
    "            else:\n",
    "                final_chunks.append(chunk)\n",
    "        \n",
    "        if oversized_count > 0:\n",
    "            print(f\"Sub-chunked {oversized_count} oversized chunks\")\n",
    "        print(f\"Final result: {len(final_chunks)} total chunks\")\n",
    "        \n",
    "        return final_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing RST content: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def display_chunks(chunks: List[RSTChunk]) -> None:\n",
    "    \"\"\"Display chunks in a notebook-friendly format\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"📄 RST CHUNKS SUMMARY ({len(chunks)} chunks)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    total_tokens = sum(chunk.token_count for chunk in chunks)\n",
    "    print(f\"📊 Total tokens: {total_tokens:,}\")\n",
    "    print(f\"📊 Average tokens per chunk: {total_tokens/len(chunks):.1f}\" if chunks else \"No chunks\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        indent = \"  \" * chunk.depth\n",
    "        content_lines = len(chunk.content.split('\\n'))\n",
    "        \n",
    "        print(f\"\\n{indent}📝 Chunk {i}: {chunk.name}\")\n",
    "        print(f\"{indent}   Type: {chunk.node_type} | Level: {chunk.level} | Lines: {content_lines} | Tokens: {chunk.token_count}\")\n",
    "        \n",
    "        # Show content preview\n",
    "        preview = chunk.content[:200].replace('\\n', ' ').strip()\n",
    "        if len(chunk.content) > 200:\n",
    "            preview += \"...\"\n",
    "        print(f\"{indent}   Preview: {preview}\")\n",
    "\n",
    "\n",
    "def save_chunks_as_markdown(chunks: List[RSTChunk], output_dir: str = \"rst_chunks\") -> None:\n",
    "    \"\"\"Save chunks as markdown files (notebook version)\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    file_unique_id = generate_unique_id()\n",
    "    saved_count = 0\n",
    "    \n",
    "    print(f\"\\n💾 Saving {len(chunks)} chunks to {output_path}/\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        # Create chunk filename\n",
    "        chunk_filename = f\"chunk_{i:03d}_{file_unique_id}.md\"\n",
    "        \n",
    "        # Create markdown content\n",
    "        markdown_content = create_chunk_markdown(chunk, \"notebook_content.rst\", [])\n",
    "        \n",
    "        # Write to file\n",
    "        chunk_file_path = output_path / chunk_filename\n",
    "        try:\n",
    "            with open(chunk_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "            \n",
    "            print(f\"  ✅ Saved: {chunk_filename}\")\n",
    "            saved_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error saving {chunk_filename}: {e}\")\n",
    "    \n",
    "    print(f\"✅ Successfully saved {saved_count} chunk files\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for RST semantic chunking\"\"\"\n",
    "    print(\"🚀 RST (reStructuredText) Semantic Chunking\")\n",
    "    print(f\"Max chunk tokens: {MAX_CHUNK_TOKENS}\")\n",
    "    print(f\"Target tokens for grouping: {TARGET_TOKENS}\")\n",
    "    \n",
    "    # Get directory from user or use current directory\n",
    "    directory = input(\"\\nEnter source directory path (or press Enter for current directory): \").strip()\n",
    "    if not directory:\n",
    "        directory = \".\"\n",
    "    \n",
    "    target_dir = Path(directory).resolve()\n",
    "    if not target_dir.exists():\n",
    "        print(f\"❌ Directory not found: {directory}\")\n",
    "        return\n",
    "    \n",
    "    # Create output directory parallel to source directory\n",
    "    output_dir = target_dir.parent / f\"{target_dir.name}_rst_chunks\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    print(f\"📁 Output directory: {output_dir}\")\n",
    "    \n",
    "    target_path = target_dir\n",
    "    input_directory = target_dir\n",
    "    \n",
    "    \n",
    "    # Collect RST files\n",
    "    rst_files = []\n",
    "    for ext in ['*.rst', '*.txt']:\n",
    "        rst_files.extend(target_path.rglob(ext))\n",
    "    \n",
    "    # Filter to actual RST files by checking content\n",
    "    actual_rst_files = []\n",
    "    for file in rst_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read(1000)  # Check first 1000 chars\n",
    "                # Simple heuristic: look for RST-like content\n",
    "                if any(marker in content for marker in ['===', '---', '~~~', '^^^', '.. ', '::']):\n",
    "                    actual_rst_files.append(file)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    rst_files = actual_rst_files\n",
    "    \n",
    "    if not rst_files:\n",
    "        print(f\"❌ No RST files found in {directory}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"🔍 Found {len(rst_files)} RST file(s)\")\n",
    "    \n",
    "    # Group by directory for display\n",
    "    by_dir = {}\n",
    "    for f in rst_files:\n",
    "        dir_path = str(f.parent.relative_to(input_directory)) if f.parent != input_directory else '.'\n",
    "        by_dir[dir_path] = by_dir.get(dir_path, []) + [f.name]\n",
    "    \n",
    "    for dir_path, files in sorted(by_dir.items()):\n",
    "        print(f\"  📂 {dir_path}: {len(files)} files\")\n",
    "        for file_name in sorted(files)[:3]:  # Show first 3 files\n",
    "            print(f\"    📄 {file_name}\")\n",
    "        if len(files) > 3:\n",
    "            print(f\"    ... and {len(files) - 3} more\")\n",
    "    \n",
    "    # Process all files automatically\n",
    "    print(f\"\\n🔄 Processing all {len(rst_files)} file(s)...\")\n",
    "    all_chunks = {}\n",
    "    all_references = {}\n",
    "    \n",
    "    for file_path in rst_files:\n",
    "        try:\n",
    "            chunks = process_rst_file(file_path)\n",
    "            all_chunks[file_path] = chunks\n",
    "            \n",
    "            # Extract references for this file\n",
    "            document, rst_content = parse_rst_file(file_path)\n",
    "            references = extract_references(document, rst_content.split('\\n'))\n",
    "            all_references[file_path] = references\n",
    "            \n",
    "            print_chunk_summary(chunks, file_path.name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Summary\n",
    "    total_chunks = sum(len(chunks) for chunks in all_chunks.values())\n",
    "    total_tokens = sum(chunk.token_count for chunks in all_chunks.values() for chunk in chunks)\n",
    "    \n",
    "    print(f\"\\n📊 Processing Summary:\")\n",
    "    print(f\"   Files processed: {len(all_chunks)}\")\n",
    "    print(f\"   Total chunks: {total_chunks}\")\n",
    "    print(f\"   Total tokens: {total_tokens:,}\")\n",
    "    print(f\"   Average tokens per chunk: {total_tokens/total_chunks:.1f}\" if total_chunks > 0 else \"   No chunks created\")\n",
    "    \n",
    "    # Save chunks automatically with parallel directory structure\n",
    "    print(f\"\\n💾 Saving chunks to markdown files...\")\n",
    "    saved_count = 0\n",
    "    \n",
    "    for file_path, chunks in all_chunks.items():\n",
    "        references = all_references.get(file_path, [])\n",
    "        saved_files = save_chunks_to_files(chunks, file_path, input_directory, output_dir, references)\n",
    "        saved_count += len(saved_files)\n",
    "    \n",
    "    print(f\"✅ Saved {saved_count} chunk files to {output_dir}\")\n",
    "    print(f\"📁 Directory structure preserved in output\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "RST (reStructuredText) AST Chunker\n",
    "\n",
    "This module implements semantic chunking for RST files using docutils for parsing.\n",
    "It follows the same patterns as the existing Python and TypeScript AST chunkers.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import secrets\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from docutils import core, nodes\n",
    "from docutils.frontend import OptionParser\n",
    "from docutils.utils import new_document\n",
    "from docutils.parsers.rst import Parser\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION AND CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "MAX_CHUNK_TOKENS = 1000\n",
    "TARGET_TOKENS = 600\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA STRUCTURES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class RSTChunk:\n",
    "    \"\"\"Represents a semantic chunk of RST content\"\"\"\n",
    "    start_line: int\n",
    "    end_line: int\n",
    "    content: str\n",
    "    node_type: str\n",
    "    name: str\n",
    "    depth: int\n",
    "    level: int = 0  # For section hierarchy (0=title, 1=section, 2=subsection, etc.)\n",
    "    token_count: int = 0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.token_count == 0:\n",
    "            self.token_count = count_tokens(self.content)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RSTReference:\n",
    "    \"\"\"Represents a reference/include in RST (similar to imports)\"\"\"\n",
    "    reference_type: str  # 'include', 'image', 'figure', 'literalinclude', etc.\n",
    "    target: str\n",
    "    line_number: int\n",
    "    directive: str\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TOKEN COUNTING\n",
    "# =============================================================================\n",
    "\n",
    "def count_tokens(content: str) -> int:\n",
    "    \"\"\"Count tokens using tiktoken for GPT-4\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "        return len(encoding.encode(content))\n",
    "    except Exception:\n",
    "        # Fallback: rough estimation (1 token ≈ 4 characters)\n",
    "        return len(content) // 4\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RST PARSING AND ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def remove_include_directives(rst_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove or comment out include directives to prevent file resolution errors.\n",
    "    This allows parsing to proceed without trying to resolve include files.\n",
    "    \"\"\"\n",
    "    lines = rst_content.split('\\n')\n",
    "    processed_lines = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        \n",
    "        # Check for include directive\n",
    "        if line.strip().startswith('.. include::'):\n",
    "            # Comment out the include directive\n",
    "            processed_lines.append(f\".. # INCLUDE DISABLED: {line.strip()}\")\n",
    "            i += 1\n",
    "            \n",
    "            # Also comment out any options that follow\n",
    "            while i < len(lines) and lines[i].startswith('   :'):\n",
    "                processed_lines.append(f\".. # INCLUDE OPTION: {lines[i].strip()}\")\n",
    "                i += 1\n",
    "        else:\n",
    "            processed_lines.append(line)\n",
    "            i += 1\n",
    "    \n",
    "    return '\\n'.join(processed_lines)\n",
    "\n",
    "\n",
    "def parse_rst_file(file_path: Path) -> Tuple[nodes.document, str]:\n",
    "    \"\"\"Parse RST file using docutils with proper working directory and include handling\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            rst_content = f.read()\n",
    "        \n",
    "        # Method 1: Try parsing in the file's directory context\n",
    "        original_cwd = Path.cwd()\n",
    "        file_dir = file_path.parent\n",
    "        \n",
    "        try:\n",
    "            # Change to file's directory so relative includes can be resolved\n",
    "            import os\n",
    "            os.chdir(file_dir)\n",
    "            \n",
    "            # Use the newer API (fixing deprecation warnings)\n",
    "            from docutils.frontend import get_default_settings\n",
    "            \n",
    "            settings = get_default_settings(Parser)\n",
    "            # Configure settings to be more permissive\n",
    "            settings.report_level = 4  # Only show errors, not warnings\n",
    "            settings.halt_level = 5    # Don't halt on warnings/errors\n",
    "            settings.warning_stream = None  # Suppress warning output\n",
    "            \n",
    "            # Use file path as source path for better error context\n",
    "            source_path = str(file_path)\n",
    "            document = new_document(source_path, settings=settings)\n",
    "            \n",
    "            parser = Parser()\n",
    "            parser.parse(rst_content, document)\n",
    "            \n",
    "            print(f\"✅ Successfully parsed {file_path.name} with includes\")\n",
    "            return document, rst_content\n",
    "            \n",
    "        except Exception as include_error:\n",
    "            print(f\"⚠️ Include resolution failed for {file_path.name}: {include_error}\")\n",
    "            \n",
    "            # Method 2: Try with includes disabled\n",
    "            try:\n",
    "                rst_content_no_includes = remove_include_directives(rst_content)\n",
    "                \n",
    "                settings = get_default_settings(Parser)\n",
    "                settings.report_level = 5  # Suppress all warnings\n",
    "                settings.halt_level = 5    # Don't halt on errors\n",
    "                settings.warning_stream = None\n",
    "                \n",
    "                document = new_document(str(file_path), settings=settings)\n",
    "                parser = Parser()\n",
    "                parser.parse(rst_content_no_includes, document)\n",
    "                \n",
    "                print(f\"✅ Successfully parsed {file_path.name} with includes disabled\")\n",
    "                return document, rst_content  # Return original content, not modified\n",
    "                \n",
    "            except Exception as parse_error:\n",
    "                print(f\"⚠️ Full parsing failed for {file_path.name}: {parse_error}\")\n",
    "                # Return minimal document for manual parsing fallback\n",
    "                settings = get_default_settings(Parser)\n",
    "                document = new_document(str(file_path), settings=settings)\n",
    "                return document, rst_content\n",
    "                \n",
    "        finally:\n",
    "            # Always restore original working directory\n",
    "            os.chdir(original_cwd)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def filter_raw_html(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove raw HTML blocks and directives that are meant for web view.\n",
    "    These are not useful for chunking and RAG purposes.\n",
    "    \"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    filtered_lines = []\n",
    "    skip_lines = 0\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if skip_lines > 0:\n",
    "            skip_lines -= 1\n",
    "            continue\n",
    "            \n",
    "        # Skip raw HTML directives\n",
    "        if line.strip().startswith('.. raw:: html'):\n",
    "            # Skip this line and look for the content block\n",
    "            j = i + 1\n",
    "            while j < len(lines) and (lines[j].startswith('   ') or lines[j].strip() == ''):\n",
    "                j += 1\n",
    "            skip_lines = j - i - 1\n",
    "            continue\n",
    "        \n",
    "        filtered_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(filtered_lines)\n",
    "\n",
    "\n",
    "def extract_references(document: nodes.document, source_lines: List[str]) -> List[RSTReference]:\n",
    "    \"\"\"Extract references/includes from RST document\"\"\"\n",
    "    references = []\n",
    "    \n",
    "    try:\n",
    "        # FIXED: Replace traverse() with findall()\n",
    "        directive_nodes = list(document.findall(condition=lambda node: \n",
    "            hasattr(node, 'tagname') and node.tagname in ['image', 'figure']))\n",
    "        \n",
    "        for node in directive_nodes:\n",
    "            line_num = getattr(node, 'line', None)\n",
    "            if line_num and hasattr(node, 'attributes'):\n",
    "                uri = node.attributes.get('uri', '')\n",
    "                if uri:\n",
    "                    references.append(RSTReference(\n",
    "                        reference_type=node.tagname,\n",
    "                        target=uri,\n",
    "                        line_number=line_num,\n",
    "                        directive=f\".. {node.tagname}:: {uri}\"\n",
    "                    ))\n",
    "        \n",
    "        # Also extract from source lines for include directives and others\n",
    "        for line_num, line_content in enumerate(source_lines, 1):\n",
    "            if line_content.strip().startswith('..'):\n",
    "                for ref_type in ['include', 'literalinclude', 'csv-table']:\n",
    "                    if f'.. {ref_type}::' in line_content:\n",
    "                        # Extract target from the directive\n",
    "                        parts = line_content.split('::', 1)\n",
    "                        if len(parts) > 1:\n",
    "                            target = parts[1].strip()\n",
    "                            references.append(RSTReference(\n",
    "                                reference_type=ref_type,\n",
    "                                target=target,\n",
    "                                line_number=line_num,\n",
    "                                directive=line_content\n",
    "                            ))\n",
    "                        break\n",
    "    except Exception:\n",
    "        # Fallback to source-based extraction if AST approach fails\n",
    "        pass\n",
    "    \n",
    "    # If AST extraction failed or found nothing, use source-based extraction\n",
    "    if not references:\n",
    "        references = extract_references_from_source('\\n'.join(source_lines))\n",
    "    \n",
    "    return references\n",
    "\n",
    "\n",
    "def extract_references_from_source(rst_content: str) -> List[RSTReference]:\n",
    "    \"\"\"Extract references from source text as fallback\"\"\"\n",
    "    references = []\n",
    "    lines = rst_content.split('\\n')\n",
    "    \n",
    "    for line_num, line in enumerate(lines, 1):\n",
    "        line = line.strip()\n",
    "        if line.startswith('..'):\n",
    "            for ref_type in ['include', 'literalinclude', 'image', 'figure', 'csv-table']:\n",
    "                if f'.. {ref_type}::' in line:\n",
    "                    parts = line.split('::', 1)\n",
    "                    if len(parts) > 1:\n",
    "                        target = parts[1].strip()\n",
    "                        references.append(RSTReference(\n",
    "                            reference_type=ref_type,\n",
    "                            target=target,\n",
    "                            line_number=line_num,\n",
    "                            directive=line\n",
    "                        ))\n",
    "                    break\n",
    "    \n",
    "    return references\n",
    "\n",
    "\n",
    "def get_section_level(node: nodes.section, title_levels: Dict[str, int]) -> int:\n",
    "    \"\"\"Determine section level based on title decoration\"\"\"\n",
    "    title = node[0]  # First child should be title\n",
    "    if isinstance(title, nodes.title):\n",
    "        # FIXED: Replace traverse() with findall()\n",
    "        nested_sections = list(node.findall(nodes.section))\n",
    "        # Remove self from count (findall includes the node itself)\n",
    "        return len([s for s in nested_sections if s != node])\n",
    "    return 0\n",
    "\n",
    "\n",
    "def extract_semantic_chunks(document: nodes.document, rst_content: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract semantic chunks from RST document\"\"\"\n",
    "    chunks = []\n",
    "    source_lines = rst_content.split('\\n')\n",
    "    \n",
    "    # Extract document title if present\n",
    "    if document.children and isinstance(document.children[0], nodes.title):\n",
    "        title_node = document.children[0]\n",
    "        title_line = getattr(title_node, 'line', 1)\n",
    "        chunks.append({\n",
    "            'type': 'document_title',\n",
    "            'name': title_node.astext(),\n",
    "            'start_line': title_line,\n",
    "            'end_line': title_line,\n",
    "            'content': title_node.astext(),\n",
    "            'level': 0,\n",
    "            'depth': 0\n",
    "        })\n",
    "    \n",
    "    # Extract sections and their content\n",
    "    # FIXED: Replace traverse() with findall()\n",
    "    sections = list(document.findall(nodes.section))\n",
    "    \n",
    "    for section in sections:\n",
    "        title = section[0] if section.children and isinstance(section[0], nodes.title) else None\n",
    "        if not title:\n",
    "            continue\n",
    "            \n",
    "        section_name = title.astext()\n",
    "        section_line = getattr(title, 'line', 1)\n",
    "        \n",
    "        # Determine section level\n",
    "        level = get_section_level(section, {})\n",
    "        \n",
    "        # Get all subsections within this section\n",
    "        # FIXED: Replace traverse() with findall()\n",
    "        subsections = [s for s in section.findall(nodes.section) if s != section]\n",
    "        \n",
    "        # Calculate section content (excluding subsections)\n",
    "        section_content_lines = []\n",
    "        section_start_line = section_line\n",
    "        section_end_line = section_line\n",
    "        \n",
    "        # Find the end of this section's content\n",
    "        if subsections:\n",
    "            # If there are subsections, content ends before first subsection\n",
    "            first_subsection_line = min(getattr(sub[0], 'line', len(source_lines)) \n",
    "                                      for sub in subsections \n",
    "                                      if sub.children and isinstance(sub[0], nodes.title))\n",
    "            section_end_line = first_subsection_line - 1\n",
    "        else:\n",
    "            # No subsections, include all content\n",
    "            # FIXED: Replace traverse() with findall()\n",
    "            all_nodes_in_section = list(section.findall())\n",
    "            if all_nodes_in_section:\n",
    "                max_line = max(getattr(node, 'line', section_line) \n",
    "                             for node in all_nodes_in_section if hasattr(node, 'line'))\n",
    "                section_end_line = max_line\n",
    "        \n",
    "        # Extract content for this section\n",
    "        if section_start_line <= len(source_lines):\n",
    "            end_idx = min(section_end_line, len(source_lines))\n",
    "            section_content = '\\n'.join(source_lines[section_start_line-1:end_idx])\n",
    "        else:\n",
    "            section_content = section.astext()\n",
    "        \n",
    "        chunks.append({\n",
    "            'type': 'section',\n",
    "            'name': section_name,\n",
    "            'start_line': section_start_line,\n",
    "            'end_line': section_end_line,\n",
    "            'content': section_content,\n",
    "            'level': level + 1,  # +1 because document title is level 0\n",
    "            'depth': level\n",
    "        })\n",
    "    \n",
    "    # Extract standalone elements that are not within sections\n",
    "    # FIXED: Replace traverse() with findall()\n",
    "    all_nodes = list(document.findall())\n",
    "    \n",
    "    for node in all_nodes:\n",
    "        line_num = getattr(node, 'line', None)\n",
    "        if not line_num:\n",
    "            continue\n",
    "            \n",
    "        # Directives (notes, warnings, etc.) - only if not within a section\n",
    "        if isinstance(node, nodes.Admonition):\n",
    "            # Check if this admonition is within a section\n",
    "            parent_section = None\n",
    "            # FIXED: Replace traverse() with parent traversal\n",
    "            current = node.parent\n",
    "            while current:\n",
    "                if isinstance(current, nodes.section):\n",
    "                    parent_section = current\n",
    "                    break\n",
    "                current = current.parent\n",
    "            \n",
    "            # Only create separate chunk if not within a section\n",
    "            if not parent_section:\n",
    "                admonition_text = node.astext()\n",
    "                admonition_type = node.tagname if hasattr(node, 'tagname') else 'admonition'\n",
    "                chunks.append({\n",
    "                    'type': f'{admonition_type}_directive',\n",
    "                    'name': f'{admonition_type}_line_{line_num}',\n",
    "                    'start_line': line_num,\n",
    "                    'end_line': line_num + admonition_text.count('\\n'),\n",
    "                    'content': admonition_text,\n",
    "                    'level': 8,\n",
    "                    'depth': 0\n",
    "                })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CHUNK CREATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_rst_chunks(semantic_nodes: List[Dict[str, Any]]) -> List[RSTChunk]:\n",
    "    \"\"\"Create RSTChunk objects from semantic nodes\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    for node_info in semantic_nodes:\n",
    "        chunk = RSTChunk(\n",
    "            start_line=node_info['start_line'],\n",
    "            end_line=node_info['end_line'],\n",
    "            content=node_info['content'],\n",
    "            node_type=node_info['type'],\n",
    "            name=node_info['name'],\n",
    "            depth=node_info.get('depth', 0),\n",
    "            level=node_info.get('level', 0)\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def group_small_chunks(chunks: List[RSTChunk], target_tokens: int = 600) -> List[RSTChunk]:\n",
    "    \"\"\"Group small chunks together to reach reasonable size\"\"\"\n",
    "    if not chunks:\n",
    "        return chunks\n",
    "    \n",
    "    # Check if everything together is under limit\n",
    "    total_tokens = sum(c.token_count for c in chunks)\n",
    "    if total_tokens <= MAX_CHUNK_TOKENS:\n",
    "        # Combine all into one chunk\n",
    "        combined_content = \"\\n\\n\".join(c.content for c in chunks)\n",
    "        return [RSTChunk(\n",
    "            start_line=min(c.start_line for c in chunks),\n",
    "            end_line=max(c.end_line for c in chunks),\n",
    "            content=combined_content,\n",
    "            node_type=\"combined_document\",\n",
    "            name=\"complete_document\",\n",
    "            depth=0,\n",
    "            level=0\n",
    "        )]\n",
    "    \n",
    "    # Group small chunks while respecting section hierarchy\n",
    "    grouped_chunks = []\n",
    "    current_group = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if (current_tokens + chunk.token_count > target_tokens and \n",
    "            current_group and chunk.token_count <= MAX_CHUNK_TOKENS):\n",
    "            \n",
    "            # Create grouped chunk\n",
    "            if len(current_group) == 1:\n",
    "                grouped_chunks.append(current_group[0])\n",
    "            else:\n",
    "                combined_content = \"\\n\\n\".join(c.content for c in current_group)\n",
    "                grouped_chunk = RSTChunk(\n",
    "                    start_line=min(c.start_line for c in current_group),\n",
    "                    end_line=max(c.end_line for c in current_group),\n",
    "                    content=combined_content,\n",
    "                    node_type=\"grouped_sections\",\n",
    "                    name=f\"sections_{current_group[0].name}_to_{current_group[-1].name}\",\n",
    "                    depth=min(c.depth for c in current_group),\n",
    "                    level=min(c.level for c in current_group)\n",
    "                )\n",
    "                grouped_chunks.append(grouped_chunk)\n",
    "            \n",
    "            current_group = [chunk]\n",
    "            current_tokens = chunk.token_count\n",
    "        else:\n",
    "            current_group.append(chunk)\n",
    "            current_tokens += chunk.token_count\n",
    "    \n",
    "    # Add remaining chunks\n",
    "    if current_group:\n",
    "        if len(current_group) == 1:\n",
    "            grouped_chunks.append(current_group[0])\n",
    "        else:\n",
    "            combined_content = \"\\n\\n\".join(c.content for c in current_group)\n",
    "            grouped_chunk = RSTChunk(\n",
    "                start_line=min(c.start_line for c in current_group),\n",
    "                end_line=max(c.end_line for c in current_group),\n",
    "                content=combined_content,\n",
    "                node_type=\"grouped_sections\",\n",
    "                name=f\"sections_{current_group[0].name}_to_{current_group[-1].name}\",\n",
    "                depth=min(c.depth for c in current_group),\n",
    "                level=min(c.level for c in current_group)\n",
    "            )\n",
    "            grouped_chunks.append(grouped_chunk)\n",
    "    \n",
    "    return grouped_chunks\n",
    "\n",
    "\n",
    "def analyze_code_blocks_in_content(content: str) -> List[Dict[str, int]]:\n",
    "    \"\"\"Analyze code block positions in RST content\"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    code_blocks = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        # Look for code block indicators\n",
    "        if (line.startswith('.. code-block::') or \n",
    "            line.startswith('.. sourcecode::') or \n",
    "            line.endswith('::') and len(line) > 2):\n",
    "            \n",
    "            start_line = i\n",
    "            i += 1\n",
    "            \n",
    "            # Skip empty lines and options\n",
    "            while i < len(lines) and (lines[i].strip() == '' or lines[i].startswith('   :')):\n",
    "                i += 1\n",
    "            \n",
    "            # Find end of indented block\n",
    "            while i < len(lines) and lines[i].startswith('    '):\n",
    "                i += 1\n",
    "            \n",
    "            code_blocks.append({\n",
    "                'start': start_line,\n",
    "                'end': i - 1\n",
    "            })\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return code_blocks\n",
    "\n",
    "\n",
    "def split_content_preserving_code_blocks(content: str, max_tokens: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split content into chunks while preserving code block integrity.\n",
    "    If a code block is too large, it gets its own chunk.\n",
    "    \"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    code_blocks = analyze_code_blocks_in_content(content)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk_lines = []\n",
    "    current_tokens = 0\n",
    "    line_idx = 0\n",
    "    \n",
    "    while line_idx < len(lines):\n",
    "        # Check if current line is start of a code block\n",
    "        current_code_block = None\n",
    "        for cb in code_blocks:\n",
    "            if cb['start'] == line_idx:\n",
    "                current_code_block = cb\n",
    "                break\n",
    "        \n",
    "        if current_code_block:\n",
    "            # We're at the start of a code block\n",
    "            code_block_lines = lines[current_code_block['start']:current_code_block['end'] + 1]\n",
    "            code_block_content = '\\n'.join(code_block_lines)\n",
    "            code_block_tokens = count_tokens(code_block_content)\n",
    "            \n",
    "            # If code block + current chunk would exceed limit, finalize current chunk\n",
    "            if current_tokens + code_block_tokens > max_tokens and current_chunk_lines:\n",
    "                chunks.append('\\n'.join(current_chunk_lines))\n",
    "                current_chunk_lines = []\n",
    "                current_tokens = 0\n",
    "            \n",
    "            # If code block itself is too large, give it its own chunk\n",
    "            if code_block_tokens > max_tokens:\n",
    "                # Finalize current chunk if it has content\n",
    "                if current_chunk_lines:\n",
    "                    chunks.append('\\n'.join(current_chunk_lines))\n",
    "                    current_chunk_lines = []\n",
    "                    current_tokens = 0\n",
    "                \n",
    "                # Code block gets its own chunk\n",
    "                chunks.append(code_block_content)\n",
    "            else:\n",
    "                # Add code block to current chunk\n",
    "                current_chunk_lines.extend(code_block_lines)\n",
    "                current_tokens += code_block_tokens\n",
    "            \n",
    "            # Move past the code block\n",
    "            line_idx = current_code_block['end'] + 1\n",
    "        else:\n",
    "            # Regular line - add if it fits\n",
    "            line = lines[line_idx]\n",
    "            line_tokens = count_tokens(line)\n",
    "            \n",
    "            if current_tokens + line_tokens > max_tokens and current_chunk_lines:\n",
    "                # Finalize current chunk\n",
    "                chunks.append('\\n'.join(current_chunk_lines))\n",
    "                current_chunk_lines = [line]\n",
    "                current_tokens = line_tokens\n",
    "            else:\n",
    "                current_chunk_lines.append(line)\n",
    "                current_tokens += line_tokens\n",
    "            \n",
    "            line_idx += 1\n",
    "    \n",
    "    # Add final chunk\n",
    "    if current_chunk_lines:\n",
    "        chunks.append('\\n'.join(current_chunk_lines))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def sub_chunk_by_lines(chunk: RSTChunk, rst_content: str) -> List[RSTChunk]:\n",
    "    \"\"\"Sub-chunk oversized chunks while preserving code block integrity\"\"\"\n",
    "    if chunk.token_count <= MAX_CHUNK_TOKENS:\n",
    "        return [chunk]\n",
    "    \n",
    "    # Extract the chunk's content from the full document\n",
    "    lines = rst_content.split('\\n')\n",
    "    chunk_lines = lines[chunk.start_line-1:chunk.end_line]\n",
    "    chunk_content = '\\n'.join(chunk_lines)\n",
    "    \n",
    "    # Split content preserving code blocks\n",
    "    sub_contents = split_content_preserving_code_blocks(chunk_content, MAX_CHUNK_TOKENS)\n",
    "    \n",
    "    if len(sub_contents) <= 1:\n",
    "        return [chunk]  # Couldn't split effectively\n",
    "    \n",
    "    sub_chunks = []\n",
    "    lines_processed = 0\n",
    "    \n",
    "    for i, sub_content in enumerate(sub_contents):\n",
    "        sub_lines = sub_content.split('\\n')\n",
    "        sub_start_line = chunk.start_line + lines_processed\n",
    "        sub_end_line = sub_start_line + len(sub_lines) - 1\n",
    "        \n",
    "        sub_chunk = RSTChunk(\n",
    "            start_line=sub_start_line,\n",
    "            end_line=sub_end_line,\n",
    "            content=sub_content,\n",
    "            node_type=f\"{chunk.node_type}_part\",\n",
    "            name=f\"{chunk.name}_part_{i+1}\",\n",
    "            depth=chunk.depth + 1,\n",
    "            level=chunk.level\n",
    "        )\n",
    "        sub_chunks.append(sub_chunk)\n",
    "        \n",
    "        lines_processed += len(sub_lines)\n",
    "    \n",
    "    return sub_chunks\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FILE PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def process_rst_file(file_path: Path) -> List[RSTChunk]:\n",
    "    \"\"\"Process a single RST file and return chunks\"\"\"\n",
    "    print(f\"\\n🔍 Processing: {file_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Parse RST file\n",
    "        document, rst_content = parse_rst_file(file_path)\n",
    "        \n",
    "        if document.children:\n",
    "            print(f\"✅ Successfully parsed RST structure\")\n",
    "        else:\n",
    "            print(\"⚠️ Empty document\")\n",
    "            return []\n",
    "        \n",
    "        # Extract references (similar to imports)\n",
    "        references = extract_references(document, rst_content.split('\\n'))\n",
    "        if references:\n",
    "            print(f\"📎 Found {len(references)} references/includes\")\n",
    "            for ref in references:\n",
    "                print(f\"  - {ref.reference_type}: {ref.target}\")\n",
    "        \n",
    "        # Find semantic chunks\n",
    "        semantic_nodes = extract_semantic_chunks(document, rst_content)\n",
    "        print(f\"Found {len(semantic_nodes)} semantic units\")\n",
    "        \n",
    "        # Show what we found\n",
    "        for node in semantic_nodes:\n",
    "            preview = node['content'][:100].replace('\\n', ' ').strip()\n",
    "            print(f\"  - {node['type']}: {node['name']} ({count_tokens(node['content'])} tokens)\")\n",
    "            print(f\"    Preview: {preview}...\")\n",
    "        \n",
    "        # Create chunks\n",
    "        base_chunks = create_rst_chunks(semantic_nodes)\n",
    "        \n",
    "        # Group small chunks\n",
    "        base_chunks = group_small_chunks(base_chunks, target_tokens=TARGET_TOKENS)\n",
    "        print(f\"Created {len(base_chunks)} semantic chunks\")\n",
    "        \n",
    "        # Apply sub-chunking for oversized chunks\n",
    "        final_chunks = []\n",
    "        oversized_count = 0\n",
    "        \n",
    "        for chunk in base_chunks:\n",
    "            if chunk.token_count > MAX_CHUNK_TOKENS:\n",
    "                print(f\"  Sub-chunking {chunk.name} ({chunk.token_count} tokens)\")\n",
    "                sub_chunks = sub_chunk_by_lines(chunk, rst_content)\n",
    "                final_chunks.extend(sub_chunks)\n",
    "                oversized_count += 1\n",
    "            else:\n",
    "                final_chunks.append(chunk)\n",
    "        \n",
    "        if oversized_count > 0:\n",
    "            print(f\"Sub-chunked {oversized_count} oversized chunks\")\n",
    "        print(f\"Final result: {len(final_chunks)} total chunks\")\n",
    "        \n",
    "        return final_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_unique_id(length: int = 6) -> str:\n",
    "    \"\"\"Generate a random unique ID\"\"\"\n",
    "    alphabet = string.ascii_lowercase + string.digits\n",
    "    return ''.join(secrets.choice(alphabet) for _ in range(length))\n",
    "\n",
    "\n",
    "def create_chunk_filename(original_filename: str, chunk_number: int, unique_id: str) -> str:\n",
    "    \"\"\"Create chunk filename: index.rst_chunk_001_a1s2d3.md\"\"\"\n",
    "    return f\"{original_filename}_chunk_{chunk_number:03d}_{unique_id}.md\"\n",
    "\n",
    "\n",
    "def create_chunk_markdown(chunk: RSTChunk, source_file_path: str, references: List[RSTReference]) -> str:\n",
    "    \"\"\"Create markdown content with YAML frontmatter\"\"\"\n",
    "    unique_id = generate_unique_id()\n",
    "    \n",
    "    # Filter references that might apply to this chunk\n",
    "    chunk_references = []\n",
    "    for ref in references:\n",
    "        if chunk.start_line <= ref.line_number <= chunk.end_line:\n",
    "            chunk_references.append(f\"{ref.reference_type}: {ref.target}\")\n",
    "    \n",
    "    frontmatter = f\"\"\"---\n",
    "file_path: \"{source_file_path}\"\n",
    "chunk_id: \"{unique_id}\"\n",
    "chunk_type: \"{chunk.node_type}\"\n",
    "chunk_name: \"{chunk.name}\"\n",
    "start_line: {chunk.start_line}\n",
    "end_line: {chunk.end_line}\n",
    "token_count: {chunk.token_count}\n",
    "depth: {chunk.depth}\n",
    "level: {chunk.level}\n",
    "language: \"rst\"\n",
    "references: {chunk_references}\n",
    "---\n",
    "\n",
    "# {chunk.name}\n",
    "\n",
    "**Type:** {chunk.node_type}  \n",
    "**Tokens:** {chunk.token_count}  \n",
    "**Depth:** {chunk.depth}  \n",
    "**Level:** {chunk.level}\n",
    "\n",
    "```rst\n",
    "{chunk.content}\n",
    "```\n",
    "\"\"\"\n",
    "    return frontmatter\n",
    "\n",
    "\n",
    "def save_chunks_to_files(chunks: List[RSTChunk], \n",
    "                        original_file_path: Path, \n",
    "                        input_directory: Path,\n",
    "                        output_base: Path,\n",
    "                        references: List[RSTReference]) -> List[str]:\n",
    "    \"\"\"Save chunks as markdown files maintaining directory structure\"\"\"\n",
    "    if not chunks:\n",
    "        return []\n",
    "    \n",
    "    # Calculate relative path from input directory\n",
    "    try:\n",
    "        rel_path = original_file_path.relative_to(input_directory)\n",
    "    except ValueError:\n",
    "        # If file is not under input directory, use just the filename\n",
    "        rel_path = original_file_path.name\n",
    "    \n",
    "    # Create output directory structure\n",
    "    output_dir = output_base / rel_path.parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate unique ID for this file\n",
    "    file_unique_id = generate_unique_id()\n",
    "    \n",
    "    saved_files = []\n",
    "    \n",
    "    # Add chunk index to each chunk and save\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        # Create chunk filename\n",
    "        chunk_filename = create_chunk_filename(\n",
    "            original_file_path.name, \n",
    "            i, \n",
    "            file_unique_id\n",
    "        )\n",
    "        \n",
    "        # Create markdown content\n",
    "        markdown_content = create_chunk_markdown(\n",
    "            chunk, \n",
    "            str(rel_path), \n",
    "            references\n",
    "        )\n",
    "        \n",
    "        # Write to file\n",
    "        chunk_file_path = output_dir / chunk_filename\n",
    "        try:\n",
    "            with open(chunk_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "            \n",
    "            saved_files.append(str(chunk_file_path))\n",
    "            print(f\"  ✅ Saved: {chunk_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error saving {chunk_filename}: {e}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "\n",
    "def print_chunk_summary(chunks: List[RSTChunk], file_name: str):\n",
    "    \"\"\"Print detailed summary of chunks\"\"\"\n",
    "    print(f\"\\n--- RST Chunk Summary for {file_name} ---\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        indent = \"  \" * chunk.depth\n",
    "        content_lines = len(chunk.content.split('\\n'))\n",
    "        \n",
    "        print(f\"{indent}{i}. {chunk.name}\")\n",
    "        print(f\"{indent}   Type: {chunk.node_type} | Level: {chunk.level} | Lines: {content_lines} | Tokens: {chunk.token_count}\")\n",
    "        \n",
    "        # Show content preview\n",
    "        preview = chunk.content[:200].replace('\\n', ' ').strip()\n",
    "        if len(chunk.content) > 200:\n",
    "            preview += \"...\"\n",
    "        print(f\"{indent}   Preview: {preview}\")\n",
    "\n",
    "\n",
    "def save_chunks_as_markdown(chunks: List[RSTChunk], output_dir: str = \"rst_chunks\") -> None:\n",
    "    \"\"\"Save chunks as markdown files (notebook version)\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    file_unique_id = generate_unique_id()\n",
    "    saved_count = 0\n",
    "    \n",
    "    print(f\"\\n💾 Saving {len(chunks)} chunks to {output_path}/\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        # Create chunk filename\n",
    "        chunk_filename = f\"chunk_{i:03d}_{file_unique_id}.md\"\n",
    "        \n",
    "        # Create markdown content\n",
    "        markdown_content = create_chunk_markdown(chunk, \"notebook_content.rst\", [])\n",
    "        \n",
    "        # Write to file\n",
    "        chunk_file_path = output_path / chunk_filename\n",
    "        try:\n",
    "            with open(chunk_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "            \n",
    "            print(f\"  ✅ Saved: {chunk_filename}\")\n",
    "            saved_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error saving {chunk_filename}: {e}\")\n",
    "    \n",
    "    print(f\"✅ Successfully saved {saved_count} chunk files\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# JUPYTER NOTEBOOK FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def process_rst_content(rst_content: str, file_name: str = \"content.rst\") -> List[RSTChunk]:\n",
    "    \"\"\"\n",
    "    Process RST content directly (for Jupyter notebooks).\n",
    "    \n",
    "    Args:\n",
    "        rst_content: Raw RST content as string\n",
    "        file_name: Name to use for the content (for display purposes)\n",
    "    \n",
    "    Returns:\n",
    "        List of RSTChunk objects\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the approach you suggested: pre-process to remove includes\n",
    "        rst_input = remove_include_directives(rst_content)\n",
    "        \n",
    "        # Parse with docutils using the pattern you mentioned\n",
    "        from docutils.frontend import get_default_settings\n",
    "        \n",
    "        settings = get_default_settings(Parser)\n",
    "        settings.report_level = 4  # Show errors but not warnings\n",
    "        settings.halt_level = 5    # Don't halt on errors\n",
    "        settings.warning_stream = None  # Suppress warnings\n",
    "        \n",
    "        document = new_document(file_name, settings=settings)\n",
    "        parser = Parser()\n",
    "        \n",
    "        try:\n",
    "            parser.parse(rst_input, document)\n",
    "            print(f\"✅ Successfully parsed RST structure for {file_name}\")\n",
    "        except Exception as parse_error:\n",
    "            print(f\"⚠️ Parse error in {file_name}: {parse_error}\")\n",
    "            # Continue with whatever was parsed\n",
    "        \n",
    "        if not document.children:\n",
    "            print(f\"⚠️ Empty document: {file_name}\")\n",
    "            return []\n",
    "        \n",
    "        # Extract references from original content (not the modified one)\n",
    "        references = extract_references_from_source(rst_content)\n",
    "        if references:\n",
    "            print(f\"📎 Found {len(references)} references/includes\")\n",
    "            for ref in references:\n",
    "                print(f\"  - {ref.reference_type}: {ref.target}\")\n",
    "        \n",
    "        # Find semantic chunks\n",
    "        semantic_nodes = extract_semantic_chunks(document, rst_content)\n",
    "        print(f\"Found {len(semantic_nodes)} semantic units\")\n",
    "        \n",
    "        # Show what we found\n",
    "        for node in semantic_nodes:\n",
    "            preview = node['content'][:100].replace('\\n', ' ').strip()\n",
    "            print(f\"  - {node['type']}: {node['name']} ({count_tokens(node['content'])} tokens)\")\n",
    "            print(f\"    Preview: {preview}...\")\n",
    "        \n",
    "        # Create chunks\n",
    "        base_chunks = create_rst_chunks(semantic_nodes)\n",
    "        \n",
    "        # Group small chunks\n",
    "        base_chunks = group_small_chunks(base_chunks, target_tokens=TARGET_TOKENS)\n",
    "        print(f\"Created {len(base_chunks)} semantic chunks\")\n",
    "        \n",
    "        # Apply sub-chunking for oversized chunks\n",
    "        final_chunks = []\n",
    "        oversized_count = 0\n",
    "        \n",
    "        for chunk in base_chunks:\n",
    "            if chunk.token_count > MAX_CHUNK_TOKENS:\n",
    "                print(f\"  Sub-chunking {chunk.name} ({chunk.token_count} tokens)\")\n",
    "                sub_chunks = sub_chunk_by_lines(chunk, rst_content)\n",
    "                final_chunks.extend(sub_chunks)\n",
    "                oversized_count += 1\n",
    "            else:\n",
    "                final_chunks.append(chunk)\n",
    "        \n",
    "        if oversized_count > 0:\n",
    "            print(f\"Sub-chunked {oversized_count} oversized chunks\")\n",
    "        print(f\"Final result: {len(final_chunks)} total chunks\")\n",
    "        \n",
    "        return final_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing RST content: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for RST semantic chunking\"\"\"\n",
    "    print(\"🚀 RST (reStructuredText) Semantic Chunking\")\n",
    "    print(f\"Max chunk tokens: {MAX_CHUNK_TOKENS}\")\n",
    "    print(f\"Target tokens for grouping: {TARGET_TOKENS}\")\n",
    "    \n",
    "    # Get directory from user or use current directory\n",
    "    directory = input(\"\\nEnter source directory path (or press Enter for current directory): \").strip()\n",
    "    if not directory:\n",
    "        directory = \".\"\n",
    "    \n",
    "    target_dir = Path(directory).resolve()\n",
    "    if not target_dir.exists():\n",
    "        print(f\"❌ Directory not found: {directory}\")\n",
    "        return\n",
    "    \n",
    "    # Create output directory parallel to source directory\n",
    "    output_dir = target_dir.parent / f\"{target_dir.name}_rst_chunks\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    print(f\"📁 Output directory: {output_dir}\")\n",
    "    \n",
    "    target_path = target_dir\n",
    "    input_directory = target_dir\n",
    "    \n",
    "    # Collect RST files\n",
    "    rst_files = []\n",
    "    for ext in ['*.rst', '*.txt']:\n",
    "        rst_files.extend(target_path.rglob(ext))\n",
    "    \n",
    "    # Filter to actual RST files by checking content\n",
    "    actual_rst_files = []\n",
    "    for file in rst_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read(1000)  # Check first 1000 chars\n",
    "                # Simple heuristic: look for RST-like content\n",
    "                if any(marker in content for marker in ['===', '---', '~~~', '^^^', '.. ', '::']):\n",
    "                    actual_rst_files.append(file)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    rst_files = actual_rst_files\n",
    "    \n",
    "    if not rst_files:\n",
    "        print(f\"❌ No RST files found in {directory}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"🔍 Found {len(rst_files)} RST file(s)\")\n",
    "    \n",
    "    # Group by directory for display\n",
    "    by_dir = {}\n",
    "    for f in rst_files:\n",
    "        dir_path = str(f.parent.relative_to(input_directory)) if f.parent != input_directory else '.'\n",
    "        if dir_path not in by_dir:\n",
    "            by_dir[dir_path] = []\n",
    "        by_dir[dir_path].append(f)\n",
    "    \n",
    "    # Show files found\n",
    "    for dir_path, files in sorted(by_dir.items()):\n",
    "        print(f\"  📂 {dir_path}:\")\n",
    "        for f in files:\n",
    "            print(f\"    - {f.name}\")\n",
    "    \n",
    "    proceed = input(f\"\\nProcess all {len(rst_files)} files? (y/n): \").strip().lower()\n",
    "    if proceed != 'y':\n",
    "        print(\"❌ Processing cancelled\")\n",
    "        return\n",
    "    \n",
    "    # Process all files\n",
    "    total_chunks = 0\n",
    "    processed_files = 0\n",
    "    \n",
    "    for file_path in rst_files:\n",
    "        try:\n",
    "            chunks = process_rst_file(file_path)\n",
    "            \n",
    "            if chunks:\n",
    "                # Save chunks\n",
    "                references = extract_references_from_source(file_path.read_text(encoding='utf-8'))\n",
    "                saved_files = save_chunks_to_files(\n",
    "                    chunks, file_path, input_directory, output_dir, references\n",
    "                )\n",
    "                \n",
    "                total_chunks += len(chunks)\n",
    "                processed_files += 1\n",
    "                \n",
    "                print_chunk_summary(chunks, file_path.name)\n",
    "            else:\n",
    "                print(f\"⚠️ No chunks generated for {file_path.name}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Processing complete!\")\n",
    "    print(f\"✅ Files processed: {processed_files}/{len(rst_files)}\")\n",
    "    print(f\"📄 Total chunks created: {total_chunks}\")\n",
    "    print(f\"📁 Output directory: {output_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55d8d6",
   "metadata": {},
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f041ae2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found RST language in tree-sitter-language-pack\n",
      "🚀 RST (reStructuredText) Semantic Chunking with Tree-sitter\n",
      "Max chunk tokens: 1000\n",
      "Target tokens for grouping: 600\n",
      "🌳 Using tree-sitter for clean, warning-free parsing\n",
      "📁 Output directory: /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils_rst_chunks\n",
      "🔍 Found 60 RST file(s)\n",
      "  📂 docs:\n",
      "    - index.rst\n",
      "    - header2.rst\n",
      "    - header0.rst\n",
      "    - header.rst\n",
      "  📂 docs/api:\n",
      "    - runtime-settings.rst\n",
      "    - publisher.rst\n",
      "    - transforms.rst\n",
      "  📂 docs/dev:\n",
      "    - policies.rst\n",
      "    - website.rst\n",
      "    - distributing.rst\n",
      "    - runtime-settings-processing.rst\n",
      "    - pysource.rst\n",
      "    - testing.rst\n",
      "    - todo.rst\n",
      "    - hacking.rst\n",
      "    - semantics.rst\n",
      "    - enthought-plan.rst\n",
      "    - enthought-rfp.rst\n",
      "    - repository.rst\n",
      "    - release.rst\n",
      "  📂 docs/dev/rst:\n",
      "    - alternatives.rst\n",
      "    - problems.rst\n",
      "  📂 docs/eps:\n",
      "    - index.rst\n",
      "    - ep-template.rst\n",
      "    - header.rst\n",
      "    - ep-001.rst\n",
      "    - ep-010.rst\n",
      "  📂 docs/howto:\n",
      "    - rst-directives.rst\n",
      "    - html-stylesheets.rst\n",
      "    - i18n.rst\n",
      "    - rst-roles.rst\n",
      "    - cmdline-tool.rst\n",
      "    - security.rst\n",
      "  📂 docs/peps:\n",
      "    - pep-0287.rst\n",
      "    - pep-0256.rst\n",
      "    - pep-0257.rst\n",
      "    - pep-0258.rst\n",
      "  📂 docs/ref:\n",
      "    - doctree.rst\n",
      "  📂 docs/ref/rst:\n",
      "    - roles.rst\n",
      "    - directives.rst\n",
      "    - mathematics.rst\n",
      "    - restructuredtext.rst\n",
      "    - history.rst\n",
      "    - definitions.rst\n",
      "    - introduction.rst\n",
      "  📂 docs/user:\n",
      "    - todo-lists.rst\n",
      "    - slide-shows.rst\n",
      "    - tools.rst\n",
      "    - config.rst\n",
      "    - html.rst\n",
      "    - smartquotes.rst\n",
      "    - manpage.rst\n",
      "    - emacs.rst\n",
      "    - mailing-lists.rst\n",
      "    - latex.rst\n",
      "    - odt.rst\n",
      "    - links.rst\n",
      "  📂 docs/user/rst:\n",
      "    - demo.rst\n",
      "    - quickstart.rst\n",
      "    - cheatsheet.rst\n",
      "\n",
      "🔍 Processing: index.rst\n",
      "🌳 Processing index.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 23 chunks\n",
      "Created 4 grouped chunks\n",
      "Final result: 4 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 4 chunks for index.rst\n",
      "  ✅ Saved: index.rst_chunk_001_6x5dhw.md\n",
      "  ✅ Saved: index.rst_chunk_002_6x5dhw.md\n",
      "  ✅ Saved: index.rst_chunk_003_6x5dhw.md\n",
      "  ✅ Saved: index.rst_chunk_004_6x5dhw.md\n",
      "\n",
      "--- RST Chunk Summary for index.rst ---\n",
      "1. sections_content_block_1_to_Introductory & Tutorial Material for End-Users\n",
      "   Type: grouped_sections | Level: 1 | Lines: 100 | Tokens: 589\n",
      "   Preview: ==========================================   Docutils Project Documentation Overview  ==========================================  :Author: David Goodger :Contact: docutils-develop@lists.sourceforge.ne...\n",
      "2. sections_content_block_13_to_Reference Material for All Groups\n",
      "   Type: grouped_sections | Level: 1 | Lines: 33 | Tokens: 340\n",
      "   Preview: ==============================================  Docutils-general:   * `Docutils Front-End Tools <user/tools.html>`__   * `Docutils Configuration <user/config.html>`__   * `Docutils Mailing Lists <user...\n",
      "3. content_block_15\n",
      "   Type: content_block | Level: 1 | Lines: 58 | Tokens: 608\n",
      "   Preview: =================================  Many of these files began as developer specifications, but now that they're mature and used by end-users and client-developers, they have become reference material....\n",
      "4. sections_API Reference Material for Client-Developers_to_content_block_23\n",
      "   Type: grouped_sections | Level: 1 | Lines: 80 | Tokens: 589\n",
      "   Preview: API Reference Material for Client-Developers  ============================================  `The Docutils Publisher <api/publisher.html>`__   entry points for using Docutils as a library `Docutils Run...\n",
      "\n",
      "🔍 Processing: header2.rst\n",
      "🌳 Processing header2.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 2 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for header2.rst\n",
      "  ✅ Saved: header2.rst_chunk_001_29m4sk.md\n",
      "\n",
      "--- RST Chunk Summary for header2.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 17 | Tokens: 108\n",
      "   Preview: .. Minimal menu bar for inclusion in documentation sources    in ``docutils/docs/*/`` sub-sub-diretories.     Attention: this is not a standalone document.   .. header::    Docutils__ | Overview__ | A...\n",
      "\n",
      "🔍 Processing: header0.rst\n",
      "🌳 Processing header0.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 2 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for header0.rst\n",
      "  ✅ Saved: header0.rst_chunk_001_8jslai.md\n",
      "\n",
      "--- RST Chunk Summary for header0.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 16 | Tokens: 105\n",
      "   Preview: .. Minimal menu bar for inclusion in documentation sources    in the ``docutils/`` parent diretory.     Attention: this is not a standalone document.   .. header::    Docutils__ | Overview__ | About__...\n",
      "\n",
      "🔍 Processing: header.rst\n",
      "🌳 Processing header.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 2 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for header.rst\n",
      "  ✅ Saved: header.rst_chunk_001_zmdbzm.md\n",
      "\n",
      "--- RST Chunk Summary for header.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 16 | Tokens: 107\n",
      "   Preview: .. Minimal menu bar for inclusion in documentation sources    in ``docutils/docs/*/`` sub-diretories.     Attention: this is not a standalone document.   .. header::    Docutils__ | Overview__ | About...\n",
      "\n",
      "🔍 Processing: doctree.rst\n",
      "🌳 Processing doctree.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 719 chunks\n",
      "Created 76 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_712_to_content_block_717 (1448 tokens)\n",
      "  📦 Sub-chunking sections_Bibliography_to_content_block_719 (2744 tokens)\n",
      "Final result: 79 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 79 chunks for doctree.rst\n",
      "  ✅ Saved: doctree.rst_chunk_001_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_002_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_003_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_004_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_005_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_006_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_007_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_008_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_009_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_010_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_011_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_012_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_013_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_014_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_015_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_016_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_017_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_018_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_019_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_020_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_021_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_022_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_023_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_024_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_025_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_026_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_027_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_028_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_029_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_030_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_031_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_032_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_033_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_034_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_035_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_036_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_037_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_038_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_039_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_040_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_041_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_042_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_043_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_044_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_045_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_046_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_047_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_048_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_049_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_050_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_051_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_052_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_053_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_054_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_055_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_056_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_057_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_058_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_059_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_060_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_061_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_062_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_063_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_064_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_065_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_066_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_067_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_068_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_069_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_070_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_071_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_072_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_073_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_074_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_075_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_076_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_077_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_078_dwwene.md\n",
      "  ✅ Saved: doctree.rst_chunk_079_dwwene.md\n",
      "\n",
      "--- RST Chunk Summary for doctree.rst ---\n",
      "1. sections_content_block_1_to_content_block_8\n",
      "   Type: grouped_sections | Level: 1 | Lines: 65 | Tokens: 437\n",
      "   Preview: .. include:: ../header.rst  ============================   The Docutils Document Tree  ============================   A Guide to the `Docutils Generic` DTD  *************************************  :Aut...\n",
      "2. sections_content_block_9_to_content_block_12\n",
      "   Type: grouped_sections | Level: 1 | Lines: 49 | Tokens: 587\n",
      "   Preview: .. raw:: html     <style type=\"text/css\"><!--      table.e-hierarchy {border: 1px solid}      table.e-hierarchy td {border: 0px; padding: 0.5em}      table.e-hierarchy a {color: black; text-decoration...\n",
      "3. sections_|   | `bibliographic elements`_   | `decorative elements`_    |   |_to_content_block_32\n",
      "   Type: grouped_sections | Level: 1 | Lines: 100 | Tokens: 605\n",
      "   Preview: |   | `bibliographic elements`_   | `decorative elements`_    |   |     +---+-----------------------------+---------------------------+---+     | [`body elements`_]...\n",
      "4. sections_content_block_33_to_content_block_44\n",
      "   Type: grouped_sections | Level: 1 | Lines: 70 | Tokens: 387\n",
      "   Preview: .. class:: narrow run-in  :Category members:    .. class:: narrow run-in    :empty: `\\<meta>`_, `\\<transition>`_   :simple: `\\<title>`_, `\\<subtitle>`_   :compound: `\\<decoration>`_, `\\<docinfo>`_  :D...\n",
      "5. sections_content_block_45_to_content_block_51\n",
      "   Type: grouped_sections | Level: 1 | Lines: 67 | Tokens: 592\n",
      "   Preview: .. class:: narrow run-in    :empty:     `\\<image>`_ , `\\<pending>`_   :simple:     `\\<comment>`_, `\\<doctest_block>`_, `\\<literal_block>`_, `\\<math_block>`_,     `\\<paragraph>`_, `\\<raw>`_, `\\<refer...\n",
      "6. sections_content_block_52_to_<abbreviation>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 72 | Tokens: 501\n",
      "   Preview: .. class:: narrow run-in  :Category members:   `\\<abbreviation>`_, `\\<acronym>`_, `\\<citation_reference>`_, `\\<emphasis>`_,   `\\<footnote_reference>`_, `\\<generated>`_, `\\<image>`_, `\\<inline>`_,   `\\...\n",
      "7. sections_content_block_60_to_<address>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 72 | Tokens: 483\n",
      "   Preview: ==============  The <abbreviation> element is an inline element representing an *abbreviation*, a shortened or contracted form of a word or phrase used to represent the whole.  :Category:   `Inline El...\n",
      "8. sections_content_block_68_to_content_block_76\n",
      "   Type: grouped_sections | Level: 1 | Lines: 81 | Tokens: 603\n",
      "   Preview: ===========  The <address> element holds the surface mailing address information for the author(s) (individual or group) of the document, or a third-party contact address.  :Category:   `Bibliographic...\n",
      "9. sections_content_block_77_to_Document Title\n",
      "   Type: grouped_sections | Level: 1 | Lines: 102 | Tokens: 602\n",
      "   Preview: .. admonition:: By the way...         You can make up your own admonition too.  Pseudo-XML_ fragment from simple parsing::      <admonition class=\"admonition-by-the-way\">         <title>...\n",
      "10. sections_content_block_92_to_Examples\n",
      "   Type: grouped_sections | Level: 1 | Lines: 93 | Tokens: 518\n",
      "   Preview: ==============      :Author: J. Random Hacker  Complete pseudo-XML_ result after parsing and applying transforms_::      <document ids=\"document-title\" names=\"document title\">         <title>...\n",
      "11. sections_content_block_102_to_<caption>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 80 | Tokens: 483\n",
      "   Preview: --------  In reStructuredText, an indented text block without preceding markup is a `block quote`_::      As a great palaeontologist once said,          This theory, that is mine, is mine.          --...\n",
      "12. sections_content_block_108_to_<citation>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 64 | Tokens: 425\n",
      "   Preview: =========  The <caption> element represents the title/caption of a `\\<figure>`_. [#]_  :Category:   `Body Subelements`_ (simple) :Analogues:  The <caption> element is analogous to the DocBook_...\n",
      "13. sections_content_block_117_to_Example\n",
      "   Type: grouped_sections | Level: 1 | Lines: 60 | Tokens: 422\n",
      "   Preview: ==========  The <citation> element contains a description of an external bibliographic source.  It is usually paired with one or more `\\<citation_reference>`_ elements that represent corresponding ref...\n",
      "14. sections_content_block_123_to_<colspec>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 96 | Tokens: 576\n",
      "   Preview: -------  reStructuredText syntax for `citation references`_ is similar to a `footnote reference`_ except for the use of a `simple reference name`_ instead of a numerical or symbolic label::      For d...\n",
      "15. sections_content_block_129_to_<compound>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 82 | Tokens: 590\n",
      "   Preview: =========  The <colspec> element contains specifications for a column in a `\\<table>`_. It is  defined in the `Exchange Table Model`_.  :Category:   `Body Subelements`_ (empty) :Analogues:  <colspec>...\n",
      "16. sections_content_block_137_to_<container>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 99 | Tokens: 591\n",
      "   Preview: ==========  The <compound> element combines multiple `body elements`_ to a single logical paragraph.  :Category:   `Compound Body Elements`_ :Analogues:  The <compound> element has no direct analogues...\n",
      "17. sections_content_block_148_to_<danger>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 90 | Tokens: 504\n",
      "   Preview: ===========  The <container> element groups multiple `body elements`_ for user- or application-specific purposes.  :Category:   `Compound Body Elements`_ :Analogues:  The <container> element is analog...\n",
      "18. sections_content_block_160_to_<decoration>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 80 | Tokens: 455\n",
      "   Preview: ========  The <danger> element is a specific *admonition*, a distinctive and self-contained notice.  :Category:   `Compound Body Elements`_ :Analogues:  <danger> has no direct analogues in common DTDs...\n",
      "19. sections_content_block_171_to_Examples\n",
      "   Type: grouped_sections | Level: 1 | Lines: 69 | Tokens: 510\n",
      "   Preview: ============  The <decoration> element is a container for `\\<header>`_ and `\\<footer>`_ elements and potential future extensions.  These elements are used for notes, time/datestamp, processing informa...\n",
      "20. sections_content_block_181_to_content_block_185\n",
      "   Type: grouped_sections | Level: 1 | Lines: 111 | Tokens: 601\n",
      "   Preview: --------  A reStructuredText `definition list`_. The classifier is optional. ::      Term         Definition.      Term : classifier         The ' : ' indicates a classifier in         definition list...\n",
      "21. sections_<description>_to_Docinfo Example\n",
      "   Type: grouped_sections | Level: 1 | Lines: 71 | Tokens: 509\n",
      "   Preview: <description>  =============  The <description> element is the part of an `\\<option_list>`_ item that contains the description of a command-line option or group of options.  :Category:   `Body Subelem...\n",
      "22. sections_content_block_195_to_Examples\n",
      "   Type: grouped_sections | Level: 1 | Lines: 71 | Tokens: 546\n",
      "   Preview: ===============      :Author: J. Random Hacker     :Contact: jrh@example.com     :Date: 2002-08-18     :Status: Work In Progress     :Version: 1     :Filename: $RCSfile$     :Copyright: This docum...\n",
      "23. sections_content_block_199_to_<emphasis>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 106 | Tokens: 586\n",
      "   Preview: --------  A reStructuredText `doctest block`_::      This is an ordinary paragraph.      >>> print('this is a Doctest block')     this is a Doctest block  Pseudo-XML_ fragment from simple parsing::...\n",
      "24. sections_content_block_208_to_<enumerated_list>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 70 | Tokens: 469\n",
      "   Preview: ==========  The <emphasis> element is an inline element representing text that has *stress emphasis*.  :Category:   `Inline Elements`_ :Analogues:  <emphasis> is analogous to the HTML_ <em> element...\n",
      "25. sections_content_block_216_to_<field>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 94 | Tokens: 552\n",
      "   Preview: =================  The <enumerated_list> element contains `\\<list_item>`_ elements which are uniformly marked with enumerator labels.  :Category:   `Compound Body Elements`_ :Analogues:  <enumerated_l...\n",
      "26. sections_content_block_225_to_Examples\n",
      "   Type: grouped_sections | Level: 1 | Lines: 69 | Tokens: 519\n",
      "   Preview: =======  The <field> element contains one item of a `\\<field_list>`_, a pair of `\\<field_name>`_ and `\\<field_body>`_ elements.  :Category:   `Body Subelements`_ (compound), `Bibliographic Elements`_...\n",
      "27. sections_content_block_235_to_content_block_243\n",
      "   Type: grouped_sections | Level: 1 | Lines: 88 | Tokens: 525\n",
      "   Preview: --------  A reStructuredText `field list <rST field list_>`__::      :Author: Me     :Version: 1     :Date: 2001-08-11     :Parameter i: integer  Pseudo-XML_ fragment from simple parsing::      <field...\n",
      "28. sections_content_block_244_to_<footnote>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 69 | Tokens: 402\n",
      "   Preview: .. figure:: larch-mini.jpg        :alt: a larch        :target: larch.jpg        :class: thumbnail        :figclass: numbered         The larch.         Larix decidua in Aletschwald.  Pseudo-XML_...\n",
      "29. sections_content_block_250_to_<footnote_reference>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 62 | Tokens: 464\n",
      "   Preview: ==========  The <footnote> element is used for labelled notes_ that provide additional context to a passage of text (*footnotes* or *endnotes*). The corresponding footnote mark in running text is set...\n",
      "30. sections_content_block_254_to_<header>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 94 | Tokens: 605\n",
      "   Preview: ====================  The <footnote_reference> element is an inline element representing a cross reference to a `\\<footnote>`_ (a footnote mark).  :Category:   `Inline Elements`_ :Analogues:  The <foo...\n",
      "31. sections_content_block_265_to_<image>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 74 | Tokens: 416\n",
      "   Preview: ========  The <header> element is a container element whose contents are meant to appear at the top of a web page, or at the top of every printed page.  :Category:   `Decorative Elements`_ :Analogues:...\n",
      "32. sections_content_block_275_to_content_block_282\n",
      "   Type: grouped_sections | Level: 1 | Lines: 70 | Tokens: 516\n",
      "   Preview: =======  The <image> element refers to an image resource that should be included in the document.  :Categories: `Body Elements`_, `Inline Elements`_ :Analogues:  <image> is analogous to the `HTML \\<im...\n",
      "33. sections_content_block_283_to_<legend>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 94 | Tokens: 550\n",
      "   Preview: .. Important::         * Wash behind your ears.        * Clean up your room.        * Back up your data.  Pseudo-XML_ fragment from simple parsing::      <important>         <bullet_list>...\n",
      "34. sections_content_block_294_to_Examples\n",
      "   Type: grouped_sections | Level: 1 | Lines: 71 | Tokens: 477\n",
      "   Preview: ========  The <legend> element contains an extended description of a `\\<figure>`_. It may complement or replace the figure `\\<caption>`_.  :Category:   `Body Subelements`_ (compound) :Analogues:  The...\n",
      "35. sections_content_block_304_to_Examples\n",
      "   Type: grouped_sections | Level: 1 | Lines: 78 | Tokens: 480\n",
      "   Preview: --------  A reStructuredText `line block`_::      Take it away, Eric the Orchestra Leader!      | A one, two, a one two three four     |     | Half a bee, philosophically,     |     must, *ipso facto*...\n",
      "36. sections_content_block_308_to_Details\n",
      "   Type: grouped_sections | Level: 1 | Lines: 95 | Tokens: 538\n",
      "   Preview: --------  A reStructuredText `enumerated list`_ with a nested `bullet list`_::      1. Outer list, item 1.         * Inner list, item 1.        * Inner list, item 2.      2. Outer list, item 2.  Pseud...\n",
      "37. sections_content_block_317_to_<math>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 73 | Tokens: 473\n",
      "   Preview: -------  :Category:   `Simple Body Elements`_ :Analogues:  <literal_block> is analogous to the HTML_ <pre> element              and to the DocBook_ <programlisting> and <screen> elements. :Processing:...\n",
      "38. sections_content_block_323_to_<meta>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 79 | Tokens: 517\n",
      "   Preview: ======  The <math> element contains text in `LaTeX math format` [#latex-math]_ that is typeset as mathematical notation (inline formula).  :Category:   `Inline Elements`_ :Analogues:  <math> is relate...\n",
      "39. sections_content_block_332_to_<option>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 90 | Tokens: 581\n",
      "   Preview: ======  The <meta> element is a container for \"hidden\" document bibliographic data, or meta-data (data about the document).  :Category:   `Structural Subelements`_  :Analogues:  <meta> is analogous to...\n",
      "40. sections_content_block_342_to_<option_list>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 78 | Tokens: 470\n",
      "   Preview: ========  The <option> element groups an option string together with zero or more option argument placeholders.  :Category:   `Body Subelements`_ (compound) :Analogues:  <option> has no direct analogu...\n",
      "41. sections_content_block_354_to_Examples\n",
      "   Type: grouped_sections | Level: 1 | Lines: 99 | Tokens: 591\n",
      "   Preview: =============  Each <option_list> element contains a two-column list of command-line options and descriptions, documenting a program's options.  :Category:   `Compound Body Elements`_ :Analogues:  <op...\n",
      "42. sections_content_block_364_to_second section\n",
      "   Type: grouped_sections | Level: 1 | Lines: 117 | Tokens: 598\n",
      "   Preview: -------- See the examples for the `\\<option_list>`_ element.    <organization>  ==============  The <organization> element contains the name of document author's organization, or the organization resp...\n",
      "43. sections_content_block_383_to_<raw>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 88 | Tokens: 598\n",
      "   Preview: ==============  As the ToC can't be generated until the entire document has been parsed, simple parsing adds a `\\<topic>`_ with a <pending> element as placeholder::      <topic classes=\"contents\"...\n",
      "44. sections_content_block_389_to_Examples\n",
      "   Type: grouped_sections | Level: 1 | Lines: 61 | Tokens: 480\n",
      "   Preview: =====  The <raw> element contains non-reStructuredText data that is to be passed untouched to the Writer.  :Category:   `Simple Body Elements`_, `Inline Elements`_ :Analogues:  The <raw> element has n...\n",
      "45. sections_content_block_396_to_<revision>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 79 | Tokens: 537\n",
      "   Preview: --------  In reStructuredText `hyperlink references`_ are indicated by a trailing underscore::      References may use simple_ reference names,     `phrase references`_, or `no reference name`__....\n",
      "46. sections_content_block_399_to_content_block_411\n",
      "   Type: grouped_sections | Level: 1 | Lines: 99 | Tokens: 589\n",
      "   Preview: ==========  The <revision> element contains the revision number of the document. It can be used alone or in conjunction with `\\<version>`_.  :Category:   `Bibliographic Elements`_ :Analogues:  <revisi...\n",
      "47. sections_content_block_412_to_content_block_425\n",
      "   Type: grouped_sections | Level: 1 | Lines: 101 | Tokens: 512\n",
      "   Preview: .. class:: run-in  :Category:   `Structural Elements`_  :Analogues:  <section> is analogous to the <section> elements in HTML_ and              DocBook_.  :Parents:    `\\<document>`_, `\\<section>`_  :...\n",
      "48. sections_content_block_426_to_<strong>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 101 | Tokens: 567\n",
      "   Preview: .. class:: run-in  :Category:   `Structural Elements`_  :Analogues:  <sidebar> is analogous to the DocBook_ <sidebar> element and              the HTML_ <aside> element.  :Processing: A <sidebar> elem...\n",
      "49. sections_content_block_437_to_Examples\n",
      "   Type: grouped_sections | Level: 1 | Lines: 107 | Tokens: 601\n",
      "   Preview: ========  The <strong> element is an inline element representing text that has **strong importance**, **seriousness**, or **urgency**.  :Category:   `Inline Elements`_ :Analogues:  <strong> is analogo...\n",
      "50. sections_content_block_451_to_content_block_464\n",
      "   Type: grouped_sections | Level: 1 | Lines: 102 | Tokens: 597\n",
      "   Preview: --------  In reStructuredText, a lone second-level section title immediately after the “document title” can become the document subtitle::      =======       Title      =======      ----------       S...\n",
      "51. sections_content_block_465_to_<tbody>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 90 | Tokens: 500\n",
      "   Preview: .. contants::  Pseudo-XML_ fragment from simple parsing::      <system_message level=\"3\" line=\"8\" source=\"example.rst\" type=\"ERROR\">         <paragraph>             Unknown directive type \"contant...\n",
      "52. sections_content_block_474_to_<tgroup>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 48 | Tokens: 312\n",
      "   Preview: =======  The <tbody> element identifies the rows that form the *body* of a `\\<table>`_ (as distinct from the header rows). It is defined in the `Exchange Table Model`_.  :Category:   `Body Subelements...\n",
      "53. sections_content_block_482_to_<tip>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 70 | Tokens: 569\n",
      "   Preview: ========  The <tgroup> element identifies a logically complete portion of a `\\<table>`_. It is defined in the `Exchange Table Model`_.  :Category:   `Body Subelements`_ (compound) :Analogues:  <tgroup...\n",
      "54. sections_content_block_490_to_Next section's title\n",
      "   Type: grouped_sections | Level: 1 | Lines: 85 | Tokens: 576\n",
      "   Preview: =====  The <tip> element is a specific *admonition*, a distinctive and self-contained notice.  :Category:   `Compound Body Elements`_ :Analogues:  <tip> is analogous to the `DocBook \\<tip>`_ element....\n",
      "55. sections_content_block_502_to_content_block_508\n",
      "   Type: grouped_sections | Level: 1 | Lines: 62 | Tokens: 353\n",
      "   Preview: ====================  Pseudo-XML_ fragment from simple parsing::      <section ids=\"a-title\" names=\"a\\ title\">         <title>             A Title         <paragraph>             A paragraph....\n",
      "56. sections_content_block_509_to_content_block_515\n",
      "   Type: grouped_sections | Level: 1 | Lines: 68 | Tokens: 446\n",
      "   Preview: .. class:: run-in  :Category:   `Structural Elements`_  :Analogues:  <topic> is analogous to the DocBook_ <simplesect> element              and the HTML_ <aside> element.  :Processing: A <topic> eleme...\n",
      "57. sections_content_block_516_to_<warning>\n",
      "   Type: grouped_sections | Level: 1 | Lines: 99 | Tokens: 596\n",
      "   Preview: .. class:: run-in  :Category:   `Structural Subelements`_ :Analogues:  <transition> is analogous to the HTML_ <hr> element. :Processing: The <transition> element is typically rendered as vertical...\n",
      "58. sections_content_block_526_to_Custom Attribute Types\n",
      "   Type: grouped_sections | Level: 1 | Lines: 68 | Tokens: 353\n",
      "   Preview: =========  The <warning> element is a specific *admonition*, a distinctive and self-contained notice.  :Category:   `Compound Body Elements`_ :Analogues:  <warning> is analogous to the `DocBook \\<warn...\n",
      "59. content_block_536\n",
      "   Type: content_block | Level: 1 | Lines: 65 | Tokens: 639\n",
      "   Preview: ======================  To highlight specific attribute value constraints, the `Docutils Generic DTD`_ defines *custom attribute types* via `parameter entities`_ that resolve to standard attribute typ...\n",
      "60. sections_Common Attributes_to_``classes``\n",
      "   Type: grouped_sections | Level: 1 | Lines: 100 | Tokens: 560\n",
      "   Preview: Common Attributes  =================  Through the `%basic.atts`_ parameter entity, all elements except `\\<meta>`_ support the attributes ids_, names_ or dupnames_, source_, and classes_.   -----------...\n",
      "61. sections_content_block_555_to_``colsep``\n",
      "   Type: grouped_sections | Level: 1 | Lines: 94 | Tokens: 588\n",
      "   Preview: ===========  Attribute type: `%classnames.type`_.  Default value: none.  The ``classes`` attribute is a space separated list containing zero or more `class names`_. It is one of the `common attributes...\n",
      "62. sections_content_block_567_to_content_block_576\n",
      "   Type: grouped_sections | Level: 1 | Lines: 77 | Tokens: 524\n",
      "   Preview: ==========  Attribute type: `%yesorno`_.  Default value: none.  The ``colsep`` attribute is used in the `\\<colspec>`_, `\\<entry>`_, `\\<table>`_, and `\\<tgroup>`_ elements to specify the presence or ab...\n",
      "63. sections_content_block_577_to_``loading``\n",
      "   Type: grouped_sections | Level: 1 | Lines: 97 | Tokens: 530\n",
      "   Preview: .. class:: field-indent-8em      :arabic:     1, 2, 3, ...     :loweralpha: a, b, c, ..., z     :upperalpha: A, B, C, ..., Z     :lowerroman: i, ii, iii, iv, ..., mmmmcmxcix [4999]     :upperroman...\n",
      "64. sections_content_block_593_to_``names``\n",
      "   Type: grouped_sections | Level: 1 | Lines: 73 | Tokens: 479\n",
      "   Preview: ===========  Attribute type: EnumeratedType_, one of \"embed\", \"link\", or \"lazy\". Default value: none.  The ``loading`` attribute is used in the `\\<image>`_ and `\\<figure>`_ elements to indicate the pr...\n",
      "65. sections_content_block_601_to_content_block_611\n",
      "   Type: grouped_sections | Level: 1 | Lines: 92 | Tokens: 605\n",
      "   Preview: =========  Attribute type: `%refnames.type`_.  Default value: none.  The ``names`` attribute is a space-separated list containing `reference names`_ of an element (spaces inside a name are backslash-e...\n",
      "66. sections_``refname``_to_``start``\n",
      "   Type: grouped_sections | Level: 1 | Lines: 93 | Tokens: 599\n",
      "   Preview: ``refname``  ===========  Attribute type: `%refname.type`_.  Default value: none.  The ``refname`` attribute contains a reference to one of the `names`_ of another element. It is used by the `\\<citati...\n",
      "67. sections_content_block_626_to_``uri``\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 474\n",
      "   Preview: =========  Attribute type: `%number`_.  Default value: none (implies 1).  The ``start`` attribute is used in the `\\<enumerated_list>`_ element to store the ordinal value of the first item in the list,...\n",
      "68. sections_content_block_636_to_``%align-h.att``\n",
      "   Type: grouped_sections | Level: 1 | Lines: 91 | Tokens: 597\n",
      "   Preview: =======  Attribute type: `CDATA`_.  Default value: none.  The ``uri`` attribute is used in the `\\<image>`_ and `\\<figure>`_ elements to refer to the image via a `URI Reference`_ [#]_. [rfc3986]_  Docu...\n",
      "69. sections_content_block_649_to_``%bodyatt``\n",
      "   Type: grouped_sections | Level: 1 | Lines: 106 | Tokens: 536\n",
      "   Preview: ----------------  The ``%align-h.att`` parameter entity contains the align_ attribute for horizontal alignment.  Entity definition::      align (left | center | right) #IMPLIED  The `\\<figure>`_ and `...\n",
      "70. sections_content_block_662_to_``%refname.att``\n",
      "   Type: grouped_sections | Level: 1 | Lines: 75 | Tokens: 485\n",
      "   Preview: ------------  The ``%bodyatt`` parameter entity is defined in the `Exchange Table Model`_ to allow customization of the `\\<table>`_ element's attribute list.  The `Docutils Generic DTD`_ redefines it...\n",
      "71. sections_content_block_671_to_``%tbl.tbody.att``\n",
      "   Type: grouped_sections | Level: 1 | Lines: 90 | Tokens: 553\n",
      "   Preview: ----------------  The ``%refname.att`` parameter entity contains the refname_ attribute, an internal reference to the `names`_ attribute of another element.  On a `\\<target>`_ element, ``refname`` ind...\n",
      "72. sections_content_block_683_to_``%inline.elements``\n",
      "   Type: grouped_sections | Level: 1 | Lines: 79 | Tokens: 546\n",
      "   Preview: ------------------  The ``%tbl.tbody.att`` parameter entity is defined in the `Exchange Table Model`_ to allow customization of the `\\<tbody>`_ element's attribute list.  The `Docutils Generic DTD`_ r...\n",
      "73. sections_content_block_695_to_content_block_704\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 460\n",
      "   Preview: --------------------  The ``%inline.elements`` parameter entity contains an OR-list of all `Inline Elements`_. The `%additional.inline.elements`_ placeholder can be used by wrapper DTDs to extend ``%i...\n",
      "74. sections_content_block_705_to_content_block_711\n",
      "   Type: grouped_sections | Level: 1 | Lines: 67 | Tokens: 422\n",
      "   Preview: .. parsed-literal::     ( ( (`%body.elements`_; | topic | sidebar)+, transition? )*,      ( (`%section.elements`_;),        (transition?, (`%section.elements`_;) )* )? )  to impose the following restr...\n",
      "  75. sections_content_block_712_to_content_block_717_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 111 | Tokens: 1028\n",
      "     Preview: .. parsed-literal::      (#PCDATA | `%inline.elements`_;)*  The ``%text.model`` parameter entity is directly employed in the content models of the following elements: `\\<abbreviation>`_, `\\<acronym>`_...\n",
      "  76. sections_content_block_712_to_content_block_717_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 55 | Tokens: 420\n",
      "     Preview: Pseudo-XML::          <document>             <section ids=\"a-title\" names=\"a title\">                 <title>                     A Title                 <paragraph>                     A paragraph....\n",
      "  77. sections_Bibliography_to_content_block_719_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 85 | Tokens: 1030\n",
      "     Preview: Bibliography  ------------  .. [docutils.dtd] `Docutils Generic DTD`,                   David Goodger,                   https://docutils.sourceforge.io/docs/ref/docutils.dtd.  .. [dpub-aria]    `Digi...\n",
      "  78. sections_Bibliography_to_content_block_719_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 74 | Tokens: 1058\n",
      "     Preview: .. _DocTitle transform: ../api/transforms.html#doctitle  .. _reader:             ../peps/pep-0258.html#readers .. _severity level:     ../peps/pep-0258.html#error-handling .. _writer: .. _writers:...\n",
      "  79. sections_Bibliography_to_content_block_719_part_3\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 50 | Tokens: 654\n",
      "     Preview: .. _\"class\" directive:          rst/directives.html#class-1 .. _class option:               rst/directives.html#class-option .. _\"code\" directive:           rst/directives.html#code .. _\"compound\" dir...\n",
      "\n",
      "🔍 Processing: pep-0287.rst\n",
      "🌳 Processing pep-0287.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 33 chunks\n",
      "Created 10 grouped chunks\n",
      "  📦 Sub-chunking sections_Rationale_to_content_block_9 (1040 tokens)\n",
      "  📦 Sub-chunking sections_Specification_to_content_block_13 (1396 tokens)\n",
      "  📦 Sub-chunking sections_content_block_14_to_content_block_23 (1325 tokens)\n",
      "  📦 Sub-chunking sections_Abstract_to_content_block_27 (1525 tokens)\n",
      "Final result: 13 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 13 chunks for pep-0287.rst\n",
      "  ✅ Saved: pep-0287.rst_chunk_001_i8y498.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_002_i8y498.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_003_i8y498.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_004_i8y498.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_005_i8y498.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_006_i8y498.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_007_i8y498.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_008_i8y498.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_009_i8y498.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_010_i8y498.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_011_i8y498.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_012_i8y498.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_013_i8y498.md\n",
      "\n",
      "--- RST Chunk Summary for pep-0287.rst ---\n",
      "1. sections_content_block_1_to_Benefits\n",
      "   Type: grouped_sections | Level: 1 | Lines: 37 | Tokens: 287\n",
      "   Preview: PEP: 287 Title: reStructuredText Docstring Format Version: $Revision$ Last-Modified: $Date$ Author: David Goodger <goodger@python.org> Discussions-To: <doc-sig@python.org> Status: Draft Type: Informat...\n",
      "2. content_block_5\n",
      "   Type: content_block | Level: 1 | Lines: 54 | Tokens: 606\n",
      "   Preview: ========  Programmers are by nature a lazy breed.  We reuse code with functions, classes, modules, and subsystems.  Through its docstring syntax, Python allows us to document our code from within.  Th...\n",
      "3. Goals\n",
      "   Type: section | Level: 1 | Lines: 1 | Tokens: 1\n",
      "   Preview: Goals\n",
      "4. content_block_7\n",
      "   Type: content_block | Level: 1 | Lines: 72 | Tokens: 621\n",
      "   Preview: =====  These are the generally accepted goals for a docstring format, as discussed in the Doc-SIG:  1. It must be readable in source form by the casual observer.  2. It must be easy to type with any s...\n",
      "  5. sections_Rationale_to_content_block_9_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 110 | Tokens: 1040\n",
      "     Preview: Rationale  =========  The lack of a standard syntax for docstrings has hampered the development of standard tools for extracting and converting docstrings into documentation in standard formats (e.g.,...\n",
      "  6. sections_Specification_to_content_block_13_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 128 | Tokens: 1038\n",
      "     Preview: Specification  =============  The specification and user documentaton for reStructuredText is quite extensive.  Rather than repeating or summarizing it all here, links to the originals are provided....\n",
      "  7. sections_Specification_to_content_block_13_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 37 | Tokens: 358\n",
      "     Preview: The intention of the markup is that there should be little need to   use explicit roles; their use is to be kept to an absolute minimum.  - Markup for \"tagged lists\" or \"label lists\": field lists....\n",
      "  8. sections_content_block_14_to_content_block_23_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 121 | Tokens: 1043\n",
      "     Preview: .. image:: mylogo.png    Substitution definitions allow the power and flexibility of   block-level directives to be shared by inline text.  For example::        The |biohazard| symbol must be us...\n",
      "  9. sections_content_block_14_to_content_block_23_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 31 | Tokens: 281\n",
      "     Preview: deleted.  It is painful to renumber the references, since it has to    be done in two places and can have a cascading effect (insert a    single new reference 1, and every other reference has to be...\n",
      "  10. sections_Abstract_to_content_block_27_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 103 | Tokens: 1043\n",
      "     Preview: Abstract         ========         This PEP proposes adding `frungible doodads`_ to the core.  It        extends PEP 9876 [#pep9876]_ via the BCA [#]_ mechanism.         ...          References...\n",
      "  11. sections_Abstract_to_content_block_27_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 51 | Tokens: 481\n",
      "     Preview: reStructuredText`_:         Unambiguous.  The rules for markup must not be open for        interpretation.  For any given input, there should be one and        only one possible output (including e...\n",
      "12. sections_References & Footnotes_to_Acknowledgements\n",
      "   Type: grouped_sections | Level: 1 | Lines: 77 | Tokens: 571\n",
      "   Preview: References & Footnotes  ======================  .. [#PEP-1] PEP 1, PEP Guidelines, Warsaw, Hylton    (http://www.python.org/peps/pep-0001.html)  .. [#PEP-9] PEP 9, Sample PEP Template, Warsaw    (http...\n",
      "13. content_block_33\n",
      "   Type: content_block | Level: 1 | Lines: 18 | Tokens: 92\n",
      "   Preview: ================  Some text is borrowed from PEP 216, Docstring Format [#PEP-216]_, by Moshe Zadka.  Special thanks to all members past & present of the Python Doc-SIG_.   ..\f Emacs settings     Local...\n",
      "\n",
      "🔍 Processing: pep-0256.rst\n",
      "🌳 Processing pep-0256.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 21 chunks\n",
      "Created 6 grouped chunks\n",
      "Final result: 6 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 6 chunks for pep-0256.rst\n",
      "  ✅ Saved: pep-0256.rst_chunk_001_qiqmff.md\n",
      "  ✅ Saved: pep-0256.rst_chunk_002_qiqmff.md\n",
      "  ✅ Saved: pep-0256.rst_chunk_003_qiqmff.md\n",
      "  ✅ Saved: pep-0256.rst_chunk_004_qiqmff.md\n",
      "  ✅ Saved: pep-0256.rst_chunk_005_qiqmff.md\n",
      "  ✅ Saved: pep-0256.rst_chunk_006_qiqmff.md\n",
      "\n",
      "--- RST Chunk Summary for pep-0256.rst ---\n",
      "1. sections_content_block_1_to_Road Map to the Docstring PEPs\n",
      "   Type: grouped_sections | Level: 1 | Lines: 48 | Tokens: 309\n",
      "   Preview: PEP: 256 Title: Docstring Processing System Framework Version: $Revision$ Last-Modified: $Date$ Author: David Goodger <goodger@python.org> Discussions-To: <doc-sig@python.org> Status: Rejected Type: S...\n",
      "2. sections_content_block_7_to_Rationale\n",
      "   Type: grouped_sections | Level: 1 | Lines: 36 | Tokens: 331\n",
      "   Preview: ==============================  There are many aspects to docstring processing.  The \"Docstring PEPs\" have broken up the issues in order to deal with each of them in isolation, or as close as possible...\n",
      "3. sections_content_block_9_to_PyDoc & Other Existing Systems\n",
      "   Type: grouped_sections | Level: 1 | Lines: 56 | Tokens: 530\n",
      "   Preview: =========  There are standard inline documentation systems for some other languages.  For example, Perl has POD_ (\"Plain Old Documentation\") and Java has Javadoc_, but neither of these mesh with the P...\n",
      "4. sections_content_block_11_to_Specification\n",
      "   Type: grouped_sections | Level: 1 | Lines: 37 | Tokens: 379\n",
      "   Preview: ------------------------------  PyDoc became part of the Python standard library as of release 2.1. It extracts and displays docstrings from within the Python interactive interpreter, from the shell c...\n",
      "5. sections_content_block_13_to_References and Footnotes\n",
      "   Type: grouped_sections | Level: 1 | Lines: 75 | Tokens: 466\n",
      "   Preview: =============  The docstring processing system framework is broken up as follows:  1. Docstring conventions.  Documents issues such as:     - What should be documented where.     - First line is a one...\n",
      "6. sections_content_block_17_to_content_block_21\n",
      "   Type: grouped_sections | Level: 1 | Lines: 68 | Tokens: 460\n",
      "   Preview: ========================  .. [#PEP-287] PEP 287, reStructuredText Docstring Format, Goodger    (http://www.python.org/peps/pep-0287.html)  .. [#PEP-257] PEP 257, Docstring Conventions, Goodger, Van Ro...\n",
      "\n",
      "🔍 Processing: pep-0257.rst\n",
      "🌳 Processing pep-0257.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 21 chunks\n",
      "Created 6 grouped chunks\n",
      "Final result: 6 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 6 chunks for pep-0257.rst\n",
      "  ✅ Saved: pep-0257.rst_chunk_001_pkpcvf.md\n",
      "  ✅ Saved: pep-0257.rst_chunk_002_pkpcvf.md\n",
      "  ✅ Saved: pep-0257.rst_chunk_003_pkpcvf.md\n",
      "  ✅ Saved: pep-0257.rst_chunk_004_pkpcvf.md\n",
      "  ✅ Saved: pep-0257.rst_chunk_005_pkpcvf.md\n",
      "  ✅ Saved: pep-0257.rst_chunk_006_pkpcvf.md\n",
      "\n",
      "--- RST Chunk Summary for pep-0257.rst ---\n",
      "1. sections_content_block_1_to_What is a Docstring?\n",
      "   Type: grouped_sections | Level: 1 | Lines: 53 | Tokens: 310\n",
      "   Preview: PEP: 257 Title: Docstring Conventions Version: $Revision$ Last-Modified: $Date$ Authors: David Goodger <goodger@python.org>,          Guido van Rossum <guido@python.org> Discussions-To: doc-sig@python...\n",
      "2. sections_content_block_9_to_One-line Docstrings\n",
      "   Type: grouped_sections | Level: 1 | Lines: 41 | Tokens: 362\n",
      "   Preview: --------------------  A docstring is a string literal that occurs as the first statement in a module, function, class, or method definition.  Such a docstring becomes the ``__doc__`` special attribute...\n",
      "3. sections_content_block_11_to_Multi-line Docstrings\n",
      "   Type: grouped_sections | Level: 1 | Lines: 46 | Tokens: 339\n",
      "   Preview: --------------------  One-liners are for really obvious cases.  They should really fit on one line.  For example::      def kos_root():         \"\"\"Return the pathname of the KOS root directory.\"\"\"...\n",
      "4. content_block_13\n",
      "   Type: content_block | Level: 1 | Lines: 83 | Tokens: 911\n",
      "   Preview: ----------------------  Multi-line docstrings consist of a summary line just like a one-line docstring, followed by a blank line, followed by a more elaborate description.  The summary line may be use...\n",
      "5. sections_Handling Docstring Indentation_to_References and Footnotes\n",
      "   Type: grouped_sections | Level: 1 | Lines: 73 | Tokens: 497\n",
      "   Preview: Handling Docstring Indentation  ------------------------------  Docstring processing tools will strip a uniform amount of indentation from the second and further lines of the docstring, equal to the m...\n",
      "6. sections_content_block_17_to_content_block_21\n",
      "   Type: grouped_sections | Level: 1 | Lines: 49 | Tokens: 261\n",
      "   Preview: ========================  .. [1] PEP 256, Docstring Processing System Framework, Goodger    (http://www.python.org/peps/pep-0256.html)  .. [2] PEP 258, Docutils Design Specification, Goodger    (http:...\n",
      "\n",
      "🔍 Processing: pep-0258.rst\n",
      "🌳 Processing pep-0258.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 58 chunks\n",
      "Created 15 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_23_to_content_block_25 (1532 tokens)\n",
      "Final result: 16 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 16 chunks for pep-0258.rst\n",
      "  ✅ Saved: pep-0258.rst_chunk_001_s43lms.md\n",
      "  ✅ Saved: pep-0258.rst_chunk_002_s43lms.md\n",
      "  ✅ Saved: pep-0258.rst_chunk_003_s43lms.md\n",
      "  ✅ Saved: pep-0258.rst_chunk_004_s43lms.md\n",
      "  ✅ Saved: pep-0258.rst_chunk_005_s43lms.md\n",
      "  ✅ Saved: pep-0258.rst_chunk_006_s43lms.md\n",
      "  ✅ Saved: pep-0258.rst_chunk_007_s43lms.md\n",
      "  ✅ Saved: pep-0258.rst_chunk_008_s43lms.md\n",
      "  ✅ Saved: pep-0258.rst_chunk_009_s43lms.md\n",
      "  ✅ Saved: pep-0258.rst_chunk_010_s43lms.md\n",
      "  ✅ Saved: pep-0258.rst_chunk_011_s43lms.md\n",
      "  ✅ Saved: pep-0258.rst_chunk_012_s43lms.md\n",
      "  ✅ Saved: pep-0258.rst_chunk_013_s43lms.md\n",
      "  ✅ Saved: pep-0258.rst_chunk_014_s43lms.md\n",
      "  ✅ Saved: pep-0258.rst_chunk_015_s43lms.md\n",
      "  ✅ Saved: pep-0258.rst_chunk_016_s43lms.md\n",
      "\n",
      "--- RST Chunk Summary for pep-0258.rst ---\n",
      "1. sections_content_block_1_to_Publisher\n",
      "   Type: grouped_sections | Level: 1 | Lines: 90 | Tokens: 573\n",
      "   Preview: PEP: 258 Title: Docutils Design Specification Version: $Revision$ Last-Modified: $Date$ Author: David Goodger <goodger@python.org> Discussions-To: <doc-sig@python.org> Status: Rejected Type: Standards...\n",
      "2. sections_content_block_13_to_Readers\n",
      "   Type: grouped_sections | Level: 1 | Lines: 34 | Tokens: 298\n",
      "   Preview: ---------  The ``docutils.core`` module contains a \"Publisher\" facade class and several convenience functions: \"publish_cmdline()\" (for command-line front ends), \"publish_file()\" (for programmatic use...\n",
      "3. sections_content_block_15_to_Transformer\n",
      "   Type: grouped_sections | Level: 1 | Lines: 80 | Tokens: 596\n",
      "   Preview: -------  Readers understand the input context (where the data is coming from), send the whole input or discrete \"chunks\" to the parser, and provide the context to bind the chunks together back into a...\n",
      "4. sections_content_block_19_to_Writers\n",
      "   Type: grouped_sections | Level: 1 | Lines: 63 | Tokens: 508\n",
      "   Preview: -----------  The Transformer class, in ``docutils/transforms/__init__.py``, stores transforms and applies them to documents.  A transformer object is attached to every new document tree.  The Publishe...\n",
      "5. sections_content_block_21_to_Input/Output\n",
      "   Type: grouped_sections | Level: 1 | Lines: 50 | Tokens: 336\n",
      "   Preview: -------  Writers produce the final output (HTML, XML, TeX, etc.).  Writers translate the internal `document tree`_ structure into the final data format, possibly running Writer-specific transforms_ fi...\n",
      "  6. sections_content_block_23_to_content_block_25_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 133 | Tokens: 1031\n",
      "     Preview: ------------  I/O classes provide a uniform API for low-level input and output. Subclasses will exist for a variety of input/output mechanisms. However, they can be considered an implementation detail...\n",
      "  7. sections_content_block_23_to_content_block_25_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 61 | Tokens: 500\n",
      "     Preview: reStructuredText PEPs.      - Package \"docutils.writers.s5_html\" generates S5/HTML slide       shows.      - Package \"docutils.writers.latex2e\" writes LaTeX.      - Package \"docutils.writers.new...\n",
      "8. sections_Front-End Tools_to_Error Handling\n",
      "   Type: grouped_sections | Level: 1 | Lines: 57 | Tokens: 502\n",
      "   Preview: Front-End Tools  ===============  The ``tools/`` directory contains several front ends for common Docutils processing.  See `Docutils Front-End Tools`_ for details.  .. _Docutils Front-End Tools:    h...\n",
      "9. sections_content_block_31_to_Processing Model\n",
      "   Type: grouped_sections | Level: 1 | Lines: 49 | Tokens: 405\n",
      "   Preview: ==============  When the parser encounters an error in markup, it inserts a system message (DTD element \"system_message\").  There are five levels of system messages:  * Level-0, \"DEBUG\": an internal r...\n",
      "10. sections_content_block_35_to_Docstring Extraction Rules\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 579\n",
      "   Preview: ----------------  This model will evolve over time, incorporating experience and discoveries.  1. The PySource Reader uses an Input class to read in Python packages    and modules, into a tree of stri...\n",
      "11. sections_content_block_39_to_Attribute Docstrings\n",
      "   Type: grouped_sections | Level: 1 | Lines: 66 | Tokens: 547\n",
      "   Preview: --------------------------  1. What to examine:     a) If the \"``__all__``\" variable is present in the module being       documented, only identifiers listed in \"``__all__``\" are       examined for do...\n",
      "12. sections_content_block_41_to_Additional Docstrings\n",
      "   Type: grouped_sections | Level: 1 | Lines: 77 | Tokens: 587\n",
      "   Preview: ''''''''''''''''''''  (This is a simplified version of PEP 224 [#PEP-224]_.)  A string literal immediately following an assignment statement is interpreted by the docstring extraction machinery as the...\n",
      "13. sections_content_block_43_to_Choice of Docstring Format\n",
      "   Type: grouped_sections | Level: 1 | Lines: 59 | Tokens: 463\n",
      "   Preview: '''''''''''''''''''''  (This idea was adapted from PEP 216 [#PEP-216]_.)  Many programmers would like to make extensive use of docstrings for API documentation.  However, docstrings do take up space i...\n",
      "14. sections_content_block_46_to_Identifier Cross-References\n",
      "   Type: grouped_sections | Level: 1 | Lines: 34 | Tokens: 371\n",
      "   Preview: --------------------------  Rather than force everyone to use a single docstring format, multiple input formats are allowed by the processing system.  A special variable, ``__docformat__``, may appear...\n",
      "15. sections_content_block_48_to_References and Footnotes\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 551\n",
      "   Preview: ---------------------------  In Python docstrings, interpreted text is used to classify and mark up program identifiers, such as the names of variables, functions, classes, and modules.  If the identi...\n",
      "16. sections_content_block_52_to_content_block_58\n",
      "   Type: grouped_sections | Level: 1 | Lines: 74 | Tokens: 440\n",
      "   Preview: ==========================  .. [#PEP-256] PEP 256, Docstring Processing System Framework, Goodger    (http://www.python.org/peps/pep-0256.html)  .. [#PEP-224] PEP 224, Attribute Docstrings, Lemburg...\n",
      "\n",
      "🔍 Processing: todo-lists.rst\n",
      "🌳 Processing todo-lists.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 27 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for todo-lists.rst\n",
      "  ✅ Saved: todo-lists.rst_chunk_001_01vtpp.md\n",
      "\n",
      "--- RST Chunk Summary for todo-lists.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 182 | Tokens: 983\n",
      "   Preview: .. include:: ../header.rst  ===================  Docutils TODO lists  ===================  TODO lists allow you to create a list of items with checkboxes. In `extended Markdown`_, they are called `tas...\n",
      "\n",
      "🔍 Processing: slide-shows.rst\n",
      "🌳 Processing slide-shows.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 182 chunks\n",
      "Created 12 grouped chunks\n",
      "Final result: 12 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 12 chunks for slide-shows.rst\n",
      "  ✅ Saved: slide-shows.rst_chunk_001_858ir1.md\n",
      "  ✅ Saved: slide-shows.rst_chunk_002_858ir1.md\n",
      "  ✅ Saved: slide-shows.rst_chunk_003_858ir1.md\n",
      "  ✅ Saved: slide-shows.rst_chunk_004_858ir1.md\n",
      "  ✅ Saved: slide-shows.rst_chunk_005_858ir1.md\n",
      "  ✅ Saved: slide-shows.rst_chunk_006_858ir1.md\n",
      "  ✅ Saved: slide-shows.rst_chunk_007_858ir1.md\n",
      "  ✅ Saved: slide-shows.rst_chunk_008_858ir1.md\n",
      "  ✅ Saved: slide-shows.rst_chunk_009_858ir1.md\n",
      "  ✅ Saved: slide-shows.rst_chunk_010_858ir1.md\n",
      "  ✅ Saved: slide-shows.rst_chunk_011_858ir1.md\n",
      "  ✅ Saved: slide-shows.rst_chunk_012_858ir1.md\n",
      "\n",
      "--- RST Chunk Summary for slide-shows.rst ---\n",
      "1. sections_content_block_1_to_content_block_13\n",
      "   Type: grouped_sections | Level: 1 | Lines: 87 | Tokens: 576\n",
      "   Preview: .. include:: <s5defs.txt>  =================================   Easy Slide Shows With reST & S5  =================================  :Authors: David Goodger & Chris Liechti :Date:    $Date$  .. This doc...\n",
      "2. sections_content_block_14_to_content_block_20\n",
      "   Type: grouped_sections | Level: 1 | Lines: 62 | Tokens: 328\n",
      "   Preview: .. topic:: Installation    :class: handout     Prerequisites: Python and the Docutils_ package have to be    installed.  See the `Docutils README`__ file for installation    instructions.     __ https...\n",
      "3. sections_content_block_21_to_content_block_28\n",
      "   Type: grouped_sections | Level: 1 | Lines: 92 | Tokens: 572\n",
      "   Preview: .. list-table::       :header-rows: 1        * - Action         - Key(s)       * - Go to the next slide         - * [Space bar]           * [Return]           * [Enter]           * [Right arrow]...\n",
      "4. sections_content_block_29_to_content_block_43\n",
      "   Type: grouped_sections | Level: 1 | Lines: 105 | Tokens: 605\n",
      "   Preview: .. class:: handout    The text size adjusts relative to the size of your browser window   automatically.  You may need to reload the document after resizing   the window.  The browser and platform a...\n",
      "5. sections_content_block_44_to_content_block_58\n",
      "   Type: grouped_sections | Level: 1 | Lines: 95 | Tokens: 542\n",
      "   Preview: .. class:: incremental  1. Some Docutils features are not supported by some themes.      .. container:: handout        For example, header rendering is not supported by the themes       supplied with...\n",
      "6. sections_content_block_59_to_content_block_81\n",
      "   Type: grouped_sections | Level: 1 | Lines: 115 | Tokens: 567\n",
      "   Preview: .. class:: handout     The default Docutils stylesheet, ``html4css1.css``, will normally    be embedded in the output HTML.  If you choose to link to a    stylesheet instead of embedding, you must...\n",
      "7. sections_content_block_82_to_content_block_105\n",
      "   Type: grouped_sections | Level: 1 | Lines: 132 | Tokens: 586\n",
      "   Preview: .. container:: handout       Sites with other S5 themes:       * http://meyerweb.com/eric/tools/s5/themes/      * http://mozilla.wikicities.com/wiki/Firefox_S5:Designs      * http://lachy.id.au/dev/...\n",
      "8. sections_content_block_106_to_content_block_118\n",
      "   Type: grouped_sections | Level: 1 | Lines: 94 | Tokens: 559\n",
      "   Preview: .. class:: handout     The \"medium-white\" and \"medium-black\" themes feature medium-sized    text.  Up to 8 lines of text are accommodated.   .. list-table::    :class: borderless     * - \"medium-white...\n",
      "9. sections_content_block_119_to_content_block_140\n",
      "   Type: grouped_sections | Level: 1 | Lines: 124 | Tokens: 548\n",
      "   Preview: .. class:: handout     We use the ``--theme-url`` option to refer to the new theme.  Open    your ``<doc>.html`` in a browser to test the new theme.  5. Rinse & repeat.      .. class:: handout...\n",
      "10. sections_content_block_141_to_Classes: Text Colours\n",
      "   Type: grouped_sections | Level: 1 | Lines: 97 | Tokens: 528\n",
      "   Preview: .. container:: handout     This is how the example works.     The animation effects are caused by placing successive images at    the same location, laying each image over the last.  For best    resul...\n",
      "11. sections_content_block_155_to_content_block_172\n",
      "   Type: grouped_sections | Level: 1 | Lines: 111 | Tokens: 603\n",
      "   Preview: =====================  :black:`black` [black], :gray:`gray`, :silver:`silver`, :white:`white` [white], :maroon:`maroon`, :red:`red`, :magenta:`magenta`, :fuchsia:`fuchsia`, :pink:`pink`, :orange:`oran...\n",
      "12. sections_content_block_173_to_content_block_182\n",
      "   Type: grouped_sections | Level: 1 | Lines: 59 | Tokens: 227\n",
      "   Preview: .. class:: handout    Adds a context submenu and a toolbar with a lot of useful   functionality, including the viewing and live editing of   stylesheets, and sizing the browser window.    __ http://...\n",
      "\n",
      "🔍 Processing: tools.rst\n",
      "🌳 Processing tools.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 63 chunks\n",
      "Created 8 grouped chunks\n",
      "Final result: 8 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 8 chunks for tools.rst\n",
      "  ✅ Saved: tools.rst_chunk_001_mjs4ps.md\n",
      "  ✅ Saved: tools.rst_chunk_002_mjs4ps.md\n",
      "  ✅ Saved: tools.rst_chunk_003_mjs4ps.md\n",
      "  ✅ Saved: tools.rst_chunk_004_mjs4ps.md\n",
      "  ✅ Saved: tools.rst_chunk_005_mjs4ps.md\n",
      "  ✅ Saved: tools.rst_chunk_006_mjs4ps.md\n",
      "  ✅ Saved: tools.rst_chunk_007_mjs4ps.md\n",
      "  ✅ Saved: tools.rst_chunk_008_mjs4ps.md\n",
      "\n",
      "--- RST Chunk Summary for tools.rst ---\n",
      "1. sections_content_block_1_to_docutils\n",
      "   Type: grouped_sections | Level: 1 | Lines: 95 | Tokens: 499\n",
      "   Preview: .. include:: ../header.rst  ==========================   Docutils Front-End Tools  ==========================  :Author: David Goodger :Contact: docutils-develop@lists.sourceforge.net :Revision: $Revis...\n",
      "2. sections_content_block_16_to_content_block_20\n",
      "   Type: grouped_sections | Level: 1 | Lines: 68 | Tokens: 554\n",
      "   Preview: --------  :CLI name: ``docutils`` :Readers:  Standalone (default), PEP :Parsers:  reStructuredText (default), Markdown (requires 3rd party packages) :Writers:  html_, html4css1_, html5_ (default), lat...\n",
      "3. sections_content_block_21_to_rst2html5\n",
      "   Type: grouped_sections | Level: 1 | Lines: 67 | Tokens: 500\n",
      "   Preview: .. caution::    Use a specific front end like rst2html4_ or rst2html5_,    if you depend on stability of the generated HTML code    (e.g., because you use a custom style sheet or post-processing    th...\n",
      "4. sections_content_block_27_to_Themes\n",
      "   Type: grouped_sections | Level: 1 | Lines: 46 | Tokens: 313\n",
      "   Preview: ---------  :Reader: Standalone :Parser: reStructuredText :Writer: html5_  The ``rst2html5`` front end reads standalone reStructuredText source files and produces `HTML 5`_ output. Correct rendering of...\n",
      "5. content_block_31\n",
      "   Type: content_block | Level: 1 | Lines: 88 | Tokens: 749\n",
      "   Preview: ``````  Each S5 theme consists of a directory containing several files: stylesheets, JavaScript, and graphics.  These are copied into a ``ui/<theme>`` directory beside the generated HTML. [#copy-theme...\n",
      "6. sections_buildhtml.py_to_rst2xetex\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 502\n",
      "   Preview: buildhtml.py  ------------  :Readers: Standalone, PEP :Parser:  reStructuredText :Writers: html_, html5_, pep_html_ :Config_: `[buildhtml application]`_  The ``buildhtml.py`` script can be found in th...\n",
      "7. sections_content_block_39_to_rst2xml\n",
      "   Type: grouped_sections | Level: 1 | Lines: 99 | Tokens: 564\n",
      "   Preview: ---------  :Reader: Standalone :Parser: reStructuredText :Writer: _`xelatex`  The ``rst2xetex`` front end reads standalone reStructuredText source files and produces `LaTeX` output for processing with...\n",
      "8. sections_content_block_53_to_content_block_63\n",
      "   Type: grouped_sections | Level: 1 | Lines: 87 | Tokens: 449\n",
      "   Preview: -------  :Reader: Standalone :Parser: reStructuredText :Writer: _`XML` (Docutils native)  The ``rst2xml`` front end produces Docutils-native XML output. This can be transformed with standard XML tools...\n",
      "\n",
      "🔍 Processing: config.rst\n",
      "🌳 Processing config.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 420 chunks\n",
      "Created 36 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_418_to_content_block_420 (1360 tokens)\n",
      "Final result: 37 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 37 chunks for config.rst\n",
      "  ✅ Saved: config.rst_chunk_001_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_002_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_003_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_004_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_005_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_006_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_007_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_008_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_009_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_010_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_011_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_012_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_013_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_014_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_015_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_016_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_017_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_018_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_019_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_020_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_021_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_022_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_023_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_024_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_025_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_026_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_027_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_028_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_029_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_030_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_031_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_032_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_033_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_034_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_035_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_036_6y8xs9.md\n",
      "  ✅ Saved: config.rst_chunk_037_6y8xs9.md\n",
      "\n",
      "--- RST Chunk Summary for config.rst ---\n",
      "1. sections_content_block_1_to_content_block_12\n",
      "   Type: grouped_sections | Level: 1 | Lines: 88 | Tokens: 538\n",
      "   Preview: .. include:: ../header.rst  ========================   Docutils Configuration  ========================  :Author: David Goodger :Contact: docutils-develop@lists.sourceforge.net :Revision: $Revision$ :...\n",
      "2. sections_content_block_13_to_Example\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 557\n",
      "   Preview: .. |config| replace:: ``--config``  In addition, configuration files may be explicitly specified with the |config|_ command-line option.  These configuration files are read after the three implicit on...\n",
      "3. sections_content_block_17_to_content_block_19\n",
      "   Type: grouped_sections | Level: 1 | Lines: 36 | Tokens: 195\n",
      "   Preview: ~~~~~~~  This is from the ``tools/docutils.conf`` configuration file supplied with Docutils::      # These entries affect all processing:     [general]     source-link: yes     datestamp: %Y-%m-%d %H:...\n",
      "4. sections_content_block_20_to_datestamp\n",
      "   Type: grouped_sections | Level: 1 | Lines: 81 | Tokens: 590\n",
      "   Preview: .. important:: Any setting may be specified in any section, but only                settings from \"`active sections`_\" will be used.   .. _active sections:  Each Docutils application_ uses a specific...\n",
      "5. sections_content_block_26_to_exit_status_level\n",
      "   Type: grouped_sections | Level: 1 | Lines: 107 | Tokens: 546\n",
      "   Preview: ---------  Include a time/datestamp in the document footer.  Contains a format string for Python's `time.strftime()`__.  Configuration file entry examples::      # Equivalent to --date command-line op...\n",
      "6. sections_content_block_42_to_language_code\n",
      "   Type: grouped_sections | Level: 1 | Lines: 111 | Tokens: 582\n",
      "   Preview: -----------------  A system message level threshold; non-halting system messages at or above this level will produce a non-zero exit status at normal exit.  Exit status is the maximum system message l...\n",
      "7. sections_content_block_58_to_output_encoding_error_handler\n",
      "   Type: grouped_sections | Level: 1 | Lines: 72 | Tokens: 557\n",
      "   Preview: -------------  Case-insensitive `language tag`_ as defined in `BCP 47`_.  Sets the document language, also used for localized directive and role names as well as Docutils-generated text.  A typical la...\n",
      "8. sections_content_block_66_to_root_prefix\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 538\n",
      "   Preview: -----------------------------  The error handler for unencodable characters in the output. Acceptable values are the `Error Handlers`_ of Python's \"codecs\" module, including:  strict     Raise an exce...\n",
      "9. sections_content_block_72_to_content_block_83\n",
      "   Type: grouped_sections | Level: 1 | Lines: 98 | Tokens: 603\n",
      "   Preview: -----------  Base directory, prepended to a filesystem path__ starting with \"/\" when including files with the `\"include\"`_, `\"raw\"`_, or `\"csv-table\"`_ directives. Also applied to the `\"uri\" attribute...\n",
      "10. sections_content_block_84_to_file_insertion_enabled\n",
      "   Type: grouped_sections | Level: 1 | Lines: 102 | Tokens: 516\n",
      "   Preview: .. WARNING:: Some standard class values are required in order to achieve              the expected output rendering; use with caution.  *Default*: empty list.  *Option*: ``--strip-class``.    strip_co...\n",
      "11. sections_content_block_101_to_pep_base_url\n",
      "   Type: grouped_sections | Level: 1 | Lines: 102 | Tokens: 590\n",
      "   Preview: ----------------------  Enable directives inserting the contents of external files, such as `\"include\"`_ directive or the `\"raw\"`_ and `\"csv-table\"`_ directives with option \"url\" or \"file\". A \"warning...\n",
      "12. sections_content_block_117_to_syntax_highlight\n",
      "   Type: grouped_sections | Level: 1 | Lines: 85 | Tokens: 570\n",
      "   Preview: ~~~~~~~~~~~~ Base URL for PEP references.  *Default*: \"https://peps.python.org/\". *Option*: ``--pep-base-url``.   pep_file_url_template  ~~~~~~~~~~~~~~~~~~~~~ Template for PEP file part of URL, interp...\n",
      "13. sections_content_block_129_to_[xml parser]\n",
      "   Type: grouped_sections | Level: 1 | Lines: 94 | Tokens: 583\n",
      "   Preview: ~~~~~~~~~~~~~~~~ Token type names used by Pygments_ when parsing contents of the `\"code\"`_ directive and role.  Supported values:  :long:  Use hierarchy of long token type names. :short: Use short tok...\n",
      "14. sections_content_block_142_to_cloak_email_addresses\n",
      "   Type: grouped_sections | Level: 1 | Lines: 159 | Tokens: 609\n",
      "   Preview: ------------  The `Docutils XML parser` processes an XML representation of a `Docutils Document Tree`_ (e.g. the output of the `Docutils XML writer <[docutils_xml writer]_>`__).  New in Docutils 0.22...\n",
      "15. sections_content_block_181_to_footnote_references\n",
      "   Type: grouped_sections | Level: 1 | Lines: 67 | Tokens: 531\n",
      "   Preview: ~~~~~~~~~~~~~~~~~~~~~ Scramble email addresses to confuse harvesters.  In the reference URI, the \"@\" will be replaced by %-escapes (as of RFC 1738).  In the visible text (link text) of an email refere...\n",
      "16. sections_content_block_190_to_content_block_194\n",
      "   Type: grouped_sections | Level: 1 | Lines: 56 | Tokens: 454\n",
      "   Preview: ~~~~~~~~~~~~~~~~~~~ Format for `footnote references`_, one of \"superscript\" or \"brackets\". See also `footnote_references [latex writers]`_.  Overrides also trim_footnote_reference_space_, if the parse...\n",
      "17. content_block_195\n",
      "   Type: content_block | Level: 1 | Lines: 77 | Tokens: 677\n",
      "   Preview: .. class:: run-in narrow    :blahtexml_: Fast conversion, support for many symbols and environments,                but no \"align\" (or other equation-aligning) environment. (C++)   :LaTeXML_:   Comp...\n",
      "18. sections_stylesheet_to_table_style\n",
      "   Type: grouped_sections | Level: 1 | Lines: 64 | Tokens: 406\n",
      "   Preview: stylesheet  ~~~~~~~~~~ List of CSS stylesheet URLs  (comma-separated_). Used verbatim.  Overrides also stylesheet_path_. [#override]_ See also `stylesheet [latex writers]`_.  *Default*: empty list.  *...\n",
      "19. sections_content_block_203_to_field_name_limit\n",
      "   Type: grouped_sections | Level: 1 | Lines: 89 | Tokens: 581\n",
      "   Preview: ~~~~~~~~~~~ Class value(s) added to all tables_. See also `table_style [latex writers]`_.  The default CSS sylesheets define:  :borderless: no borders around table cells, :align-left, align-center, al...\n",
      "20. sections_content_block_215_to_image_loading\n",
      "   Type: grouped_sections | Level: 1 | Lines: 57 | Tokens: 466\n",
      "   Preview: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" The maximum length (in characters) for one-column `field names`_. Longer field names will span an entire row of the table used to render the field list.  0 indicates \"no limit\".  See...\n",
      "21. sections_content_block_224_to_content_block_241\n",
      "   Type: grouped_sections | Level: 1 | Lines: 94 | Tokens: 583\n",
      "   Preview: \"\"\"\"\"\"\"\"\"\"\"\"\" Indicate at which point images should be loaded.  Overridden by the `\"loading\"`_ option of the `\"image\"`_ and `\"figure\"`_ directives.  Supported values:  :embed: Embed still images into...\n",
      "22. sections_content_block_242_to_documentclass\n",
      "   Type: grouped_sections | Level: 1 | Lines: 107 | Tokens: 596\n",
      "   Preview: .. class:: run-in narrow  :compact_lists_:  disable compact lists. :template__:      ``docutils/writers/s5_html/template.txt`` in the                   installation directory.  For the exact machine-s...\n",
      "23. sections_content_block_262_to_latex_preamble\n",
      "   Type: grouped_sections | Level: 1 | Lines: 107 | Tokens: 572\n",
      "   Preview: ~~~~~~~~~~~~~ Specify LaTeX documentclass.  *Default*: \"article\".  *Option*: ``--documentclass``.   documentoptions  ~~~~~~~~~~~~~~~ Specify document options.  Multiple options can be given, separated...\n",
      "24. sections_content_block_279_to_content_block_288\n",
      "   Type: grouped_sections | Level: 1 | Lines: 82 | Tokens: 579\n",
      "   Preview: ~~~~~~~~~~~~~~ LaTeX code that will be inserted in the document preamble. Can be used to load packages with options or (re-) define LaTeX macros without writing a custom style file.  :Default: writer...\n",
      "25. sections_content_block_289_to_stylesheet\n",
      "   Type: grouped_sections | Level: 1 | Lines: 47 | Tokens: 389\n",
      "   Preview: .. admonition:: Provisional     To be replaced by a dedicated `interpreted text role`_ for references    (cf. TODO__).  *Default*: \"\" (use ``\\hyperref``).  *Option*: ``--reference-label``.  __ ../ref/...\n",
      "26. sections_content_block_295_to_stylesheet_path\n",
      "   Type: grouped_sections | Level: 1 | Lines: 54 | Tokens: 452\n",
      "   Preview: ~~~~~~~~~~ Comma-separated_ list of style files (LaTeX packages). Used verbatim (under Windows, path separators are normalized to forward slashes). Overrides also stylesheet_path__. [#override]_ See a...\n",
      "27. sections_content_block_299_to_content_block_305\n",
      "   Type: grouped_sections | Level: 1 | Lines: 75 | Tokens: 549\n",
      "   Preview: ~~~~~~~~~~~~~~~ List of style files (comma-separated_). Relative paths are expanded if a matching file is found in the stylesheet_dirs__. If embed_stylesheet__ is False, paths are rewritten relative t...\n",
      "28. sections_content_block_306_to_font_encoding\n",
      "   Type: grouped_sections | Level: 1 | Lines: 92 | Tokens: 556\n",
      "   Preview: .. admonition:: Provisional     Name, values, and behaviour may change in future versions or the    option may be removed.  *Default*: empty list (don't use BibTeX).  *Option* ``--use-bibtex``.  .. _B...\n",
      "29. sections_content_block_323_to_text_references\n",
      "   Type: grouped_sections | Level: 1 | Lines: 59 | Tokens: 445\n",
      "   Preview: \"\"\"\"\"\"\"\"\"\"\"\"\" String with `LaTeX font encoding`_.  Multiple encodings can be specified separated by commas. The last value becomes the document default.  *Default*: \"T1\".  *Option*: ``--font-encoding`...\n",
      "30. sections_content_block_332_to_endnotes_end_doc\n",
      "   Type: grouped_sections | Level: 1 | Lines: 100 | Tokens: 569\n",
      "   Preview: ~~~~~~~~~~~~~~~~  Use a text rendering instead of the macros ``UR`` and ``MT`` for reference targets (URI references and email addresses). Some systems (e.g. Solaris troff) do not support these macros...\n",
      "31. sections_content_block_348_to_prune\n",
      "   Type: grouped_sections | Level: 1 | Lines: 117 | Tokens: 604\n",
      "   Preview: ~~~~~~~~~~~~~~~~ Generate endnotes at end of document, not footnotes at bottom of page.  :Default: False. :Options: ``--endnotes-end-doc``, ``--no-endnotes-end-doc``.   generate_oowriter_toc  ~~~~~~~~...\n",
      "32. sections_content_block_370_to_writer\n",
      "   Type: grouped_sections | Level: 1 | Lines: 56 | Tokens: 336\n",
      "   Preview: ~~~~~ List of glob-style patterns [#globbing]_ (colon-separated_). Matching directories are skipped. Values are appended. [#append-values]_  Patterns are expanded similar to path settings [#pwd]_ and...\n",
      "33. sections_content_block_378_to_writer\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 601\n",
      "   Preview: ~~~~~~ `HTML writer`_ version. One of \"html\", \"html4\", \"html5\".  :Default: \"html\" (use Docutils' `default HTML writer`_). :Option:  ``--writer``  New in 0.17. Obsoletes the ``html_writer`` option.  .....\n",
      "34. sections_content_block_386_to_sectnum_prefix\n",
      "   Type: grouped_sections | Level: 1 | Lines: 114 | Tokens: 584\n",
      "   Preview: ~~~~~~ Writer component name. One of \"html\", \"html4\", \"html5\", \"latex\", \"xelatex\", \"odt\", \"xml\", \"pseudoxml\", \"manpage\", \"pep_html\", \"s5\", an alias, or the import name of a plug-in writer module.  *De...\n",
      "35. sections_content_block_408_to_Input Encoding Auto-Detection\n",
      "   Type: grouped_sections | Level: 1 | Lines: 86 | Tokens: 583\n",
      "   Preview: ~~~~~~~~~~~~~~ Stores the value of the `\"sectnum\" directive`_'s \"prefix\" option if sectnum_xform_ is False.  No default. [#SectNum]_   sectnum_start  ~~~~~~~~~~~~~ Stores the value of the `\"sectnum\" d...\n",
      "  36. sections_content_block_418_to_content_block_420_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 105 | Tokens: 1042\n",
      "     Preview: -----------------------------  Up to Docutils 0.21, the input_encoding_ default value was ``None`` and the actual input encoding detected from a `Unicode byte order mark` (BOM_) or an `encoding declar...\n",
      "  37. sections_content_block_418_to_content_block_420_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 28 | Tokens: 317\n",
      "     Preview: .. _literal blocks: ../ref/rst/restructuredtext.html#literal-blocks .. _option lists: ../ref/rst/restructuredtext.html#option-lists .. _tables: ../ref/rst/restructuredtext.html#tables  .. _Docutils HT...\n",
      "\n",
      "🔍 Processing: html.rst\n",
      "🌳 Processing html.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 20 chunks\n",
      "Created 5 grouped chunks\n",
      "Final result: 5 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 5 chunks for html.rst\n",
      "  ✅ Saved: html.rst_chunk_001_564gwe.md\n",
      "  ✅ Saved: html.rst_chunk_002_564gwe.md\n",
      "  ✅ Saved: html.rst_chunk_003_564gwe.md\n",
      "  ✅ Saved: html.rst_chunk_004_564gwe.md\n",
      "  ✅ Saved: html.rst_chunk_005_564gwe.md\n",
      "\n",
      "--- RST Chunk Summary for html.rst ---\n",
      "1. sections_content_block_1_to_html5\n",
      "   Type: grouped_sections | Level: 1 | Lines: 41 | Tokens: 234\n",
      "   Preview: .. include:: ../header.rst  =====================  Docutils HTML writers  =====================   .. contents::  This document describes the HTML writers provided by Docutils.  The default `length uni...\n",
      "2. sections_content_block_8_to_html4css1\n",
      "   Type: grouped_sections | Level: 1 | Lines: 50 | Tokens: 591\n",
      "   Preview: -----  :aliases:   _`html5_polyglot`, xhtml :front-end: rst2html5_ :config:    `[html5 writer]`_  The *html5* writer generates valid XML that conforms to the `HTML standard`_ (`polyglot HTML`_). [#saf...\n",
      "3. sections_content_block_10_to_s5_html\n",
      "   Type: grouped_sections | Level: 1 | Lines: 55 | Tokens: 561\n",
      "   Preview: ---------  :aliases:   html4, html_, xhtml10 :front-end: rst2html4_ :config:    `[html4css1 writer]`_  The HTML Writer module, ``docutils/writers/html4css1.py``, was the first Docutils writer and up t...\n",
      "4. sections_content_block_14_to_.. other references\n",
      "   Type: grouped_sections | Level: 1 | Lines: 71 | Tokens: 573\n",
      "   Preview: ~~~~~~~  :alias:     s5 :front-end: rst2s5_ :config:    `[s5_html writer]`_, `[html4css1 writer]`_  The `s5` writer inherits from html4css1_. It produces XHTML for use with S5_, the “Simple Standards-...\n",
      "5. content_block_20\n",
      "   Type: content_block | Level: 1 | Lines: 42 | Tokens: 433\n",
      "   Preview: ----------------  .. _HTML Compatibility Guidelines: https://www.w3.org/TR/xhtml1/#guidelines .. _transitional version:     https://www.w3.org/TR/xhtml1/#a_dtd_XHTML-1.0-Transitional  .. _polyglot...\n",
      "\n",
      "🔍 Processing: smartquotes.rst\n",
      "🌳 Processing smartquotes.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 22 chunks\n",
      "Created 4 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_9_to_content_block_12 (1800 tokens)\n",
      "Final result: 5 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 5 chunks for smartquotes.rst\n",
      "  ✅ Saved: smartquotes.rst_chunk_001_bxslcd.md\n",
      "  ✅ Saved: smartquotes.rst_chunk_002_bxslcd.md\n",
      "  ✅ Saved: smartquotes.rst_chunk_003_bxslcd.md\n",
      "  ✅ Saved: smartquotes.rst_chunk_004_bxslcd.md\n",
      "  ✅ Saved: smartquotes.rst_chunk_005_bxslcd.md\n",
      "\n",
      "--- RST Chunk Summary for smartquotes.rst ---\n",
      "1. sections_content_block_1_to_content_block_8\n",
      "   Type: grouped_sections | Level: 1 | Lines: 68 | Tokens: 471\n",
      "   Preview: .. include:: ../header.rst  =========================  Smart Quotes for Docutils  =========================  :Author: Günter Milde,          based on SmartyPants by John Gruber, Brad Choate, and Chad...\n",
      "  2. sections_content_block_9_to_content_block_12_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 197 | Tokens: 1053\n",
      "     Preview: .. class:: booktabs  =========  ========= == ========  ========== Input      Output       Input     Output =========  ========= == ========  ========== ``\\\\``     \\\\           ``\\...``  \\... ``\\\"``...\n",
      "  3. sections_content_block_9_to_content_block_12_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 128 | Tokens: 747\n",
      "     Preview: :lv: .. class:: language-lv      \"'Latvian' quotes\"  :nl: .. class:: language-nl      \"'Dutch' quotes\"  :nl-x-altquot: .. class:: language-nl-x-altquot      \"'Dutch' alternative quotes\"      .. # 'nl-...\n",
      "4. sections_Caveats_to_content_block_19\n",
      "   Type: grouped_sections | Level: 1 | Lines: 60 | Tokens: 476\n",
      "   Preview: Caveats  =======   Why You Might Not Want to Use \"Smart\" Quotes in Your Documents  --------------------------------------------------------------  For one thing, you might not care.  Most normal, ment...\n",
      "5. sections_content_block_20_to_content_block_22\n",
      "   Type: grouped_sections | Level: 1 | Lines: 51 | Tokens: 484\n",
      "   Preview: .. class:: language-de-CH     \"Er sagt: 'Ich fass' es nicht.'\"  becomes     «Er sagt: ‹Ich fass› es nicht.›»  with a single closing guillemet in place of the apostrophe.  In such cases, it's best t...\n",
      "\n",
      "🔍 Processing: manpage.rst\n",
      "🌳 Processing manpage.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 22 chunks\n",
      "Created 4 grouped chunks\n",
      "Final result: 4 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 4 chunks for manpage.rst\n",
      "  ✅ Saved: manpage.rst_chunk_001_xalgyu.md\n",
      "  ✅ Saved: manpage.rst_chunk_002_xalgyu.md\n",
      "  ✅ Saved: manpage.rst_chunk_003_xalgyu.md\n",
      "  ✅ Saved: manpage.rst_chunk_004_xalgyu.md\n",
      "\n",
      "--- RST Chunk Summary for manpage.rst ---\n",
      "1. sections_content_block_1_to_Module information\n",
      "   Type: grouped_sections | Level: 1 | Lines: 39 | Tokens: 294\n",
      "   Preview: .. include:: ../header.rst  ==============================   manpage writer for Docutils_  ==============================  :Author: Engelbert Gruber :Contact: docutils-develop@lists.sourceforge.net :R...\n",
      "2. sections_content_block_6_to_SYNOPSIS\n",
      "   Type: grouped_sections | Level: 1 | Lines: 73 | Tokens: 533\n",
      "   Preview: ==================  Man pages are organized into numbered sections.  A system's *intro*\\(1) or *man*\\(1) page lists them.  For example, section 1 documents user commands, and section 3 presents progra...\n",
      "3. sections_content_block_14_to_Conventions\n",
      "   Type: grouped_sections | Level: 1 | Lines: 74 | Tokens: 522\n",
      "   Preview: ========    ``man`` ``[-c|-w|-tZT device] [-adhu7V] [-m system[,...]] [-L locale]``  The *man-db* project's own *man* page, which may be installed as *man*\\(1) or *gman*\\(1), explains the sectional...\n",
      "4. sections_content_block_20_to_content_block_22\n",
      "   Type: grouped_sections | Level: 1 | Lines: 48 | Tokens: 385\n",
      "   Preview: ===========  * Newlines, line breaks, and sentences    One should try to comply with the `troff line break rules`__ after   ``.``, ``?``, and ``!`` punctuation signs in the rST source.  Use a   new li...\n",
      "\n",
      "🔍 Processing: emacs.rst\n",
      "🌳 Processing emacs.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 102 chunks\n",
      "Created 15 grouped chunks\n",
      "Final result: 15 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 15 chunks for emacs.rst\n",
      "  ✅ Saved: emacs.rst_chunk_001_779ffc.md\n",
      "  ✅ Saved: emacs.rst_chunk_002_779ffc.md\n",
      "  ✅ Saved: emacs.rst_chunk_003_779ffc.md\n",
      "  ✅ Saved: emacs.rst_chunk_004_779ffc.md\n",
      "  ✅ Saved: emacs.rst_chunk_005_779ffc.md\n",
      "  ✅ Saved: emacs.rst_chunk_006_779ffc.md\n",
      "  ✅ Saved: emacs.rst_chunk_007_779ffc.md\n",
      "  ✅ Saved: emacs.rst_chunk_008_779ffc.md\n",
      "  ✅ Saved: emacs.rst_chunk_009_779ffc.md\n",
      "  ✅ Saved: emacs.rst_chunk_010_779ffc.md\n",
      "  ✅ Saved: emacs.rst_chunk_011_779ffc.md\n",
      "  ✅ Saved: emacs.rst_chunk_012_779ffc.md\n",
      "  ✅ Saved: emacs.rst_chunk_013_779ffc.md\n",
      "  ✅ Saved: emacs.rst_chunk_014_779ffc.md\n",
      "  ✅ Saved: emacs.rst_chunk_015_779ffc.md\n",
      "\n",
      "--- RST Chunk Summary for emacs.rst ---\n",
      "1. sections_content_block_1_to_Checking situation\n",
      "   Type: grouped_sections | Level: 1 | Lines: 57 | Tokens: 414\n",
      "   Preview: .. -*- coding: utf-8 -*-   .. include:: ../header.rst  ========================================     Emacs Support for reStructuredText  ========================================  :Authors: Stefan Merte...\n",
      "2. sections_content_block_11_to_Switching ``rst-mode`` on\n",
      "   Type: grouped_sections | Level: 1 | Lines: 81 | Tokens: 559\n",
      "   Preview: ------------------  Here are some steps to check your situation:  #. In Emacs_ switch to an empty buffer and try ::       M-x rst-mode     If this works you have ``rst.el`` installed somewhere. You ca...\n",
      "3. sections_content_block_15_to_content_block_17\n",
      "   Type: grouped_sections | Level: 1 | Lines: 80 | Tokens: 540\n",
      "   Preview: -------------------------  By default ``rst-mode`` is switched on for files ending in ``.rst`` or ``.rest``. If in a buffer you want to switch ``rst-mode`` on manually use ::    M-x rst-mode  If you w...\n",
      "4. sections_content_block_18_to_Promoting and Demoting Many Sections\n",
      "   Type: grouped_sections | Level: 1 | Lines: 59 | Tokens: 525\n",
      "   Preview: .. note:: The key bindings have been completely revamped in ``rst.el``           V1.0.0. This was necessary to make room for new           functionality. Some of the old bindings still work but give...\n",
      "5. sections_content_block_24_to_Movements and Selection for Text Blocks\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 554\n",
      "   Preview: ------------------------------------  When you are re-organizing the structure of a document, it can be useful to change the level of a number of section titles. The same key binding can be used to do...\n",
      "6. sections_content_block_36_to_content_block_42\n",
      "   Type: grouped_sections | Level: 1 | Lines: 81 | Tokens: 536\n",
      "   Preview: ---------------------------------------  The understanding of reStructuredText_ of ``rst-mode`` is used to set all the variables influencing Emacs' understanding of paragraphs. Thus all operations on...\n",
      "7. sections_content_block_43_to_Straightening Existing Bullet List Hierarchies\n",
      "   Type: grouped_sections | Level: 1 | Lines: 108 | Tokens: 575\n",
      "   Preview: .. note:: Since Emacs_ V24.4 ``electric-indent-mode`` is globally on.           This breaks indentation in ``rst-mode`` and renders           ``rst-mode`` mostly useless. This is fixed in V1.4.1 of...\n",
      "8. sections_content_block_51_to_Converting Documents from Emacs\n",
      "   Type: grouped_sections | Level: 1 | Lines: 78 | Tokens: 433\n",
      "   Preview: ----------------------------------------------  If you invoke ``rst-straighten-bullets-region`` (``C-c C-l C-s``), the existing bullets in the active region will be replaced to reflect their respectiv...\n",
      "9. sections_content_block_61_to_Navigating Using the Table of Contents\n",
      "   Type: grouped_sections | Level: 1 | Lines: 74 | Tokens: 593\n",
      "   Preview: ===============================  ``rst-mode`` provides a number of functions for running documents being edited through the docutils tools. The key bindings for these commands start with ``C-c C-c``....\n",
      "10. sections_content_block_71_to_content_block_73\n",
      "   Type: grouped_sections | Level: 1 | Lines: 56 | Tokens: 482\n",
      "   Preview: --------------------------------------  When you are editing long documents, it can be a bit difficult to orient yourself in the structure of your text. To that effect, a function is provided that pre...\n",
      "11. sections_content_block_74_to_Customizing Section Title Formatting\n",
      "   Type: grouped_sections | Level: 1 | Lines: 68 | Tokens: 479\n",
      "   Preview: .. contents::   ..       1  Introduction       2  Debugging Solution Patterns         2.1  Recognize That a Bug Exists         2.2  Subdivide and Isolate         2.3  Identify and Verify Assumptions...\n",
      "12. sections_content_block_82_to_Customizing Faces\n",
      "   Type: grouped_sections | Level: 1 | Lines: 53 | Tokens: 476\n",
      "   Preview: ------------------------------------  For a couple of things the reStructuredText_ syntax offers a choice of options on how to do things exactly. Some of these choices influence the operation of ``rst...\n",
      "13. sections_content_block_86_to_Editing Tables: Emacs table mode\n",
      "   Type: grouped_sections | Level: 1 | Lines: 84 | Tokens: 582\n",
      "   Preview: -----------------  The faces used for font-locking can be defined in the ``rst-faces`` customization group. The customization options ending in ``-face`` are only there for backward compatibility so p...\n",
      "14. sections_content_block_98_to_Credits\n",
      "   Type: grouped_sections | Level: 1 | Lines: 66 | Tokens: 599\n",
      "   Preview: --------------------------------  You may want to check out `Emacs table mode`_ to create an edit tables, it allows creating ASCII tables compatible with reStructuredText_.  .. _Emacs table mode: http...\n",
      "15. content_block_102\n",
      "   Type: content_block | Level: 1 | Lines: 28 | Tokens: 261\n",
      "   Preview: =======  Part of the original code of ``rst.el`` has been written by Martin Blais and David Goodger and Wei-Wei Guo. The font-locking came from Stefan Merten.  Most of the code has been modified, enha...\n",
      "\n",
      "🔍 Processing: mailing-lists.rst\n",
      "🌳 Processing mailing-lists.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 12 chunks\n",
      "Created 3 grouped chunks\n",
      "Final result: 3 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 3 chunks for mailing-lists.rst\n",
      "  ✅ Saved: mailing-lists.rst_chunk_001_zhgm4m.md\n",
      "  ✅ Saved: mailing-lists.rst_chunk_002_zhgm4m.md\n",
      "  ✅ Saved: mailing-lists.rst_chunk_003_zhgm4m.md\n",
      "\n",
      "--- RST Chunk Summary for mailing-lists.rst ---\n",
      "1. sections_content_block_1_to_Docutils-users\n",
      "   Type: grouped_sections | Level: 1 | Lines: 44 | Tokens: 345\n",
      "   Preview: .. include:: ../header.rst  =========================   Docutils_ Mailing Lists  =========================  :Author: Lea Wiemann :Contact: docutils-develop@lists.sourceforge.net :Revision: $Revision$...\n",
      "2. sections_content_block_6_to_Docutils-checkins\n",
      "   Type: grouped_sections | Level: 1 | Lines: 61 | Tokens: 506\n",
      "   Preview: --------------  The `Docutils-users mailing list`_ is a place to discuss any issues related to the usage of Docutils and reStructuredText.  (Please be sure to check the FAQ_ first.)  There are several...\n",
      "3. sections_content_block_10_to_content_block_12\n",
      "   Type: grouped_sections | Level: 1 | Lines: 57 | Tokens: 495\n",
      "   Preview: -----------------  All check-ins to the `Subversion repository`_ cause a \"check-in email\" to the `Docutils-checkins list`_.  In order to stay informed about current development, developers are advised...\n",
      "\n",
      "🔍 Processing: latex.rst\n",
      "🌳 Processing latex.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 181 chunks\n",
      "Created 34 grouped chunks\n",
      "Final result: 34 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 34 chunks for latex.rst\n",
      "  ✅ Saved: latex.rst_chunk_001_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_002_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_003_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_004_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_005_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_006_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_007_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_008_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_009_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_010_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_011_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_012_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_013_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_014_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_015_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_016_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_017_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_018_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_019_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_020_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_021_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_022_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_023_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_024_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_025_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_026_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_027_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_028_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_029_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_030_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_031_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_032_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_033_sv2xsw.md\n",
      "  ✅ Saved: latex.rst_chunk_034_sv2xsw.md\n",
      "\n",
      "--- RST Chunk Summary for latex.rst ---\n",
      "1. sections_content_block_1_to_Docutils specific LaTeX macros\n",
      "   Type: grouped_sections | Level: 1 | Lines: 66 | Tokens: 439\n",
      "   Preview: .. include:: ../header.rst  ================================   Generating LaTeX with Docutils  ================================  :Author: Engelbert Gruber, Günter Milde :Contact: docutils-develop@list...\n",
      "2. sections_content_block_11_to_content_block_13\n",
      "   Type: grouped_sections | Level: 1 | Lines: 58 | Tokens: 588\n",
      "   Preview: ------------------------------  Some Docutils objects have no LaTeX counterpart, they will be typeset using a Docutils specific LaTeX *macro* (command, environment, or length) to allow customization....\n",
      "3. sections_content_block_14_to_content_block_21\n",
      "   Type: grouped_sections | Level: 1 | Lines: 83 | Tokens: 604\n",
      "   Preview: .. class:: align-center  ====  =========================  ===================  bp   \"big\" point (`DTP point`)  1 bp  = 1/72 in  cc   cîcero                     1 cc = 12 dd  dd   didôt...\n",
      "4. sections_Classes_to_content_block_23\n",
      "   Type: grouped_sections | Level: 1 | Lines: 50 | Tokens: 407\n",
      "   Preview: Classes  -------  The `\"classes\" attribute`_ is one of the common attributes, shared by all Docutils elements. In HTML, the common use is to provide selection criteria for style rules in CSS styleshee...\n",
      "5. sections_content_block_24_to_Style sheets\n",
      "   Type: grouped_sections | Level: 1 | Lines: 62 | Tokens: 517\n",
      "   Preview: .. rubric:: Notes  * Class arguments may contain numbers and hyphens, which need special   treatment in LaTeX command names (see `class directive`_). The commands   ``\\csname`` and ``\\endcsname`` or t...\n",
      "6. sections_content_block_28_to_Templates\n",
      "   Type: grouped_sections | Level: 1 | Lines: 79 | Tokens: 567\n",
      "   Preview: ````````````  A common way of LaTeX customization is the preparation of custom style sheets, either as simple files with LaTeX code snippets or as home-made `LaTeX packages`_ (see the clsguide_ for an...\n",
      "7. sections_content_block_32_to_admonitions\n",
      "   Type: grouped_sections | Level: 1 | Lines: 77 | Tokens: 507\n",
      "   Preview: `````````  Some customizations require commands at places other than the insertion point of stylesheets or depend on the deletion/replacement of parts of the document. This can be done via a custom te...\n",
      "8. sections_content_block_40_to_content_block_44\n",
      "   Type: grouped_sections | Level: 1 | Lines: 91 | Tokens: 575\n",
      "   Preview: -----------  Admonitions__ are specially marked \"topics\" that can appear anywhere an ordinary body element can.  __ ../ref/rst/directives.html#admonitions  Environment:   ``DUadmonition``    (Command...\n",
      "9. sections_content_block_45_to_document class\n",
      "   Type: grouped_sections | Level: 1 | Lines: 64 | Tokens: 434\n",
      "   Preview: .. role:: sub(subscript)    As \"sub\" inherits from the standard \"subscript\" role, the LaTeX macro   only needs to set the size and shape::      \\newcommand{\\DUrolesub}{\\normalsize\\itshape}  Example...\n",
      "10. sections_content_block_50_to_document title\n",
      "   Type: grouped_sections | Level: 1 | Lines: 61 | Tokens: 522\n",
      "   Preview: --------------  There are hundreds of `LaTeX document classes`_ installed by modern LaTeX distributions, provided by publishers, or available at CTAN_.  Setting:   documentclass_  Popular document cla...\n",
      "11. sections_content_block_54_to_figure and table captions\n",
      "   Type: grouped_sections | Level: 1 | Lines: 81 | Tokens: 544\n",
      "   Preview: --------------  A lone top-level section title is (usually) transformed to the document title (see `section structure`_).  Settings:   doctitle_xform_, documentclass_, use-latex-docinfo_, template_  T...\n",
      "12. sections_content_block_60_to_font\n",
      "   Type: grouped_sections | Level: 1 | Lines: 66 | Tokens: 435\n",
      "   Preview: -------------------------  The caption_ package provides many ways to customise the captions in floating environments like figure and table.  The chngcntr_ package helps to configure the numbering of...\n",
      "13. content_block_65\n",
      "   Type: content_block | Level: 1 | Lines: 99 | Tokens: 852\n",
      "   Preview: ----  The selected text font influences the *look*, the *feel*, and the *readability* of the document (cf. `Typography on the information highway`_). Selecting a suitable font also solves the problem...\n",
      "14. choice of suitable fonts\n",
      "   Type: section | Level: 1 | Lines: 1 | Tokens: 4\n",
      "   Preview: choice of suitable fonts\n",
      "15. content_block_67\n",
      "   Type: content_block | Level: 1 | Lines: 83 | Tokens: 604\n",
      "   Preview: ````````````````````````  With xelatex_/lualatex_, you may use any of the system fonts.  The `LaTeX Font Catalogue`_ provides information and examples for a wide range of fonts available for use with...\n",
      "16. sections_content_block_68_to_font encoding\n",
      "   Type: grouped_sections | Level: 1 | Lines: 56 | Tokens: 466\n",
      "   Preview: .. table:: Font packages for standard Postscript fonts               (cf. `Using common Postscript fonts with LaTeX`_)       ========= ============ ============= ============= =========      Packag...\n",
      "17. sections_content_block_70_to_footnotes\n",
      "   Type: grouped_sections | Level: 1 | Lines: 51 | Tokens: 319\n",
      "   Preview: -------------  LaTeX font encodings are described in detail in the encguide_ which is part of the LaTeX base documentation.  Setting:   font-encoding_  Default:   \"T1\"  Example 1:   Use the (obsolete)...\n",
      "18. sections_content_block_74_to_hyphenation\n",
      "   Type: grouped_sections | Level: 1 | Lines: 65 | Tokens: 537\n",
      "   Preview: ---------  By default, footnotes are set with Docutils-specific wrappers around the standard ``\\footnotemark`` and ``\\footnotetext`` commands.  You can configure the footnote layout similar to standar...\n",
      "19. sections_content_block_76_to_line blocks\n",
      "   Type: grouped_sections | Level: 1 | Lines: 92 | Tokens: 545\n",
      "   Preview: -----------  The amount of hyphenation is influenced by ``\\hyphenpenalty``, setting it to 10000 almost prevents hyphenation. As this produces lines with more space between words one should increase La...\n",
      "20. sections_content_block_85_to_content_block_91\n",
      "   Type: grouped_sections | Level: 1 | Lines: 106 | Tokens: 597\n",
      "   Preview: -----------  In `line blocks`__, newlines and leading whitespace are respected.  Environment:   ``DUlineblock``: special list environment for line blocks  Length:   ``\\DUlineblockindent``: indentation...\n",
      "21. sections_content_block_92_to_page layout\n",
      "   Type: grouped_sections | Level: 1 | Lines: 88 | Tokens: 465\n",
      "   Preview: .. class:: compact      * first item     * second item    The following lines for the `LaTeX preamble`_ use the enumitem_ package to   remove spacing from all lists with class argument \"compact\"::...\n",
      "22. sections_content_block_101_to_page numbering\n",
      "   Type: grouped_sections | Level: 1 | Lines: 72 | Tokens: 573\n",
      "   Preview: -----------  By default, paper size and margin settings are determined by the document class.  The following packages help to configure the page layout:  a) The `typearea`_ package (part of the `KOMA-...\n",
      "23. sections_content_block_105_to_section headings\n",
      "   Type: grouped_sections | Level: 1 | Lines: 110 | Tokens: 580\n",
      "   Preview: --------------  Example:   Number pages by chapter (using the chappg_ package)::      \\usepackage{chappg}    See the `chappg documentation`_ for details.  .. _chappg: https://ctan.org/pkg/chappg .. _c...\n",
      "24. sections_content_block_115_to_size of a \"px\"\n",
      "   Type: grouped_sections | Level: 1 | Lines: 78 | Tokens: 567\n",
      "   Preview: ----------------  Settings: documentclass_, use-part-section_  Section headings are converted into LaTeX macros according to their level, the document class and the value of the use-part-section_ sett...\n",
      "25. sections_content_block_122_to_table width and column widths\n",
      "   Type: grouped_sections | Level: 1 | Lines: 59 | Tokens: 357\n",
      "   Preview: --------------  The `length unit`_ \"px\" is `defined in pdfTeX and LuaTeX`__, the \"XeTeX\" writer uses the ``\\pdfpxdimen`` macro as workaround.  Default:   1 px = 1/72 in  Example:   Set the value to ma...\n",
      "26. sections_content_block_127_to_content_block_128\n",
      "   Type: grouped_sections | Level: 1 | Lines: 58 | Tokens: 532\n",
      "   Preview: -----------------------------  To support automatic line breaks, standard LaTeX tables need to know the column widths. Therefore, the LaTeX writer determines default values for both, relative column w...\n",
      "27. sections_content_block_129_to_title reference role\n",
      "   Type: grouped_sections | Level: 1 | Lines: 63 | Tokens: 593\n",
      "   Preview: .. math::        w = \\frac{100\\,\\%}{40} \\left( n + \\sum_{i=1}^n w_i \\right)     with an upper limit of 100%.     .. For details see `docutils.writers.latex2e.Table.get_colspecs()`.  .. _colwidth: ....\n",
      "28. sections_content_block_134_to_content_block_138\n",
      "   Type: grouped_sections | Level: 1 | Lines: 82 | Tokens: 528\n",
      "   Preview: --------------------  `Title reference`_ is the default `default role`_ for `interpreted text`_.  Command:   ``\\DUroletitlereference``  Default:   use slanted font (``\\textsl``)  Example:   set title...\n",
      "29. sections_content_block_139_to_Bad looking PDF output\n",
      "   Type: grouped_sections | Level: 1 | Lines: 97 | Tokens: 554\n",
      "   Preview: .. class:: field-indent-4em    :utf8:  by the standard `inputenc`_ package with only limited coverage           (mainly accented characters).    :utf8x: supported by the `ucs`_ package covers a wide...\n",
      "30. sections_content_block_151_to_Glyph not defined in PD1 encoding\n",
      "   Type: grouped_sections | Level: 1 | Lines: 62 | Tokens: 451\n",
      "   Preview: ``````````````````````    What I am looking for when I try Docutils is PDF files of high quality.   Unfortunately that never is the case.    So am I just stupid or is there a way to get really high qu...\n",
      "31. sections_content_block_157_to_Why are my images too big?\n",
      "   Type: grouped_sections | Level: 1 | Lines: 65 | Tokens: 600\n",
      "   Preview: `````````````````````````````````  If a section title or other link contains non-Latin (e.g. Cyrillic) characters, the LaTeX log contains lots of warnings like::    Package hyperref Warning: Glyph not...\n",
      "32. sections_content_block_161_to_Search and text extraction\n",
      "   Type: grouped_sections | Level: 1 | Lines: 64 | Tokens: 564\n",
      "   Preview: ``````````````````````````  In LaTeX, the size of the *pixel unit* defaults to 1 px = 1/72 in while the `CSS3 pixel unit`_ is defined as 1 px = 1/96 in.  This is why pixmap images without size specifi...\n",
      "33. sections_content_block_169_to_Footnotes and citations\n",
      "   Type: grouped_sections | Level: 1 | Lines: 54 | Tokens: 427\n",
      "   Preview: ``````````````````````````  Search for text that contains characters outside the ASCII range might fail.  See font_ and `font encoding`_ (as well as `Searching PDF files`_ for background information)....\n",
      "34. sections_content_block_175_to_content_block_181\n",
      "   Type: grouped_sections | Level: 1 | Lines: 65 | Tokens: 455\n",
      "   Preview: ```````````````````````  Initially both were implemented using figure floats, because hyperlinking back and forth seemed to be impossible. Later the `figure directive`_ was added that puts images into...\n",
      "\n",
      "🔍 Processing: odt.rst\n",
      "🌳 Processing odt.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 93 chunks\n",
      "Created 19 grouped chunks\n",
      "Final result: 19 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 19 chunks for odt.rst\n",
      "  ✅ Saved: odt.rst_chunk_001_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_002_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_003_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_004_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_005_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_006_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_007_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_008_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_009_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_010_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_011_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_012_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_013_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_014_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_015_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_016_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_017_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_018_3idkpz.md\n",
      "  ✅ Saved: odt.rst_chunk_019_3idkpz.md\n",
      "\n",
      "--- RST Chunk Summary for odt.rst ---\n",
      "1. sections_content_block_1_to_Command line options\n",
      "   Type: grouped_sections | Level: 1 | Lines: 100 | Tokens: 531\n",
      "   Preview: .. include:: ../header.rst  =======================  ODT Writer for Docutils  =======================  :Author: Dave Kuhlman :Contact: docutils-develop@lists.sourceforge.net :Revision: $Revision$ :Dat...\n",
      "2. content_block_15\n",
      "   Type: content_block | Level: 1 | Lines: 63 | Tokens: 620\n",
      "   Preview: --------------------  The following command line options are specific to ``odtwriter``:  --stylesheet=<URL>      Specify a stylesheet URL, used verbatim.                         Default: writers/odf_o...\n",
      "3. sections_Styles and Classes_to_Styles used by odtwriter\n",
      "   Type: grouped_sections | Level: 1 | Lines: 54 | Tokens: 574\n",
      "   Preview: Styles and Classes  ==================  ``odtwriter`` uses a number of styles that are defined in ``styles.xml`` in the default ``styles.odt``.  This section describes those styles.  Note that with th...\n",
      "4. sections_content_block_19_to_Paragraph styles\n",
      "   Type: grouped_sections | Level: 1 | Lines: 30 | Tokens: 253\n",
      "   Preview: ------------------------  This section describes the styles used by ``odtwriter``.  Note that we do not describe the \"look\" of these styles.  That can be easily changed by using ``oowriter`` to edit t...\n",
      "5. content_block_21\n",
      "   Type: content_block | Level: 1 | Lines: 87 | Tokens: 608\n",
      "   Preview: ~~~~~~~~~~~~~~~~  rststyle-attribution     The style for attributions, for example, the attribution in a     ``.. epigraph::`` directive.  Derived from     ``rststyle-blockquote``.  rststyle-blockinde...\n",
      "6. sections_Character styles_to_Admonition styles\n",
      "   Type: grouped_sections | Level: 1 | Lines: 98 | Tokens: 563\n",
      "   Preview: Character styles  ~~~~~~~~~~~~~~~~  rststyle-emphasis     Emphasis.  Normally rendered as italics.  rststyle-inlineliteral     An inline literal.  rststyle-strong     Strong emphasis.  Normally render...\n",
      "7. sections_content_block_27_to_Table styles\n",
      "   Type: grouped_sections | Level: 1 | Lines: 89 | Tokens: 537\n",
      "   Preview: ~~~~~~~~~~~~~~~~~  rststyle-admon-attention-hdr     The style for the attention admonition header/title.  rststyle-admon-attention-body     The style for the attention admonition body/paragraph.  rsts...\n",
      "8. content_block_32\n",
      "   Type: content_block | Level: 1 | Lines: 41 | Tokens: 407\n",
      "   Preview: ~~~~~~~~~~~~  A table style is generated by ``oowriter`` for each table that you create.  Therefore, ``odtwriter`` attempts to do something similar. These styles are created in the ``content.xml`` doc...\n",
      "9. sections_content_block_33_to_Heading and title styles\n",
      "   Type: grouped_sections | Level: 1 | Lines: 81 | Tokens: 564\n",
      "   Preview: .. class:: rststyle-table-vegetabledata  The default table style will be applied to all tables for which you do not specify a style with the \"..  class::\" directive.  Customize the table prop...\n",
      "10. sections_content_block_39_to_Why custom style names\n",
      "   Type: grouped_sections | Level: 1 | Lines: 74 | Tokens: 412\n",
      "   Preview: ~~~~~~~~~~~~~~~~~~~~~~~~~  rststyle-heading{1|2|3|4|5}     The styles for headings (section titles and sub-titles).  Five     levels of sub-headings are provided: rststyle-heading1 through     rststyl...\n",
      "11. sections_content_block_49_to_How to use custom style names\n",
      "   Type: grouped_sections | Level: 1 | Lines: 30 | Tokens: 289\n",
      "   Preview: ~~~~~~~~~~~~~~~~~~~~~~  Here are a few reasons and ideas:  - Suppose that your organization has a standard set of styles in   OOo ``oowriter`` and suppose that the use of these styles is   required. Y...\n",
      "12. sections_content_block_51_to_Roles\n",
      "   Type: grouped_sections | Level: 1 | Lines: 65 | Tokens: 481\n",
      "   Preview: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  In order to define custom style names and to generate documents that contain them, do the following:   1. Create a configuration file containing a \"Formats\" section.  Th...\n",
      "13. sections_content_block_57_to_Syntax highlighting\n",
      "   Type: grouped_sections | Level: 1 | Lines: 73 | Tokens: 550\n",
      "   Preview: -------  You can use a Docutils custom interpreted text role to attach a character style to an inline area of text.  This capability also enables you to attach a new character style (with a new name)...\n",
      "14. sections_content_block_65_to_content_block_67\n",
      "   Type: grouped_sections | Level: 1 | Lines: 70 | Tokens: 439\n",
      "   Preview: -------------------  ``odtwriter`` can add syntax highlighting to code in code blocks.  In order to activate this, do all of the following:  1. Install `Pygments`_ and ...  2. Use the command line opt...\n",
      "15. sections_content_block_68_to_Images and figures\n",
      "   Type: grouped_sections | Level: 1 | Lines: 82 | Tokens: 488\n",
      "   Preview: .. container:: style-1 style-2 style-3          a block of text  - Only ``style-1`` is used; ``style-2`` and ``style-3`` are ignored.  - ``rststyle-style-1`` must be defined.  It should be an exis...\n",
      "16. sections_content_block_76_to_content_block_81\n",
      "   Type: grouped_sections | Level: 1 | Lines: 66 | Tokens: 579\n",
      "   Preview: ------------------  The ODT Writer only supports fixed `length units`_ (\"cm\", \"mm\", \"in\", \"pc\", \"pt\", \"px) for the size attributes \"width\", and \"height\". The fallback unit (used for attribute values w...\n",
      "17. sections_content_block_82_to_Custom header/footers: inserting page numbers, date, time, etc\n",
      "   Type: grouped_sections | Level: 1 | Lines: 68 | Tokens: 513\n",
      "   Preview: .. meta::        :keywords: reStructuredText, docutils, formatting        :description lang=en: A reST document, contains formatted            text in a formatted style.        :custom_var: Value...\n",
      "18. sections_content_block_89_to_Credits\n",
      "   Type: grouped_sections | Level: 1 | Lines: 74 | Tokens: 407\n",
      "   Preview: ----------------------------------------------------------------  You can specify custom headers and footers for your document from the command line.  These headers and footers can be used to insert f...\n",
      "19. content_block_93\n",
      "   Type: content_block | Level: 1 | Lines: 31 | Tokens: 230\n",
      "   Preview: =======  Stefan Merten designed and implemented the custom style names capability.  Thank you, Stefan.  Michael Schutte supports the Debian GNU/Linux distribution of ``odtwriter``.  Thank you, Michael...\n",
      "\n",
      "🔍 Processing: links.rst\n",
      "🌳 Processing links.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 37 chunks\n",
      "Created 7 grouped chunks\n",
      "Final result: 7 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 7 chunks for links.rst\n",
      "  ✅ Saved: links.rst_chunk_001_em97z6.md\n",
      "  ✅ Saved: links.rst_chunk_002_em97z6.md\n",
      "  ✅ Saved: links.rst_chunk_003_em97z6.md\n",
      "  ✅ Saved: links.rst_chunk_004_em97z6.md\n",
      "  ✅ Saved: links.rst_chunk_005_em97z6.md\n",
      "  ✅ Saved: links.rst_chunk_006_em97z6.md\n",
      "  ✅ Saved: links.rst_chunk_007_em97z6.md\n",
      "\n",
      "--- RST Chunk Summary for links.rst ---\n",
      "1. sections_content_block_1_to_Markdown\n",
      "   Type: grouped_sections | Level: 1 | Lines: 47 | Tokens: 239\n",
      "   Preview: .. include:: ../header.rst  =====================   Docutils_ Link List  =====================  :Author: Lea Wiemann, the Docutils team :Contact: docutils-develop@lists.sourceforge.net :Revision: $Rev...\n",
      "2. sections_content_block_11_to_PDF\n",
      "   Type: grouped_sections | Level: 1 | Lines: 54 | Tokens: 440\n",
      "   Preview: ````````  * `myst-docutils`_ --- the MyST_ Markdown parser for `single page builds`_.    .. _myst-docutils: https://pypi.org/project/myst-docutils/   .. _MyST: https://mystmd.org/guide/quickstart-myst...\n",
      "3. sections_content_block_15_to_Editors\n",
      "   Type: grouped_sections | Level: 1 | Lines: 86 | Tokens: 576\n",
      "   Preview: ```  * RinohType_ --- pure Python PDF writer    .. _RinohType: https://pypi.python.org/pypi/RinohType  * `rst2pdf (reportlab)`__ --- PDF writer based on ReportLab_.    __ https://pypi.org/project/rst2...\n",
      "4. content_block_21\n",
      "   Type: content_block | Level: 1 | Lines: 82 | Tokens: 710\n",
      "   Preview: -------  Editors and IDEs with reStructuredText support.  * Eclipse_ IDE with `ReST Editor`__ plug-in.    .. _Eclipse: https://eclipseide.org/   __  http://resteditor.sourceforge.net/  * Emacs__ exten...\n",
      "5. sections_Related Applications_to_Wikis\n",
      "   Type: grouped_sections | Level: 1 | Lines: 85 | Tokens: 587\n",
      "   Preview: Related Applications  ====================  Applications using Docutils/reStructuredText and helper applications.    Converters  ----------  Alternative implementations to convert between reStructured...\n",
      "6. sections_content_block_29_to_Presentations\n",
      "   Type: grouped_sections | Level: 1 | Lines: 60 | Tokens: 410\n",
      "   Preview: -----  * MoinMoin_ includes a `ReStructuredText parser`__.    .. _MoinMoin: http://moinmo.in/   __ http://moinmo.in/HelpOnParsers/ReStructuredText  * Trac_ supports  `reStructuredText as alternative w...\n",
      "7. sections_content_block_35_to_content_block_37\n",
      "   Type: grouped_sections | Level: 1 | Lines: 39 | Tokens: 301\n",
      "   Preview: -------------  There is native support for `slide shows with S5`__.  __ https://docutils.sourceforge.io/docs/user/slide-shows.s5.html  * InkSlide_ quick and easy presentations using Inkscape_. InkSlid...\n",
      "\n",
      "🔍 Processing: runtime-settings.rst\n",
      "🌳 Processing runtime-settings.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 19 chunks\n",
      "Created 3 grouped chunks\n",
      "Final result: 3 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 3 chunks for runtime-settings.rst\n",
      "  ✅ Saved: runtime-settings.rst_chunk_001_is4mvg.md\n",
      "  ✅ Saved: runtime-settings.rst_chunk_002_is4mvg.md\n",
      "  ✅ Saved: runtime-settings.rst_chunk_003_is4mvg.md\n",
      "\n",
      "--- RST Chunk Summary for runtime-settings.rst ---\n",
      "1. sections_content_block_1_to_attributes\n",
      "   Type: grouped_sections | Level: 1 | Lines: 87 | Tokens: 422\n",
      "   Preview: .. include:: ../header.rst  ===========================   Docutils Runtime Settings  ===========================  :Author: David Goodger, Günter Milde :Contact: docutils-develop@lists.sourceforge.net...\n",
      "2. sections_content_block_13_to_settings_spec\n",
      "   Type: grouped_sections | Level: 1 | Lines: 79 | Tokens: 427\n",
      "   Preview: ----------  .. _SettingsSpec.settings_spec:  `settings_spec`    a sequence of     1. option group title (string or None)     2. description (string or None)     3. option tuples with        a) help te...\n",
      "3. content_block_19\n",
      "   Type: content_block | Level: 1 | Lines: 33 | Tokens: 288\n",
      "   Preview: -------------  The name ``settings_spec`` may refer to  a) an instance of the SettingsSpec_ class, b) the data structure `SettingsSpec.settings_spec`_ which is used to    store settings details, or c)...\n",
      "\n",
      "🔍 Processing: publisher.rst\n",
      "🌳 Processing publisher.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 54 chunks\n",
      "Created 10 grouped chunks\n",
      "Final result: 10 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 10 chunks for publisher.rst\n",
      "  ✅ Saved: publisher.rst_chunk_001_phlvy7.md\n",
      "  ✅ Saved: publisher.rst_chunk_002_phlvy7.md\n",
      "  ✅ Saved: publisher.rst_chunk_003_phlvy7.md\n",
      "  ✅ Saved: publisher.rst_chunk_004_phlvy7.md\n",
      "  ✅ Saved: publisher.rst_chunk_005_phlvy7.md\n",
      "  ✅ Saved: publisher.rst_chunk_006_phlvy7.md\n",
      "  ✅ Saved: publisher.rst_chunk_007_phlvy7.md\n",
      "  ✅ Saved: publisher.rst_chunk_008_phlvy7.md\n",
      "  ✅ Saved: publisher.rst_chunk_009_phlvy7.md\n",
      "  ✅ Saved: publisher.rst_chunk_010_phlvy7.md\n",
      "\n",
      "--- RST Chunk Summary for publisher.rst ---\n",
      "1. sections_content_block_1_to_content_block_11\n",
      "   Type: grouped_sections | Level: 1 | Lines: 86 | Tokens: 557\n",
      "   Preview: .. include:: ../header.rst  ========================   The Docutils Publisher  ========================  :Author: David Goodger :Contact: docutils-develop@lists.sourceforge.net :Date: $Date$ :Revision...\n",
      "2. sections_content_block_12_to_publish_doctree()\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 601\n",
      "   Preview: .. parsed-literal::      def publish_file(source__\\ =None, source_path__\\ =None,                      destination_\\ =None, destination_path__\\ =None,                      reader_\\ =None, reader_name=N...\n",
      "3. sections_content_block_18_to_content_block_25\n",
      "   Type: grouped_sections | Level: 1 | Lines: 83 | Tokens: 578\n",
      "   Preview: -----------------  For programmatic use with `string input`_. Parse input into a `Docutils Document Tree`_ data structure. Return a `nodes.document`_ instance.   .. parsed-literal::      publish_doctr...\n",
      "4. sections_content_block_26_to_HTML4 Writer\n",
      "   Type: grouped_sections | Level: 1 | Lines: 46 | Tokens: 264\n",
      "   Preview: .. contents::    :local:  Example:   post-process the output document with a custom function   ``post_process()`` before encoding with user-customizable   encoding and errors::         def publish_byt...\n",
      "5. content_block_32\n",
      "   Type: content_block | Level: 1 | Lines: 124 | Tokens: 894\n",
      "   Preview: ^^^^^^^^^^^^  _`\"body\"`     Equivalent to `\"fragment\"`_.  It is *not* equivalent to `\"html_body\"`_.  _`\"body_prefix\"`     Contains ::          </head>         <body>         <div class=\"document\" ...>...\n",
      "6. sections_PEP/HTML Writer_to_Parts Provided by the LaTeX Writers\n",
      "   Type: grouped_sections | Level: 1 | Lines: 33 | Tokens: 175\n",
      "   Preview: PEP/HTML Writer  ^^^^^^^^^^^^^^^  The PEP/HTML writer provides the same parts as the `HTML4 writer`_, plus the following:  _`\"pepnum\"`     The PEP number (extracted from the `header preamble`__)....\n",
      "7. sections_content_block_40_to_Entry Point Functions\n",
      "   Type: grouped_sections | Level: 1 | Lines: 84 | Tokens: 580\n",
      "   Preview: ```````````````````````````````````  All parts returned by the LaTeX writers (\"latex\" writer and \"xelatex\" writer) are of data type `str`.  _`\"abstract\"`     Formatted content of the \"abstract\" `bibli...\n",
      "8. sections_content_block_44_to_String I/O\n",
      "   Type: grouped_sections | Level: 1 | Lines: 66 | Tokens: 414\n",
      "   Preview: ---------------------  The functions rst2html(), rst2html4(), rst2html5(), rst2latex(), rst2man(), rst2odt(), rst2pseudoxml(), rst2s5(), rst2xetex(), and rst2xml() are wrappers around `publish_cmdline...\n",
      "9. sections_content_block_50_to_Settings Specification\n",
      "   Type: grouped_sections | Level: 1 | Lines: 66 | Tokens: 450\n",
      "   Preview: ----------  The functions `publish_string()`_, `publish_doctree()`_, `publish_from_doctree()`_, and `publish_parts()`_ use the string I/O interface provided by the `docutils.io.StringInput` and `docut...\n",
      "10. content_block_54\n",
      "   Type: content_block | Level: 1 | Lines: 79 | Tokens: 605\n",
      "   Preview: ----------------------  See also `Runtime Settings`_.  _`settings` : docutils.frontend.Values   Runtime settings object.   If `settings` is passed, it's assumed to be the end result of   `runtime sett...\n",
      "\n",
      "🔍 Processing: transforms.rst\n",
      "🌳 Processing transforms.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 64 chunks\n",
      "Created 8 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_49_to_content_block_64 (1363 tokens)\n",
      "Final result: 9 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 9 chunks for transforms.rst\n",
      "  ✅ Saved: transforms.rst_chunk_001_k5si8k.md\n",
      "  ✅ Saved: transforms.rst_chunk_002_k5si8k.md\n",
      "  ✅ Saved: transforms.rst_chunk_003_k5si8k.md\n",
      "  ✅ Saved: transforms.rst_chunk_004_k5si8k.md\n",
      "  ✅ Saved: transforms.rst_chunk_005_k5si8k.md\n",
      "  ✅ Saved: transforms.rst_chunk_006_k5si8k.md\n",
      "  ✅ Saved: transforms.rst_chunk_007_k5si8k.md\n",
      "  ✅ Saved: transforms.rst_chunk_008_k5si8k.md\n",
      "  ✅ Saved: transforms.rst_chunk_009_k5si8k.md\n",
      "\n",
      "--- RST Chunk Summary for transforms.rst ---\n",
      "1. sections_content_block_1_to_Transforms Listed in Priority Order\n",
      "   Type: grouped_sections | Level: 1 | Lines: 51 | Tokens: 329\n",
      "   Preview: .. include:: ../header.rst  =====================   Docutils Transforms  =====================  :Author: David Goodger :Contact: docutils-develop@lists.sourceforge.net :Revision: $Revision$ :Date: $Da...\n",
      "2. content_block_8\n",
      "   Type: content_block | Level: 1 | Lines: 105 | Tokens: 864\n",
      "   Preview: ===================================  Transform classes each have a `default_priority` attribute which is used by the Transformer to apply transforms in order (low to high). The default priority can be...\n",
      "3. sections_Transform Priority Range Categories_to_Transforms Added by Components\n",
      "   Type: grouped_sections | Level: 1 | Lines: 22 | Tokens: 124\n",
      "   Preview: Transform Priority Range Categories  -----------------------------------  ====  ====  ================================================  Priority ----------  -------------------------------------------...\n",
      "4. sections_content_block_12_to_content_block_17\n",
      "   Type: grouped_sections | Level: 1 | Lines: 98 | Tokens: 540\n",
      "   Preview: ==============================  .. _readers:  readers.Reader:   | universal.StripComments             (740_)   | universal.Decorations               (820_)   | universal.ExposeInternals           (840...\n",
      "5. sections_content_block_18_to_content_block_20\n",
      "   Type: grouped_sections | Level: 1 | Lines: 57 | Tokens: 319\n",
      "   Preview: .. class:: field-indent-12em  :Module: frontmatter_ :Added by:  standalone_ Reader :Default priority_: 340_ :Configuration_ setting: docinfo_xform_ (default: True)  Given a document starting [#pre-doc...\n",
      "6. sections_content_block_21_to_Another Top-Level Title\n",
      "   Type: grouped_sections | Level: 1 | Lines: 106 | Tokens: 576\n",
      "   Preview: .. class:: field-indent-12em  :Module: frontmatter_ :Added by:  standalone_ Reader :Default priority_: 320_ :Configuration_ setting: doctitle_xform_ (default: True)  Under the conditions explained bel...\n",
      "7. sections_content_block_33_to_PreBibliographic Elements\n",
      "   Type: grouped_sections | Level: 1 | Lines: 101 | Tokens: 428\n",
      "   Preview: =======================        Another paragraph.     The DocTitle transform will leave the document tree as-is.    The document has no title. It is recommended to set the    `metadata title`_...\n",
      "  8. sections_content_block_49_to_content_block_64_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 124 | Tokens: 1054\n",
      "     Preview: -------------------------  The document tree elements `\\<comment>`_, `\\<decoration>`_, `\\<footer>`_, `\\<header>`_, `\\<meta>`_, `\\<pending>`_, `\\<raw>`_, `\\<substitution_definition>`_, `\\<subtitle>`_,...\n",
      "  9. sections_content_block_49_to_content_block_64_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 26 | Tokens: 308\n",
      "     Preview: .. _`<topic>`: ../ref/doctree.html#topic .. _metadata title: ../ref/doctree.html#title-attribute  .. reStructuredText Markup Specification .. _bibliographic field: .. _bibliographic fields: ../ref/rst...\n",
      "\n",
      "🔍 Processing: index.rst\n",
      "🌳 Processing index.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 5 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for index.rst\n",
      "  ✅ Saved: index.rst_chunk_001_fkx2d7.md\n",
      "\n",
      "--- RST Chunk Summary for index.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 30 | Tokens: 159\n",
      "   Preview: .. include:: ../header.rst  ================================   Docutils Enhancement Proposals  ================================  A framework for proposing major new features, collecting community inpu...\n",
      "\n",
      "🔍 Processing: ep-template.rst\n",
      "🌳 Processing ep-template.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 26 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for ep-template.rst\n",
      "  ✅ Saved: ep-template.rst_chunk_001_tbk3ci.md\n",
      "\n",
      "--- RST Chunk Summary for ep-template.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 123 | Tokens: 461\n",
      "   Preview: .. include:: header.rst  ==========================  <EP number> — <title>  ==========================  :Author:  <name and optional e-mail; use ``:Authors:`` for a list> :Discussions-To: <current dis...\n",
      "\n",
      "🔍 Processing: header.rst\n",
      "🌳 Processing header.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 2 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for header.rst\n",
      "  ✅ Saved: header.rst_chunk_001_ju8a7z.md\n",
      "\n",
      "--- RST Chunk Summary for header.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 14 | Tokens: 70\n",
      "   Preview: .. Minimal menu bar for inclusion in enhancement proposal sources    in ``docutils/docs/eps/``.     Attention: this is not a standalone document.   .. header::    Docutils__ | Overview__ | `Enhancemen...\n",
      "\n",
      "🔍 Processing: ep-001.rst\n",
      "🌳 Processing ep-001.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 39 chunks\n",
      "Created 6 grouped chunks\n",
      "Final result: 6 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 6 chunks for ep-001.rst\n",
      "  ✅ Saved: ep-001.rst_chunk_001_aodtv6.md\n",
      "  ✅ Saved: ep-001.rst_chunk_002_aodtv6.md\n",
      "  ✅ Saved: ep-001.rst_chunk_003_aodtv6.md\n",
      "  ✅ Saved: ep-001.rst_chunk_004_aodtv6.md\n",
      "  ✅ Saved: ep-001.rst_chunk_005_aodtv6.md\n",
      "  ✅ Saved: ep-001.rst_chunk_006_aodtv6.md\n",
      "\n",
      "--- RST Chunk Summary for ep-001.rst ---\n",
      "1. sections_content_block_1_to_content_block_14\n",
      "   Type: grouped_sections | Level: 1 | Lines: 98 | Tokens: 598\n",
      "   Preview: .. include:: header.rst  =========================================  EP 1 — Docutils EP Purpose and Guidelines  =========================================  :Author:  Günter Milde :Discussions-To: `featu...\n",
      "2. sections_content_block_15_to_content_block_17\n",
      "   Type: grouped_sections | Level: 1 | Lines: 23 | Tokens: 198\n",
      "   Preview: .. class:: field-indent-8em  :Docutils-Version: Number of the first release including the new feature. :Post-History: List of past discussion threads and/or issue tickets. :Type:     One of \"Standard\"...\n",
      "3. content_block_18\n",
      "   Type: content_block | Level: 1 | Lines: 92 | Tokens: 654\n",
      "   Preview: .. class:: description  Motivation:   Clearly explain why the existing specification is inadequate to address   the problem that the proposal solves.    The motivation is critical for proposals that w...\n",
      "4. sections_Backwards Compatibility_to_content_block_34\n",
      "   Type: grouped_sections | Level: 1 | Lines: 110 | Tokens: 585\n",
      "   Preview: Backwards Compatibility  =======================  .. Describe potential impact and severity on pre-existing code.  `Enhancement Proposals` will supplement rather than replace existing communication ch...\n",
      "5. content_block_35\n",
      "   Type: content_block | Level: 1 | Lines: 127 | Tokens: 866\n",
      "   Preview: .. class:: field-indent-3em  :EPs:  used by  :AEP__:  AiiDA__ workflow manager for computational science    __ https://github.com/aiidateam/AEP   __ https://www.aiida.net/  :CEP__:  Cassandra__ distri...\n",
      "6. sections_.. URI references_to_content_block_39\n",
      "   Type: grouped_sections | Level: 1 | Lines: 42 | Tokens: 313\n",
      "   Preview: .. URI references     --------------  .. _Python Enhancement Proposals: .. _PEP: .. _PEPs: https://peps.python.org/ .. _PEP review & resolution:     https://peps.python.org/pep-0001/#pep-review-resolu...\n",
      "\n",
      "🔍 Processing: ep-010.rst\n",
      "🌳 Processing ep-010.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 25 chunks\n",
      "Created 6 grouped chunks\n",
      "Final result: 6 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 6 chunks for ep-010.rst\n",
      "  ✅ Saved: ep-010.rst_chunk_001_yh98u3.md\n",
      "  ✅ Saved: ep-010.rst_chunk_002_yh98u3.md\n",
      "  ✅ Saved: ep-010.rst_chunk_003_yh98u3.md\n",
      "  ✅ Saved: ep-010.rst_chunk_004_yh98u3.md\n",
      "  ✅ Saved: ep-010.rst_chunk_005_yh98u3.md\n",
      "  ✅ Saved: ep-010.rst_chunk_006_yh98u3.md\n",
      "\n",
      "--- RST Chunk Summary for ep-010.rst ---\n",
      "1. sections_content_block_1_to_content_block_8\n",
      "   Type: grouped_sections | Level: 1 | Lines: 56 | Tokens: 319\n",
      "   Preview: .. include:: header.rst  =====================================================  EP 10 — Public API and Backwards Compatibility Policy  =====================================================  :Authors:...\n",
      "2. sections_content_block_9_to_Specification\n",
      "   Type: grouped_sections | Level: 1 | Lines: 42 | Tokens: 432\n",
      "   Preview: .. class:: description  Authors   writing or maintaining reStructuredText documents.  End-Users   of Docutils native `front-end tools`_ (optionally with 3rd-party   drop-in extensions) or alternative...\n",
      "3. sections_content_block_11_to_How to Teach This\n",
      "   Type: grouped_sections | Level: 1 | Lines: 68 | Tokens: 439\n",
      "   Preview: =============  .. Describe the syntax and semantics of any new feature.  Docutils public APIs are:  * the `reStructuredText specification`_,  * the `Docutils document structure`_ (`Docutils Document T...\n",
      "4. sections_content_block_17_to_Open Issues\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 546\n",
      "   Preview: =================  .. How to teach users, new and experienced,    how to apply the proposal to their work.  * Move the API specification_ and the backwards compatibility declaration   to the `Docutils...\n",
      "5. sections_content_block_21_to_References\n",
      "   Type: grouped_sections | Level: 1 | Lines: 46 | Tokens: 358\n",
      "   Preview: ===========  .. Any points that are still being decided/discussed.  * Differentiate between \"core API\" and \"extended API\"?    Cf. the `Docutils Project Policies`_      When Docutils reaches version 1....\n",
      "6. sections_content_block_23_to_content_block_25\n",
      "   Type: grouped_sections | Level: 1 | Lines: 41 | Tokens: 370\n",
      "   Preview: ==========  .. A collection of URLs used as references through the proposal.  .. _API Reference Material for Client-Developers:     ../index.html#api-reference-material-for-client-developers .. _doctr...\n",
      "\n",
      "🔍 Processing: policies.rst\n",
      "🌳 Processing policies.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 53 chunks\n",
      "Created 12 grouped chunks\n",
      "Final result: 12 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 12 chunks for policies.rst\n",
      "  ✅ Saved: policies.rst_chunk_001_yzs6b6.md\n",
      "  ✅ Saved: policies.rst_chunk_002_yzs6b6.md\n",
      "  ✅ Saved: policies.rst_chunk_003_yzs6b6.md\n",
      "  ✅ Saved: policies.rst_chunk_004_yzs6b6.md\n",
      "  ✅ Saved: policies.rst_chunk_005_yzs6b6.md\n",
      "  ✅ Saved: policies.rst_chunk_006_yzs6b6.md\n",
      "  ✅ Saved: policies.rst_chunk_007_yzs6b6.md\n",
      "  ✅ Saved: policies.rst_chunk_008_yzs6b6.md\n",
      "  ✅ Saved: policies.rst_chunk_009_yzs6b6.md\n",
      "  ✅ Saved: policies.rst_chunk_010_yzs6b6.md\n",
      "  ✅ Saved: policies.rst_chunk_011_yzs6b6.md\n",
      "  ✅ Saved: policies.rst_chunk_012_yzs6b6.md\n",
      "\n",
      "--- RST Chunk Summary for policies.rst ---\n",
      "1. sections_content_block_1_to_Python Coding Conventions\n",
      "   Type: grouped_sections | Level: 1 | Lines: 57 | Tokens: 478\n",
      "   Preview: .. include:: ../header.rst  ===========================   Docutils Project Policies  ===========================  :Author: David Goodger; open to all Docutils developers :Contact: docutils-develop@lis...\n",
      "2. sections_content_block_6_to_Sub-Subsection\n",
      "   Type: grouped_sections | Level: 1 | Lines: 86 | Tokens: 565\n",
      "   Preview: =========================  Contributed code will not be refused merely because it does not strictly adhere to these conditions; as long as it's internally consistent, clean, and correct, it probably w...\n",
      "3. sections_content_block_18_to_Branches\n",
      "   Type: grouped_sections | Level: 1 | Lines: 75 | Tokens: 526\n",
      "   Preview: ``````````````        Sub-Sub-Subsection       ..................  * Use two blank lines before each section/subsection/etc. title.  One   blank line is sufficient between immediately adjacent t...\n",
      "4. sections_content_block_24_to_Check-ins\n",
      "   Type: grouped_sections | Level: 1 | Lines: 66 | Tokens: 563\n",
      "   Preview: --------  (These branch policies go into effect with Docutils 0.4.)  The \"docutils\" directory of the **trunk** (a.k.a. the **Docutils core**) is used for active -- but stable, fully tested, and review...\n",
      "5. content_block_28\n",
      "   Type: content_block | Level: 1 | Lines: 68 | Tokens: 775\n",
      "   Preview: ---------  Changes or additions to the Docutils core and maintenance branches carry a commitment to the Docutils user community.  Developers must be prepared to fix and maintain any code they have com...\n",
      "6. sections_Version Identification_to_``docutils.__version_info__``\n",
      "   Type: grouped_sections | Level: 1 | Lines: 14 | Tokens: 90\n",
      "   Preview: Version Identification  ======================  The state of development of the current Docutils codebase is stored in two forms: the sequence `docutils.__version_info__`_ and the `PEP 440`_ conforman...\n",
      "7. content_block_32\n",
      "   Type: content_block | Level: 1 | Lines: 79 | Tokens: 689\n",
      "   Preview: -----------------------------  ``docutils.__version_info__`` is an instance of ``docutils.VersionInfo`` based on collections.namedtuple_. It is modelled on `sys.version_info`_ and has the following at...\n",
      "8. sections_``docutils.__version__``_to_content_block_38\n",
      "   Type: grouped_sections | Level: 1 | Lines: 69 | Tokens: 575\n",
      "   Preview: ``docutils.__version__``  ------------------------  The text string ``docutils.__version__`` is a human readable, `PEP 440`_-conforming version specifier.  For version comparison operations, use `docu...\n",
      "9. sections_content_block_39_to_Setting Up For Docutils Development\n",
      "   Type: grouped_sections | Level: 1 | Lines: 65 | Tokens: 504\n",
      "   Preview: .. note:: The backwards compatibility policy outlined below is a stub.  Docutils' backwards compatibility policy follows the rules for Python in :PEP:`387`.  * The scope of the public API is laid out...\n",
      "10. sections_content_block_43_to_The Sandbox\n",
      "   Type: grouped_sections | Level: 1 | Lines: 38 | Tokens: 177\n",
      "   Preview: ===================================  When making changes to the code, testing_ is a must.  The code should be run to verify that it produces the expected results, and the entire test suite should be r...\n",
      "11. sections_content_block_49_to_Parallel Projects\n",
      "   Type: grouped_sections | Level: 1 | Lines: 53 | Tokens: 453\n",
      "   Preview: -----------  The `sandbox directory`_ is a place to play around, to try out and share ideas.  It's a part of the Subversion repository but it isn't distributed as part of Docutils releases.  Feel free...\n",
      "12. sections_content_block_51_to_content_block_53\n",
      "   Type: grouped_sections | Level: 1 | Lines: 56 | Tokens: 426\n",
      "   Preview: -----------------  Parallel projects contain useful code that is not central to the functioning of Docutils.  Examples are specialized add-ons or plug-ins, and applications of Docutils.  They use Docu...\n",
      "\n",
      "🔍 Processing: website.rst\n",
      "🌳 Processing website.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 14 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for website.rst\n",
      "  ✅ Saved: website.rst_chunk_001_0736vi.md\n",
      "\n",
      "--- RST Chunk Summary for website.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 106 | Tokens: 608\n",
      "   Preview: .. include:: ../header.rst  ===================   Docutils Web Site  ===================  :Author: David Goodger; open to all Docutils developers :Contact: docutils-develop@lists.sourceforge.net :Date...\n",
      "\n",
      "🔍 Processing: distributing.rst\n",
      "🌳 Processing distributing.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 20 chunks\n",
      "Created 3 grouped chunks\n",
      "Final result: 3 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 3 chunks for distributing.rst\n",
      "  ✅ Saved: distributing.rst_chunk_001_9ytum2.md\n",
      "  ✅ Saved: distributing.rst_chunk_002_9ytum2.md\n",
      "  ✅ Saved: distributing.rst_chunk_003_9ytum2.md\n",
      "\n",
      "--- RST Chunk Summary for distributing.rst ---\n",
      "1. sections_content_block_1_to_Python Files\n",
      "   Type: grouped_sections | Level: 1 | Lines: 65 | Tokens: 473\n",
      "   Preview: .. include:: ../header.rst  ===============================   Docutils_ Distributor's Guide  ===============================  :Author: Lea Wiemann :Contact: docutils-develop@lists.sourceforge.net :Rev...\n",
      "2. sections_content_block_8_to_Configuration File\n",
      "   Type: grouped_sections | Level: 1 | Lines: 90 | Tokens: 583\n",
      "   Preview: ============  The Docutils Python files must be installed into the ``site-packages/`` directory of Python.  Installing with pip_ should do the trick, but if you want to place the files yourself, you c...\n",
      "3. sections_content_block_18_to_content_block_20\n",
      "   Type: grouped_sections | Level: 1 | Lines: 21 | Tokens: 137\n",
      "   Preview: ==================  It is possible to have a system-wide configuration file at ``/etc/docutils.conf``.  However, this is usually not necessary.  You should *not* install ``tools/docutils.conf`` into `...\n",
      "\n",
      "🔍 Processing: runtime-settings-processing.rst\n",
      "🌳 Processing runtime-settings-processing.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 11 chunks\n",
      "Created 3 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_1_to_content_block_9 (1599 tokens)\n",
      "Final result: 4 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 4 chunks for runtime-settings-processing.rst\n",
      "  ✅ Saved: runtime-settings-processing.rst_chunk_001_0kfh3g.md\n",
      "  ✅ Saved: runtime-settings-processing.rst_chunk_002_0kfh3g.md\n",
      "  ✅ Saved: runtime-settings-processing.rst_chunk_003_0kfh3g.md\n",
      "  ✅ Saved: runtime-settings-processing.rst_chunk_004_0kfh3g.md\n",
      "\n",
      "--- RST Chunk Summary for runtime-settings-processing.rst ---\n",
      "  1. sections_content_block_1_to_content_block_9_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 161 | Tokens: 1050\n",
      "     Preview: .. include:: ../header.rst  =============================   Runtime Settings Processing  =============================  :Author: David Goodger, Günter Milde :Contact: docutils-develop@lists.sourceforg...\n",
      "  2. sections_content_block_1_to_content_block_9_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 63 | Tokens: 548\n",
      "     Preview: the `defaults` argument passed to ``OptionParser(…)`` in step 5.     This means that the `settings_overrides` argument of the    `convenience functions`_ has priority over all    ``SettingsSpec.set...\n",
      "3. Runtime settings processing for other applications\n",
      "   Type: section | Level: 1 | Lines: 1 | Tokens: 6\n",
      "   Preview: Runtime settings processing for other applications\n",
      "4. content_block_11\n",
      "   Type: content_block | Level: 1 | Lines: 85 | Tokens: 712\n",
      "   Preview: ==================================================  The `convenience functions`_ , ``core.publish_file()``, ``core.publish_string()``, or ``core.publish_parts()`` do not parse the command line for set...\n",
      "\n",
      "🔍 Processing: pysource.rst\n",
      "🌳 Processing pysource.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 10 chunks\n",
      "Created 3 grouped chunks\n",
      "Final result: 3 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 3 chunks for pysource.rst\n",
      "  ✅ Saved: pysource.rst_chunk_001_e6rnfx.md\n",
      "  ✅ Saved: pysource.rst_chunk_002_e6rnfx.md\n",
      "  ✅ Saved: pysource.rst_chunk_003_e6rnfx.md\n",
      "\n",
      "--- RST Chunk Summary for pysource.rst ---\n",
      "1. sections_content_block_1_to_Model\n",
      "   Type: grouped_sections | Level: 1 | Lines: 29 | Tokens: 142\n",
      "   Preview: .. include:: ../header.rst  ======================   Python Source Reader  ====================== :Author: David Goodger :Contact: docutils-develop@lists.sourceforge.net :Revision: $Revision$ :Date: $...\n",
      "2. sections_content_block_6_to_Docstring Extractor\n",
      "   Type: grouped_sections | Level: 1 | Lines: 53 | Tokens: 496\n",
      "   Preview: =====  The Python Source Reader (\"PySource\") model that's evolving in my mind goes something like this:  1. Extract the docstring/namespace [#]_ tree from the module(s) and/or    package(s).     .. [#...\n",
      "3. sections_content_block_8_to_content_block_10\n",
      "   Type: grouped_sections | Level: 1 | Lines: 60 | Tokens: 380\n",
      "   Preview: ===================  We need code that scans a parsed Python module, and returns an ordered tree containing the names, docstrings (including attribute and additional docstrings), and additional info (...\n",
      "\n",
      "🔍 Processing: testing.rst\n",
      "🌳 Processing testing.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 23 chunks\n",
      "Created 5 grouped chunks\n",
      "Final result: 5 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 5 chunks for testing.rst\n",
      "  ✅ Saved: testing.rst_chunk_001_pf49ki.md\n",
      "  ✅ Saved: testing.rst_chunk_002_pf49ki.md\n",
      "  ✅ Saved: testing.rst_chunk_003_pf49ki.md\n",
      "  ✅ Saved: testing.rst_chunk_004_pf49ki.md\n",
      "  ✅ Saved: testing.rst_chunk_005_pf49ki.md\n",
      "\n",
      "--- RST Chunk Summary for testing.rst ---\n",
      "1. sections_content_block_1_to_Testing across multiple Python versions\n",
      "   Type: grouped_sections | Level: 1 | Lines: 64 | Tokens: 436\n",
      "   Preview: .. include:: ../header.rst  ===================   Docutils_ Testing  ===================  :Authors: Lea Wiemann <LeWiemann@gmail.com>;           David Goodger <goodger@python.org>;           Docutils...\n",
      "2. sections_content_block_9_to_Writing New Tests\n",
      "   Type: grouped_sections | Level: 1 | Lines: 78 | Tokens: 567\n",
      "   Preview: ---------------------------------------  A Docutils release has a commitment to support a minimum Python version and beyond (see dependencies__ in README.rst). Before a release is cut, tests must pass...\n",
      "3. sections_content_block_14_to_The Testing Process\n",
      "   Type: grouped_sections | Level: 1 | Lines: 101 | Tokens: 577\n",
      "   Preview: -----------------  When writing new tests, it very often helps to see how a similar test is implemented.  For example, the files in the ``test_parsers/test_rst/`` directory all look very similar.  So...\n",
      "4. sections_content_block_21_to_Creating New Tests\n",
      "   Type: grouped_sections | Level: 1 | Lines: 39 | Tokens: 366\n",
      "   Preview: -------------------  When running ``test_functional.py``, all config files in ``functional/tests/`` are processed.  (Config files whose names begin with an underscore are ignored.)  The current workin...\n",
      "5. content_block_23\n",
      "   Type: content_block | Level: 1 | Lines: 26 | Tokens: 250\n",
      "   Preview: ------------------  In order to create a new test, put the input test file into ``functional/input/``.  Then create a config file in ``functional/tests/`` which sets at least input and output file nam...\n",
      "\n",
      "🔍 Processing: todo.rst\n",
      "🌳 Processing todo.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 136 chunks\n",
      "Created 36 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_8_to_content_block_12 (2091 tokens)\n",
      "  📦 Sub-chunking sections_content_block_17_to_content_block_20 (1472 tokens)\n",
      "  📦 Sub-chunking sections_reStructuredText Parser_to_content_block_42 (2855 tokens)\n",
      "  📦 Sub-chunking sections_content_block_94_to_content_block_96 (1727 tokens)\n",
      "Final result: 42 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 42 chunks for todo.rst\n",
      "  ✅ Saved: todo.rst_chunk_001_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_002_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_003_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_004_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_005_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_006_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_007_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_008_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_009_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_010_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_011_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_012_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_013_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_014_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_015_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_016_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_017_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_018_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_019_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_020_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_021_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_022_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_023_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_024_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_025_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_026_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_027_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_028_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_029_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_030_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_031_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_032_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_033_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_034_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_035_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_036_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_037_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_038_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_039_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_040_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_041_u7y3h8.md\n",
      "  ✅ Saved: todo.rst_chunk_042_u7y3h8.md\n",
      "\n",
      "--- RST Chunk Summary for todo.rst ---\n",
      "1. sections_content_block_1_to_Repository\n",
      "   Type: grouped_sections | Level: 1 | Lines: 69 | Tokens: 397\n",
      "   Preview: .. include:: ../header.rst  ======================   Docutils_ To Do List  ======================  :Author: David Goodger (with input from many); open to all Docutils          developers :Contact: doc...\n",
      "  2. sections_content_block_8_to_content_block_12_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 120 | Tokens: 1045\n",
      "     Preview: ==========  Move to a Git repository.  See `feature requests #58`__ (with pointers to Sphinx issues and discussion).  __ https://sourceforge.net/p/docutils/feature-requests/58/  * From a `post by Davi...\n",
      "  3. sections_content_block_8_to_content_block_12_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 106 | Tokens: 1033\n",
      "     Preview: * Move some general-interest sandboxes out of individuals'   directories, into subprojects?  * Add option for file (and URL) access restriction to make Docutils   usable in Wikis and similar applicati...\n",
      "  4. sections_content_block_8_to_content_block_12_part_3\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 4 | Tokens: 13\n",
      "     Preview: Aahz's idea:        First the ToC::\n",
      "5. sections_content_block_13_to_content_block_15\n",
      "   Type: grouped_sections | Level: 1 | Lines: 38 | Tokens: 241\n",
      "   Preview: .. ToC-list::               Introduction.rst               Objects.rst               Data.rst               Control.rst        Then a sample use::             .. include:: ToC.rst...\n",
      "6. content_block_16\n",
      "   Type: content_block | Level: 1 | Lines: 73 | Tokens: 715\n",
      "   Preview: .. include:: manifest.rst        As I said earlier in chapter :chapter:`objects`, the       reference count gets increased every time a binding is made.  * Add support for _`multiple output file...\n",
      "  7. sections_content_block_17_to_content_block_20_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 118 | Tokens: 1031\n",
      "     Preview: .. |version| include:: version.rst      This document describes version |version| of ...    (cf. Grzegorz Adam Hankiewicz's post from 2014-10-01 in docutils-devel)  * Add an ``:optional: <replacem...\n",
      "  8. sections_content_block_17_to_content_block_20_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 43 | Tokens: 441\n",
      "     Preview: I may have used them instead of rolling my own.)    .. _traits: http://code.enthought.com/traits/   .. _SciPy: http://www.scipy.org/  * Add support for _`plugins`.  * _`Config directories`: Currentl...\n",
      "9. sections_object numbering and object references_to_content_block_24\n",
      "   Type: grouped_sections | Level: 1 | Lines: 70 | Tokens: 543\n",
      "   Preview: object numbering and object references  --------------------------------------  For equations, tables & figures.  These would be the equivalent of DocBook's \"formal\" elements.  In LaTeX, automatic cou...\n",
      "10. sections_content_block_25_to_Python Source Reader\n",
      "   Type: grouped_sections | Level: 1 | Lines: 106 | Tokens: 583\n",
      "   Preview: .. fignum::            :prefix-ref: \"Figure \"            :prefix-caption: \"Fig. \"            :suffix-caption: :      The position of the role (prefix or suffix) could also be utilized    .. _O...\n",
      "11. content_block_37\n",
      "   Type: content_block | Level: 1 | Lines: 105 | Tokens: 770\n",
      "   Preview: ====================  General:  * Analyze Tony Ibbs' PySource code.  * Analyze Doug Hellmann's HappyDoc project.  * Investigate how POD handles literate programming.  * Take the best ideas and integra...\n",
      "  12. sections_reStructuredText Parser_to_content_block_42_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 115 | Tokens: 1028\n",
      "     Preview: reStructuredText Parser  =======================  Also see the `... Or Not To Do?`__ list.  __ rst/alternatives.html#or-not-to-do    Misc  ----  * A list problem::        * foo             * bar...\n",
      "  13. sections_reStructuredText Parser_to_content_block_42_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 129 | Tokens: 1042\n",
      "     Preview: disabled at run-time.  Subclassing is probably not enough because it   makes it difficult to apply multiple extensions.  * Generalize the \"doctest block\" construct (which is overly   Python-centric)...\n",
      "  14. sections_reStructuredText Parser_to_content_block_42_part_3\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 86 | Tokens: 784\n",
      "     Preview: The initial colon (\":\") can be thought of as a type of bullet    We could even have segment titles::        :: title  : title   : title       : segment : segment : segment       : segment : segment...\n",
      "15. sections_Adaptable file extensions_to_Proposals\n",
      "   Type: grouped_sections | Level: 1 | Lines: 58 | Tokens: 419\n",
      "   Preview: Adaptable file extensions  -------------------------   Questions  `````````  Should Docutils support adaptable file extensions in hyperlinks?    In the rST source, sister documents are \".rst\" files. I...\n",
      "16. content_block_48\n",
      "   Type: content_block | Level: 1 | Lines: 39 | Tokens: 400\n",
      "   Preview: `````````  How about using \".*\" to indicate \"choose the most appropriate filename extension\"?  For example::      .. _Another Document: another.*  * My point about using ``.*`` is that any other mecha...\n",
      "17. sections_content_block_49_to_Math Markup\n",
      "   Type: grouped_sections | Level: 1 | Lines: 63 | Tokens: 495\n",
      "   Preview: .. role:: link(rewrite)        :transform: .rst|.html    and then to use it::      for more information see :link:`README.rst`    it would be useful if it supported an additional option   ``:forma...\n",
      "18. sections_content_block_51_to_alternative input formats\n",
      "   Type: grouped_sections | Level: 1 | Lines: 32 | Tokens: 251\n",
      "   Preview: -----------  * Use a \"Transform\" for math format conversions as extensively discussed   in the `math directive issues`__ thread in May 2008?    __ http://osdir.com/ml/text.docutils.devel/2008-05/threa...\n",
      "19. content_block_53\n",
      "   Type: content_block | Level: 1 | Lines: 66 | Tokens: 657\n",
      "   Preview: `````````````````````````  Use a directive option to specify an alternative input format, e.g. (but not limited to):  MathML_   Not for hand-written code but maybe useful when pasted in (or included...\n",
      "20. sections_LaTeX output_to_OpenOffice output\n",
      "   Type: grouped_sections | Level: 1 | Lines: 71 | Tokens: 525\n",
      "   Preview: LaTeX output  ````````````  Which equation environments should be supported by the math directive?  * one line:    + numbered: `equation`   + unnumbered: `equation*`  * multiline (test for ``\\\\`` outs...\n",
      "21. sections_content_block_59_to_Directives\n",
      "   Type: grouped_sections | Level: 1 | Lines: 22 | Tokens: 184\n",
      "   Preview: `````````````````  * The `OpenDocument standard`_ version 1.1 says:      Mathematical content is represented by MathML 2.0    However, putting MathML into an ODP file seems tricky as these   (maybe ou...\n",
      "22. content_block_61\n",
      "   Type: content_block | Level: 1 | Lines: 92 | Tokens: 900\n",
      "   Preview: ----------  Directives below are often referred to as \"module.directive\", the directive function.  The \"module.\" is not part of the directive name when used in a document.  * Allow for field lists in...\n",
      "23. content_block_62\n",
      "   Type: content_block | Level: 1 | Lines: 12 | Tokens: 106\n",
      "   Preview: .. include::              :url: https://www.example.org/inclusion.rst      - Strip blank lines from begin and end of a literal included file or       file section. This would correspond to t...\n",
      "24. content_block_63\n",
      "   Type: content_block | Level: 1 | Lines: 68 | Tokens: 598\n",
      "   Preview: .. raw:: html            :destination: head             <link ...>      It needs thought & discussion though, to come up with a consistent     set of destination labels and consistent behavior...\n",
      "25. sections_content_block_64_to_content_block_83\n",
      "   Type: grouped_sections | Level: 1 | Lines: 98 | Tokens: 511\n",
      "   Preview: .. directive:: incr              .. class:: incremental           .. incr::          \"``.. incr::``\" above is equivalent to \"``.. class:: incremental``\".      Another example::           .. di...\n",
      "26. sections_content_block_84_to_Interpreted Text\n",
      "   Type: grouped_sections | Level: 1 | Lines: 64 | Tokens: 487\n",
      "   Preview: .. eqn::             .EQ            delim %%            .EN            %sum from i=o to inf c sup i~=~lim from {m -> inf}            sum from i=0 to m sup i%            .EQ            delim of...\n",
      "27. sections_content_block_87_to_content_block_88\n",
      "   Type: grouped_sections | Level: 1 | Lines: 51 | Tokens: 447\n",
      "   Preview: ----------------  Interpreted text is entirely a reStructuredText markup construct, a way to get around built-in limitations of the medium.  Some roles are intended to introduce new doctree elements,...\n",
      "28. content_block_89\n",
      "   Type: content_block | Level: 1 | Lines: 60 | Tokens: 462\n",
      "   Preview: .. role:: red(raw-formatting)            :prefix:                :html: <font color=\"red\">                :latex: {\\color{red}            :suffix:                :html: </font>...\n",
      "29. sections_content_block_90_to_Doctree pruning\n",
      "   Type: grouped_sections | Level: 1 | Lines: 67 | Tokens: 498\n",
      "   Preview: .. acronyms::                reST                   reStructuredText               DPS                   Docstring Processing System         Would this list remain in the document as a glos...\n",
      "  30. sections_content_block_94_to_content_block_96_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 115 | Tokens: 1023\n",
      "     Preview: ---------------  [DG 2017-01-02: These are not definitive to-dos, just one developer's opinion. Added 2009-10-13 by Günter Milde, in r6178.] [Updated by GM 2017-02-04]  The number of doctree nodes can...\n",
      "  31. sections_content_block_94_to_content_block_96_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 83 | Tokens: 703\n",
      "     Preview: unique; and/or   * duplicate footnote numbers that need to be renumbered.    Should this be done before or after reference-resolving transforms   are applied?  What about references from within on...\n",
      "32. HTML Writer\n",
      "   Type: section | Level: 1 | Lines: 1 | Tokens: 2\n",
      "   Preview: HTML Writer\n",
      "33. content_block_98\n",
      "   Type: content_block | Level: 1 | Lines: 69 | Tokens: 635\n",
      "   Preview: ===========  * Make the _`list compacting` logic more generic: For example, allow   for literal blocks or line blocks inside of compact list items.    This is not implementable as long as list compact...\n",
      "34. sections_PEP/HTML Writer_to_content_block_102\n",
      "   Type: grouped_sections | Level: 1 | Lines: 41 | Tokens: 309\n",
      "   Preview: PEP/HTML Writer  ===============  * Remove the generic style information (duplicated from html4css1.css)   from pep.css to avoid redundancy.    Set ``stylesheet-path`` to \"html4css.css,pep.css\" and th...\n",
      "35. sections_content_block_103_to_Bug fixes\n",
      "   Type: grouped_sections | Level: 1 | Lines: 71 | Tokens: 566\n",
      "   Preview: .. Note:: This item and the following items are partially      accomplished by the S5 1.2 code (currently in alpha), which has      not yet been integrated into Docutils.  * Speaker's notes -- how t...\n",
      "36. sections_content_block_109_to_Footnotes\n",
      "   Type: grouped_sections | Level: 1 | Lines: 64 | Tokens: 387\n",
      "   Preview: ---------  * Too deeply nested lists fail: generate a warning and provide   a workaround.    2017-02-09 this is fixed for enumeration in 0.13.1    for others, cf. sandbox/latex-variants/tests/rst-leve...\n",
      "37. content_block_116\n",
      "   Type: content_block | Level: 1 | Lines: 60 | Tokens: 592\n",
      "   Preview: `````````  + True footnotes with LaTeX auto-numbering (as option ``--latex-footnotes``)   (also for target-footnotes):   Write ``\\footnote{<footnote content>}`` at the place of the   ``<footnote_refer...\n",
      "38. sections_Other LaTeX constructs and packages instead of re-implementations_to_Tables\n",
      "   Type: grouped_sections | Level: 1 | Lines: 51 | Tokens: 310\n",
      "   Preview: Other LaTeX constructs and packages instead of re-implementations  `````````````````````````````````````````````````````````````````  * Check the generated source with package `nag`.  * enumitem_ (tex...\n",
      "39. sections_content_block_122_to_Image and figure directives\n",
      "   Type: grouped_sections | Level: 1 | Lines: 57 | Tokens: 511\n",
      "   Preview: ``````  * Improve/simplify logic to set the column width in the output.    + Assumed reST line length for table width setting configurable, or   + use `ltxtable` (a combination of `tabularx` (auto-wid...\n",
      "40. sections_content_block_124_to_problematic URLs\n",
      "   Type: grouped_sections | Level: 1 | Lines: 80 | Tokens: 549\n",
      "   Preview: ```````````````````````````  * compare the test case in:    + `<../../test/functional/input/data/standard.rst>`__   + `<../../test/functional/expected/standalone_rst_html4css1.html>`__   + `<../../tes...\n",
      "41. sections_content_block_132_to_Front-End Tools\n",
      "   Type: grouped_sections | Level: 1 | Lines: 72 | Tokens: 526\n",
      "   Preview: ````````````````  * ^^ LaTeX's special syntax for characters results in \"strange\" replacements   (both with \\href and \\url).    `file with ^^ <../strange^^name>`__:   `<../strange^^name>`__  * Unbalan...\n",
      "42. content_block_136\n",
      "   Type: content_block | Level: 1 | Lines: 31 | Tokens: 179\n",
      "   Preview: ===============  * Parameterize help text & defaults somehow?  Perhaps a callback?  Or   initialize ``settings_spec`` in ``__init__`` or ``init_options``?  * Disable common options that don't apply?...\n",
      "\n",
      "🔍 Processing: hacking.rst\n",
      "🌳 Processing hacking.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 25 chunks\n",
      "Created 5 grouped chunks\n",
      "Final result: 5 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 5 chunks for hacking.rst\n",
      "  ✅ Saved: hacking.rst_chunk_001_yqe2gr.md\n",
      "  ✅ Saved: hacking.rst_chunk_002_yqe2gr.md\n",
      "  ✅ Saved: hacking.rst_chunk_003_yqe2gr.md\n",
      "  ✅ Saved: hacking.rst_chunk_004_yqe2gr.md\n",
      "  ✅ Saved: hacking.rst_chunk_005_yqe2gr.md\n",
      "\n",
      "--- RST Chunk Summary for hacking.rst ---\n",
      "1. sections_content_block_1_to_content_block_10\n",
      "   Type: grouped_sections | Level: 1 | Lines: 83 | Tokens: 521\n",
      "   Preview: .. include:: ../header.rst  ==========================   Docutils_ Hacker's Guide  ==========================  :Author: Lea Wiemann :Contact: docutils-develop@lists.sourceforge.net :Revision: $Revisio...\n",
      "2. sections_content_block_11_to_Transforming the Document\n",
      "   Type: grouped_sections | Level: 1 | Lines: 47 | Tokens: 426\n",
      "   Preview: .. code:: python      from docutils import frontend, utils     from docutils.parsers.rst import Parser     settings = frontend.get_default_settings(Parser)     with open('test.rst', encoding='utf-8')...\n",
      "3. sections_content_block_14_to_Writing the Document\n",
      "   Type: grouped_sections | Level: 1 | Lines: 50 | Tokens: 444\n",
      "   Preview: -------------------------  In the node tree above, the ``reference`` node does not contain the target URI (``https://www.python.org/``) yet.  Assigning the target URI (from the ``target`` node) to the...\n",
      "4. sections_content_block_16_to_Modifying the Document Tree Before It Is Written\n",
      "   Type: grouped_sections | Level: 1 | Lines: 54 | Tokens: 565\n",
      "   Preview: --------------------  To get an HTML document out of the node tree, we use a **Writer**, the HTML writer in this case (``docutils/writers/html4css1.py``).  The writer receives the node tree and return...\n",
      "5. sections_content_block_20_to_content_block_25\n",
      "   Type: grouped_sections | Level: 1 | Lines: 66 | Tokens: 412\n",
      "   Preview: ------------------------------------------------  You can modify the document tree right before the writer is called. One possibility is to use the publish_doctree_ and publish_from_doctree_ functions...\n",
      "\n",
      "🔍 Processing: semantics.rst\n",
      "🌳 Processing semantics.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 7 chunks\n",
      "Created 3 grouped chunks\n",
      "Final result: 3 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 3 chunks for semantics.rst\n",
      "  ✅ Saved: semantics.rst_chunk_001_rzh4wt.md\n",
      "  ✅ Saved: semantics.rst_chunk_002_rzh4wt.md\n",
      "  ✅ Saved: semantics.rst_chunk_003_rzh4wt.md\n",
      "\n",
      "--- RST Chunk Summary for semantics.rst ---\n",
      "1. sections_content_block_1_to_PythonDoc\n",
      "   Type: grouped_sections | Level: 1 | Lines: 36 | Tokens: 328\n",
      "   Preview: .. include:: ../header.rst  =====================   Docstring Semantics  ===================== :Author: David Goodger :Contact: docutils-develop@lists.sourceforge.net :Revision: $Revision$ :Date: $Dat...\n",
      "2. sections_content_block_5_to_Other Ideas\n",
      "   Type: grouped_sections | Level: 1 | Lines: 51 | Tokens: 437\n",
      "   Preview: =========  (Not to be confused with Daniel Larsson's pythondoc_ project.)  A Python version of the JavaDoc_ semantics (not syntax).  A set of conventions which are understood by the Docutils.  What Ja...\n",
      "3. content_block_7\n",
      "   Type: content_block | Level: 1 | Lines: 40 | Tokens: 316\n",
      "   Preview: ===========  - Can we extract comments from parsed modules?  Could be handy for   documenting function/method parameters::        def method(self,                  source,        # path of input file...\n",
      "\n",
      "🔍 Processing: enthought-plan.rst\n",
      "🌳 Processing enthought-plan.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 31 chunks\n",
      "Created 6 grouped chunks\n",
      "  📦 Sub-chunking sections_Recommendation_to_content_block_29 (1526 tokens)\n",
      "Final result: 7 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 7 chunks for enthought-plan.rst\n",
      "  ✅ Saved: enthought-plan.rst_chunk_001_duuuya.md\n",
      "  ✅ Saved: enthought-plan.rst_chunk_002_duuuya.md\n",
      "  ✅ Saved: enthought-plan.rst_chunk_003_duuuya.md\n",
      "  ✅ Saved: enthought-plan.rst_chunk_004_duuuya.md\n",
      "  ✅ Saved: enthought-plan.rst_chunk_005_duuuya.md\n",
      "  ✅ Saved: enthought-plan.rst_chunk_006_duuuya.md\n",
      "  ✅ Saved: enthought-plan.rst_chunk_007_duuuya.md\n",
      "\n",
      "--- RST Chunk Summary for enthought-plan.rst ---\n",
      "1. sections_content_block_1_to_Development Plan\n",
      "   Type: grouped_sections | Level: 1 | Lines: 69 | Tokens: 600\n",
      "   Preview: ===========================================   Plan for Enthought API Documentation Tool  ===========================================  :Author: David Goodger :Contact: docutils-develop@lists.sourceforg...\n",
      "2. sections_content_block_9_to_Copyright & License\n",
      "   Type: grouped_sections | Level: 1 | Lines: 51 | Tokens: 459\n",
      "   Preview: ================  1. Analyze prior art, most notably Epydoc_ and HappyDoc_, to see how    they do what they do.  I have no desire to reinvent wheels    unnecessarily.  I want to take the best ideas fr...\n",
      "3. sections_content_block_13_to_Doc Comment Syntax\n",
      "   Type: grouped_sections | Level: 1 | Lines: 58 | Tokens: 390\n",
      "   Preview: ===================  Most existing Docutils files have been placed in the public domain, as follows::      :Copyright: This document has been placed in the public domain.  This is in conjunction with...\n",
      "4. content_block_17\n",
      "   Type: content_block | Level: 1 | Lines: 82 | Tokens: 665\n",
      "   Preview: ------------------  The \"traits\" construct is implemented as dictionaries, where standalone strings would be Python syntax errors.  Therefore traits require documentation in comments.  We also need a...\n",
      "  5. sections_Recommendation_to_content_block_29_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 158 | Tokens: 1054\n",
      "     Preview: Recommendation  ``````````````  I recommend adopting \"#*\" on every line::      # This is an ordinary non-doc comment.      #* This is a documentation comment, with an asterisk after the     #* hash ma...\n",
      "  6. sections_Recommendation_to_content_block_29_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 59 | Tokens: 472\n",
      "     Preview: * \"--\" could be used, but only in limited and well-known contexts::           term -- definition       This is the syntax used by StructuredText (one of      reStructuredText's predecessors).  It w...\n",
      "7. sections_Recommendation_to_content_block_31\n",
      "   Type: grouped_sections | Level: 1 | Lines: 29 | Tokens: 245\n",
      "   Preview: Recommendation  ``````````````  Combining these ideas, the function definition becomes::      def max_gas(temperature, pressure, api, specific_gravity=.56):         \"\"\"         Computes the maximum di...\n",
      "\n",
      "🔍 Processing: enthought-rfp.rst\n",
      "🌳 Processing enthought-rfp.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 19 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for enthought-rfp.rst\n",
      "  ✅ Saved: enthought-rfp.rst_chunk_001_8e50vb.md\n",
      "\n",
      "--- RST Chunk Summary for enthought-rfp.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 165 | Tokens: 950\n",
      "   Preview: ==================================   Enthought API Documentation Tool  ================================== -----------------------   Request for Proposals  -----------------------  :Author: Janet Swish...\n",
      "\n",
      "🔍 Processing: repository.rst\n",
      "🌳 Processing repository.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 24 chunks\n",
      "Created 5 grouped chunks\n",
      "Final result: 5 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 5 chunks for repository.rst\n",
      "  ✅ Saved: repository.rst_chunk_001_6fsld3.md\n",
      "  ✅ Saved: repository.rst_chunk_002_6fsld3.md\n",
      "  ✅ Saved: repository.rst_chunk_003_6fsld3.md\n",
      "  ✅ Saved: repository.rst_chunk_004_6fsld3.md\n",
      "  ✅ Saved: repository.rst_chunk_005_6fsld3.md\n",
      "\n",
      "--- RST Chunk Summary for repository.rst ---\n",
      "1. sections_content_block_1_to_Repository Access Methods\n",
      "   Type: grouped_sections | Level: 1 | Lines: 82 | Tokens: 528\n",
      "   Preview: .. include:: ../header.rst  =====================================   The Docutils_ Version Repository  =====================================  :Author: Lea Wiemann, Docutils developers :Contact: docutil...\n",
      "2. sections_content_block_11_to_Editable installs\n",
      "   Type: grouped_sections | Level: 1 | Lines: 67 | Tokens: 407\n",
      "   Preview: -------------------------  To get a checkout, first determine the root of the repository depending on your preferred protocol:  anonymous access: (read only)     Subversion_: ``https://svn.code.sf.net...\n",
      "3. content_block_17\n",
      "   Type: content_block | Level: 1 | Lines: 60 | Tokens: 400\n",
      "   Preview: =================  There are several ways to ensure that edits to the Docutils code are picked up by Python.  We'll assume that the Docutils \"trunk\" is checked out under the ``~/projects/`` directory....\n",
      "4. sections_content_block_18_to_Setting Up Your Subversion Client For Development\n",
      "   Type: grouped_sections | Level: 1 | Lines: 55 | Tokens: 358\n",
      "   Preview: .. CAUTION::        This method is **not** recommended for day-to-day development!        If you ever forget to reinstall the \"docutils\" package, Python       won't see your latest changes. Confusi...\n",
      "5. sections_content_block_22_to_content_block_24\n",
      "   Type: grouped_sections | Level: 1 | Lines: 67 | Tokens: 551\n",
      "   Preview: -------------------------------------------------  Before committing changes to the repository, please ensure that the following lines are contained (and uncommented) in your local ~/.subversion/confi...\n",
      "\n",
      "🔍 Processing: release.rst\n",
      "🌳 Processing release.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 5 chunks\n",
      "Created 1 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_1_to_content_block_5 (1734 tokens)\n",
      "Final result: 2 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 2 chunks for release.rst\n",
      "  ✅ Saved: release.rst_chunk_001_szpnap.md\n",
      "  ✅ Saved: release.rst_chunk_002_szpnap.md\n",
      "\n",
      "--- RST Chunk Summary for release.rst ---\n",
      "  1. sections_content_block_1_to_content_block_5_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 151 | Tokens: 1035\n",
      "     Preview: .. include:: ../header.rst  =============================   Docutils_ Release Procedure  =============================  :Authors: David Goodger; Lea Wiemann; open to all Docutils developers :Contact:...\n",
      "  2. sections_content_block_1_to_content_block_5_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 97 | Tokens: 700\n",
      "     Preview: - tag #.# (Note: only directory docutils is copied)::        svn copy svn+ssh://grubert@svn.code.sf.net/p/docutils/code/trunk/docutils \\                svn+ssh://grubert@svn.code.sf.net/p/docutils/c...\n",
      "\n",
      "🔍 Processing: rst-directives.rst\n",
      "🌳 Processing rst-directives.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 20 chunks\n",
      "Created 7 grouped chunks\n",
      "Final result: 7 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 7 chunks for rst-directives.rst\n",
      "  ✅ Saved: rst-directives.rst_chunk_001_1zxw26.md\n",
      "  ✅ Saved: rst-directives.rst_chunk_002_1zxw26.md\n",
      "  ✅ Saved: rst-directives.rst_chunk_003_1zxw26.md\n",
      "  ✅ Saved: rst-directives.rst_chunk_004_1zxw26.md\n",
      "  ✅ Saved: rst-directives.rst_chunk_005_1zxw26.md\n",
      "  ✅ Saved: rst-directives.rst_chunk_006_1zxw26.md\n",
      "  ✅ Saved: rst-directives.rst_chunk_007_1zxw26.md\n",
      "\n",
      "--- RST Chunk Summary for rst-directives.rst ---\n",
      "1. sections_content_block_1_to_The Directive Class\n",
      "   Type: grouped_sections | Level: 1 | Lines: 47 | Tokens: 340\n",
      "   Preview: .. include:: ../header.rst  =======================================   Creating reStructuredText_ Directives  =======================================  :Authors: Dethe Elza, David Goodger, Lea Wiemann :...\n",
      "2. content_block_6\n",
      "   Type: content_block | Level: 1 | Lines: 103 | Tokens: 827\n",
      "   Preview: ===================  Directives are created by defining a directive class that inherits from ``docutils.parsers.rst.Directive``::      from docutils.parsers import rst      class MyDirective(rst.Direc...\n",
      "3. Option Conversion Functions\n",
      "   Type: section | Level: 1 | Lines: 1 | Tokens: 3\n",
      "   Preview: Option Conversion Functions\n",
      "4. content_block_8\n",
      "   Type: content_block | Level: 1 | Lines: 70 | Tokens: 614\n",
      "   Preview: ===========================  An option specification (``Directive.option_spec``) must be defined detailing the options available to the directive.  An option spec is a mapping of option name to conver...\n",
      "5. sections_Error Handling_to_Admonitions\n",
      "   Type: grouped_sections | Level: 1 | Lines: 82 | Tokens: 592\n",
      "   Preview: Error Handling  ==============  If your directive implementation encounters an error during processing, you should call ``self.error()`` inside the ``run()`` method::      if error_condition:...\n",
      "6. sections_content_block_16_to_\"image\"\n",
      "   Type: grouped_sections | Level: 1 | Lines: 72 | Tokens: 541\n",
      "   Preview: -----------  `Admonition directives`__, such as \"note\" and \"caution\", are quite simple.  They have no directive arguments or options.  Admonition directive content is interpreted as ordinary reStructu...\n",
      "7. sections_content_block_18_to_content_block_20\n",
      "   Type: grouped_sections | Level: 1 | Lines: 71 | Tokens: 541\n",
      "   Preview: -------  .. _image: ../ref/rst/directives.html#image  The \"image_\" directive is used to insert a picture into a document. This directive has one argument, the path to the image file, and supports seve...\n",
      "\n",
      "🔍 Processing: html-stylesheets.rst\n",
      "🌳 Processing html-stylesheets.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 3 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for html-stylesheets.rst\n",
      "  ✅ Saved: html-stylesheets.rst_chunk_001_xikr1t.md\n",
      "\n",
      "--- RST Chunk Summary for html-stylesheets.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 100 | Tokens: 836\n",
      "   Preview: .. include:: ../header.rst  ==============================================   Writing HTML (CSS) Stylesheets for Docutils_  ==============================================  :Author: Lea Wiemann :Contact...\n",
      "\n",
      "🔍 Processing: i18n.rst\n",
      "🌳 Processing i18n.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 17 chunks\n",
      "Created 4 grouped chunks\n",
      "Final result: 4 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 4 chunks for i18n.rst\n",
      "  ✅ Saved: i18n.rst_chunk_001_7kpwpv.md\n",
      "  ✅ Saved: i18n.rst_chunk_002_7kpwpv.md\n",
      "  ✅ Saved: i18n.rst_chunk_003_7kpwpv.md\n",
      "  ✅ Saved: i18n.rst_chunk_004_7kpwpv.md\n",
      "\n",
      "--- RST Chunk Summary for i18n.rst ---\n",
      "1. sections_content_block_1_to_Language Module Names\n",
      "   Type: grouped_sections | Level: 1 | Lines: 52 | Tokens: 386\n",
      "   Preview: .. include:: ../header.rst  ================================   Docutils_ Internationalization  ================================  :Author: David Goodger :Contact: docutils-develop@lists.sourceforge.net...\n",
      "2. sections_content_block_7_to_Docutils Language Module\n",
      "   Type: grouped_sections | Level: 1 | Lines: 31 | Tokens: 363\n",
      "   Preview: =====================  Language modules are named using `language tags`_ as defined in `BCP 47`_. [#]_ in lowercase, converting hyphens to underscores [#]_.  A typical language identifier consists of...\n",
      "3. sections_content_block_9_to_reStructuredText Language Module\n",
      "   Type: grouped_sections | Level: 1 | Lines: 49 | Tokens: 416\n",
      "   Preview: ========================  Modules in ``docutils/languages`` contain language mappings for markup-independent language-specific features of Docutils.  To make a new language module, just copy the ``en....\n",
      "4. sections_content_block_11_to_content_block_17\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 586\n",
      "   Preview: ================================  Modules in ``docutils/parsers/rst/languages`` contain language mappings for language-specific features of the reStructuredText parser.  To make a new language module,...\n",
      "\n",
      "🔍 Processing: rst-roles.rst\n",
      "🌳 Processing rst-roles.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 16 chunks\n",
      "Created 5 grouped chunks\n",
      "Final result: 5 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 5 chunks for rst-roles.rst\n",
      "  ✅ Saved: rst-roles.rst_chunk_001_kezafo.md\n",
      "  ✅ Saved: rst-roles.rst_chunk_002_kezafo.md\n",
      "  ✅ Saved: rst-roles.rst_chunk_003_kezafo.md\n",
      "  ✅ Saved: rst-roles.rst_chunk_004_kezafo.md\n",
      "  ✅ Saved: rst-roles.rst_chunk_005_kezafo.md\n",
      "\n",
      "--- RST Chunk Summary for rst-roles.rst ---\n",
      "1. sections_content_block_1_to_Define the Role Function\n",
      "   Type: grouped_sections | Level: 1 | Lines: 35 | Tokens: 212\n",
      "   Preview: .. include:: ../header.rst  ==================================================   Creating reStructuredText Interpreted Text Roles  ==================================================  :Authors: David G...\n",
      "2. sections_content_block_6_to_Specify Role Function Options and Content\n",
      "   Type: grouped_sections | Level: 1 | Lines: 53 | Tokens: 408\n",
      "   Preview: ========================  The role function creates and returns inline elements (nodes) and does any additional processing required.  Its signature is as follows::      def role_fn(name, rawtext, text...\n",
      "3. sections_content_block_8_to_Examples\n",
      "   Type: grouped_sections | Level: 1 | Lines: 71 | Tokens: 593\n",
      "   Preview: =========================================  Function attributes are for customization, and are interpreted by the `\"role\" directive`_.  If unspecified, role function attributes are assumed to have the...\n",
      "4. sections_content_block_12_to_RFC Reference Role\n",
      "   Type: grouped_sections | Level: 1 | Lines: 26 | Tokens: 150\n",
      "   Preview: ========  For the most direct and accurate information, \"Use the Source, Luke!\". All standard roles are documented in `reStructuredText Interpreted Text Roles`_, and the source code implementing them...\n",
      "5. content_block_16\n",
      "   Type: content_block | Level: 1 | Lines: 64 | Tokens: 619\n",
      "   Preview: ------------------  This role allows easy references to RFCs_ (Request For Comments documents) by automatically providing the base URL, http://www.faqs.org/rfcs/, and appending the RFC document itself...\n",
      "\n",
      "🔍 Processing: cmdline-tool.rst\n",
      "🌳 Processing cmdline-tool.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 4 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for cmdline-tool.rst\n",
      "  ✅ Saved: cmdline-tool.rst_chunk_001_3rgaa2.md\n",
      "\n",
      "--- RST Chunk Summary for cmdline-tool.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 81 | Tokens: 560\n",
      "   Preview: .. include:: ../header.rst  ===============================================   Inside A Docutils Command-Line Front-End Tool  ===============================================  :Author: David Goodger :Co...\n",
      "\n",
      "🔍 Processing: security.rst\n",
      "🌳 Processing security.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 26 chunks\n",
      "Created 4 grouped chunks\n",
      "Final result: 4 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 4 chunks for security.rst\n",
      "  ✅ Saved: security.rst_chunk_001_kvpb9p.md\n",
      "  ✅ Saved: security.rst_chunk_002_kvpb9p.md\n",
      "  ✅ Saved: security.rst_chunk_003_kvpb9p.md\n",
      "  ✅ Saved: security.rst_chunk_004_kvpb9p.md\n",
      "\n",
      "--- RST Chunk Summary for security.rst ---\n",
      "1. sections_content_block_1_to_External Data Insertion\n",
      "   Type: grouped_sections | Level: 1 | Lines: 64 | Tokens: 381\n",
      "   Preview: .. include:: ../header.rst  =============================   Deploying Docutils Securely  =============================  :Author: David Goodger :Contact: docutils-develop@lists.sourceforge.net :Date: $...\n",
      "2. sections_content_block_12_to_Programmatically Via Application Default Settings\n",
      "   Type: grouped_sections | Level: 1 | Lines: 63 | Tokens: 440\n",
      "   Preview: -----------------------  There are several `reStructuredText directives`_ that can insert external data (files and URLs) into the output document.  These directives are:  * \"include_\", by its very nat...\n",
      "3. sections_content_block_20_to_Version Applicability\n",
      "   Type: grouped_sections | Level: 1 | Lines: 71 | Tokens: 496\n",
      "   Preview: -------------------------------------------------  If your application calls Docutils via one of the `convenience functions`_, you can pass a dictionary of default settings that override the component...\n",
      "4. sections_content_block_24_to_content_block_26\n",
      "   Type: grouped_sections | Level: 1 | Lines: 33 | Tokens: 199\n",
      "   Preview: =====================  The \"file_insertion_enabled_\" and \"raw_enabled_\" settings were added to Docutils 0.3.9; previous versions will ignore these settings.  A bug existed in the configuration file ha...\n",
      "\n",
      "🔍 Processing: alternatives.rst\n",
      "🌳 Processing alternatives.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 148 chunks\n",
      "Created 40 grouped chunks\n",
      "  📦 Sub-chunking sections_Inline External Targets_to_content_block_38 (2755 tokens)\n",
      "  📦 Sub-chunking sections_content_block_69_to_content_block_77 (1194 tokens)\n",
      "  📦 Sub-chunking sections_Adjacent citation references_to_content_block_85 (3010 tokens)\n",
      "  📦 Sub-chunking sections_Syntax for Questions & Answers_to_content_block_93 (1863 tokens)\n",
      "  📦 Sub-chunking sections_content_block_94_to_content_block_98 (1434 tokens)\n",
      "Final result: 47 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 47 chunks for alternatives.rst\n",
      "  ✅ Saved: alternatives.rst_chunk_001_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_002_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_003_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_004_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_005_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_006_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_007_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_008_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_009_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_010_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_011_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_012_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_013_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_014_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_015_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_016_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_017_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_018_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_019_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_020_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_021_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_022_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_023_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_024_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_025_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_026_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_027_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_028_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_029_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_030_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_031_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_032_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_033_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_034_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_035_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_036_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_037_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_038_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_039_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_040_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_041_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_042_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_043_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_044_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_045_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_046_yvlesa.md\n",
      "  ✅ Saved: alternatives.rst_chunk_047_yvlesa.md\n",
      "\n",
      "--- RST Chunk Summary for alternatives.rst ---\n",
      "1. sections_content_block_1_to_content_block_8\n",
      "   Type: grouped_sections | Level: 1 | Lines: 95 | Tokens: 574\n",
      "   Preview: .. include:: ../../header2.rst  ==================================================   A Record of reStructuredText Syntax Alternatives  ==================================================  :Author: Davi...\n",
      "2. sections_content_block_9_to_Interpreted Text \"Roles\"\n",
      "   Type: grouped_sections | Level: 1 | Lines: 73 | Tokens: 537\n",
      "   Preview: .. fields::            Author: Me           Version: 1     Advantages: explicit and unambiguous, RFC822-compliant.    Disadvantage: cumbersome.     Conclusion: rejected for the general case (bu...\n",
      "3. content_block_11\n",
      "   Type: content_block | Level: 1 | Lines: 96 | Tokens: 701\n",
      "   Preview: ========================  The original purpose of interpreted text was as a mechanism for descriptive markup, to describe the nature or role of a word or phrase.  For example, in XML we could say \"<fu...\n",
      "4. sections_``term`` Role_to_Anonymous Hyperlinks\n",
      "   Type: grouped_sections | Level: 1 | Lines: 49 | Tokens: 444\n",
      "   Preview: ``term`` Role  =============  Add a \"term\" role for unfamiliar or specialized terminology? Probably not as a standard role; there is no real use case, and emphasis is enough for most cases. For semant...\n",
      "5. sections_content_block_17_to_content_block_20\n",
      "   Type: grouped_sections | Level: 1 | Lines: 73 | Tokens: 553\n",
      "   Preview: ====================  Alan Jaffray came up with this idea, along with the following syntax::      Search the `Python DOC-SIG mailing list archives`{}_.      .. _: https://mail.python.org/pipermail/doc...\n",
      "6. content_block_21\n",
      "   Type: content_block | Level: 1 | Lines: 34 | Tokens: 259\n",
      "   Preview: .. Note::        The comment text was intentionally made to look like a hyperlink       target.     Origins:     * Except for the colon (a delimiter necessary to allow for      phrase-links), hyper...\n",
      "7. content_block_22\n",
      "   Type: content_block | Level: 1 | Lines: 75 | Tokens: 597\n",
      "   Preview: .. blah:: http://somewhere    directive        .. blah: http://somewhere     comment     The hyperlink-connoted underscores have become first-level syntax.     Advantages:     + Anonymous targe...\n",
      "8. sections_content_block_23_to_Backquotes in Phrase-Links\n",
      "   Type: grouped_sections | Level: 1 | Lines: 35 | Tokens: 234\n",
      "   Preview: .. blah:: http://somewhere    directive        .. blah: http://somewhere     comment     This is the same as the current syntax, except for anonymous    targets which drop their \"__: \".     Adv...\n",
      "9. sections_content_block_25_to_Substitution Mechanism\n",
      "   Type: grouped_sections | Level: 1 | Lines: 53 | Tokens: 468\n",
      "   Preview: ==========================  [From a 2001-06-05 Doc-SIG post in reply to questions from Doug Hellmann.]  The first draft of the spec, posted to the Doc-SIG in November 2000, used square brackets for ph...\n",
      "10. content_block_27\n",
      "   Type: content_block | Level: 1 | Lines: 19 | Tokens: 195\n",
      "   Preview: ======================  Substitutions arose out of a Doc-SIG thread begun on 2001-10-28 by Alan Jaffray, \"reStructuredText inline markup\".  It reminded me of a missing piece of the reStructuredText pu...\n",
      "11. content_block_28\n",
      "   Type: content_block | Level: 1 | Lines: 96 | Tokens: 754\n",
      "   Preview: .. |biohazard| image:: biohazard.png        [height=20 width=20]  The ``|biohazard|`` substitution reference will be replaced in-line by whatever the ``.. |biohazard|`` substitution definition gen...\n",
      "12. sections_content_block_29_to_content_block_33\n",
      "   Type: grouped_sections | Level: 1 | Lines: 48 | Tokens: 375\n",
      "   Preview: .. sub:: biohazard            .. image:: biohazard.png              [height=20 width=20]          .. sub:: parrot           That bird wouldn't *voom* if you put 10,000,000 volts           throu...\n",
      "13. sections_content_block_34_to_content_block_35\n",
      "   Type: grouped_sections | Level: 1 | Lines: 42 | Tokens: 357\n",
      "   Preview: .. _Jonathan: lj:: user=jhl        .. _Zope: https://www.zope.dev/     (``::`` after ``.. _Jonathan: lj``.)     The \"Zope\" target is a simple external hyperlink, but the    \"Jonathan\" target co...\n",
      "14. content_block_36\n",
      "   Type: content_block | Level: 1 | Lines: 72 | Tokens: 602\n",
      "   Preview: .. |biohazard| image:: biohazard.png        [height=20 width=20]     .. _biohazard: https://www.cdc.gov/  There have been several suggestions for the naming of these constructs, originally called...\n",
      "  15. sections_Inline External Targets_to_content_block_38_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 135 | Tokens: 1032\n",
      "     Preview: Inline External Targets  =======================  Currently reStructuredText has two hyperlink syntax variations:  * Named hyperlinks::        This is a named reference_ of one word (\"reference\").  He...\n",
      "  16. sections_Inline External Targets_to_content_block_38_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 114 | Tokens: 1041\n",
      "     Preview: This is an anonymous reference__        __<https://www.example.org/reference/> of one word        (\"reference\").  Here is a `phrase reference`__        __<https://www.example.org/phrase_referen...\n",
      "  17. sections_Inline External Targets_to_content_block_38_part_3\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 75 | Tokens: 682\n",
      "     Preview: `phrase reference`__ [-> https://www.example.org]      `phrase reference` <https://www.example.org>_  None of these variations are clearly superior to #3 above.  Some have problems that exclude th...\n",
      "18. sections_Doctree Representation of Transitions_to_Document\n",
      "   Type: grouped_sections | Level: 1 | Lines: 14 | Tokens: 78\n",
      "   Preview: Doctree Representation of Transitions  =====================================  (Although not reStructuredText-specific, this section fits best in this document.)  Having added the \"horizontal rule\" con...\n",
      "19. sections_content_block_42_to_Syntax for Line Blocks\n",
      "   Type: grouped_sections | Level: 1 | Lines: 91 | Tokens: 566\n",
      "   Preview: ========      Paragraph 1      --------      Paragraph 2  The horizontal rule indicates a \"transition\" (in prose terms) or the start of a new \"division\".  Before implementation, the parsed documen...\n",
      "20. sections_content_block_44_to_Syntax\n",
      "   Type: grouped_sections | Level: 1 | Lines: 68 | Tokens: 542\n",
      "   Preview: ======================  * An early idea: How about a literal-block-like prefix, perhaps   \"``;;``\"?  (It is, after all, a *semi-literal* literal block, no?)   Example::        Take it away, Eric the O...\n",
      "21. sections_content_block_46_to_Internal Representation\n",
      "   Type: grouped_sections | Level: 1 | Lines: 33 | Tokens: 225\n",
      "   Preview: ------  Perhaps line block syntax like this would do::       | M6: James Bond      | MIB: Mr. J.      | IMF: not decided yet, but probably one of the following:      |   Ethan Hunt      |   Jim Phelps...\n",
      "22. sections_content_block_48_to_Output\n",
      "   Type: grouped_sections | Level: 1 | Lines: 72 | Tokens: 524\n",
      "   Preview: -----------------------  Line blocks are currently represented as text blobs as follows::       <!ELEMENT line_block %text.model;>      <!ATTLIST line_block          %basic.atts;          %fixedspace....\n",
      "23. sections_content_block_50_to_content_block_54\n",
      "   Type: grouped_sections | Level: 1 | Lines: 65 | Tokens: 508\n",
      "   Preview: ------  In HTML, line blocks are currently output as \"<pre>\" blocks, which gives us significant whitespace and line breaks, but doesn't allow long lines to wrap and causes monospaced output without st...\n",
      "24. sections_content_block_55_to_content_block_58\n",
      "   Type: grouped_sections | Level: 1 | Lines: 88 | Tokens: 591\n",
      "   Preview: .. list-table::            * - Treat             - Quantity             - Description           * - Albatross!             - 299             - On a stick!           * - Crunchy Frog!...\n",
      "25. sections_content_block_59_to_content_block_63\n",
      "   Type: grouped_sections | Level: 1 | Lines: 83 | Tokens: 566\n",
      "   Preview: .. field-list-table::            - :1: Treat              :2: Quantity              :3: Description            ...  4. Another natural variant is to allow a description list with field    lists...\n",
      "26. sections_content_block_64_to_| body row 1, column 1   || column 2   | column 3 |\n",
      "   Type: grouped_sections | Level: 1 | Lines: 71 | Tokens: 596\n",
      "   Preview: .. list-table::          :widths: 15 10 35    Automatic defaults from the text used?  * How to handle row and/or column spans?    In a field list, column-spans can be indicated by specifying the...\n",
      "  27. sections_content_block_69_to_content_block_77_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 115 | Tokens: 1034\n",
      "     Preview: +------------------------++------------+----------+    Or this idea from Nick Moffitt::        +-----+---+---+        | XOR # T | F |        +=====+===+===+        |   T # F | T |        +-----+...\n",
      "  28. sections_content_block_69_to_content_block_77_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 20 | Tokens: 160\n",
      "     Preview: #1. First item.        #3. Aha - I edited this in later.        #2. Second item.     The initial proposal required unique enumerators within a list, but    this limits the convenience of a feat...\n",
      "  29. sections_Adjacent citation references_to_content_block_85_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 125 | Tokens: 1044\n",
      "     Preview: Adjacent citation references  ============================  A special case for inline markup was proposed and implemented: multiple citation references could be joined into one::     [cite1]_[cite2]_...\n",
      "  30. sections_Adjacent citation references_to_content_block_85_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 120 | Tokens: 1039\n",
      "     Preview: remove the \".. \", prefix a \"_\"::          _[1] Footnote 1         _[#] Auto-numbered footnote.         _[#label] Auto-labeled footnote.      The leading underscore syntax (earlier dropped because...\n",
      "  31. sections_Adjacent citation references_to_content_block_85_part_3\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 82 | Tokens: 926\n",
      "     Preview: footnotes themselves correct, this is clearly not as nice.  And if     the indentation should be to the left margin instead, I like that     even less).      and the third (new) proposal::...\n",
      "  32. sections_Syntax for Questions & Answers_to_content_block_93_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 140 | Tokens: 1033\n",
      "     Preview: Syntax for Questions & Answers  ==============================  Implement as a generic two-column marked list?  As a standalone (non-directive) construct?  (Is the markup ambiguous?)  Add support to p...\n",
      "  33. sections_Syntax for Questions & Answers_to_content_block_93_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 78 | Tokens: 829\n",
      "     Preview: PEPs (and this document!) have an Emacs stanza at the bottom, in a    comment.  Having to write \".. comment::\" would be very obtrusive.     Comments *should* be dirt-easy to do.  It should be easy...\n",
      "  34. sections_content_block_94_to_content_block_98_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 118 | Tokens: 1047\n",
      "     Preview: .. warning:: this should be a comment         Here, we're trading fairly common a silent error (directive        falsely treated as a comment) for a fairly uncommon explicitly        flagge...\n",
      "  35. sections_content_block_94_to_content_block_98_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 39 | Tokens: 386\n",
      "     Preview: Perl-based parser (unpublished).  He brought up some interesting   ideas.  The implementation was flawed, however, by the change in   semantics required for backslash escapes.    __ https://article....\n",
      "36. sections_Index Entries & Indexes_to_content_block_107\n",
      "   Type: grouped_sections | Level: 1 | Lines: 106 | Tokens: 599\n",
      "   Preview: Index Entries & Indexes  =======================  Were I writing a book with an index, I guess I'd need two different kinds of index targets: inline/implicit and out-of-line/explicit.  For example::...\n",
      "37. sections_from 2002-06-24 docutils-develop posts_to_Indented Lists\n",
      "   Type: grouped_sections | Level: 1 | Lines: 58 | Tokens: 392\n",
      "   Preview: from 2002-06-24 docutils-develop posts  --------------------------------------      If all of your index entries will appear verbatim in the text,     this should be sufficient.  If not (e.g., if you...\n",
      "38. sections_content_block_116_to_Sloppy Indentation of List Items\n",
      "   Type: grouped_sections | Level: 1 | Lines: 48 | Tokens: 486\n",
      "   Preview: ==============  Allow for variant styles by interpreting indented lists as if they weren't indented?  For example, currently the list below will be parsed as a list within a block quote::      paragra...\n",
      "39. sections_content_block_118_to_David's Idea for Lazy Indentation\n",
      "   Type: grouped_sections | Level: 1 | Lines: 54 | Tokens: 301\n",
      "   Preview: ================================  Perhaps the indentation shouldn't be so strict.  Currently, this is required::      1. First line,        second line.  Anything wrong with this? ::      1. First lin...\n",
      "40. content_block_122\n",
      "   Type: content_block | Level: 1 | Lines: 93 | Tokens: 686\n",
      "   Preview: ---------------------------------  Consider a paragraph in a word processor.  It is a single logical line of text which ends with a newline, soft-wrapped arbitrarily at the right edge of the page or s...\n",
      "41. sections_Multiple Roles in Interpreted Text_to_Parameterized Interpreted Text\n",
      "   Type: grouped_sections | Level: 1 | Lines: 26 | Tokens: 152\n",
      "   Preview: Multiple Roles in Interpreted Text  ==================================  In reStructuredText, inline markup cannot be nested (yet; `see above`__).  This also applies to interpreted text.  In order to s...\n",
      "42. sections_content_block_126_to_content_block_127\n",
      "   Type: grouped_sections | Level: 1 | Lines: 56 | Tokens: 508\n",
      "   Preview: ==============================  In some cases it may be expedient to pass parameters to interpreted text, analogous to function calls.  Ideas:  1. Parameterize the interpreted text role itself (sugges...\n",
      "43. sections_content_block_128_to_Character Processing\n",
      "   Type: grouped_sections | Level: 1 | Lines: 54 | Tokens: 372\n",
      "   Preview: .. |CSS-DVD| acronym:: Content Scrambling System        :text: CSS  ----------------------------------------------------------------------  This whole idea may be going beyond the scope of reStruc...\n",
      "44. sections_content_block_133_to_Page Or Line Breaks\n",
      "   Type: grouped_sections | Level: 1 | Lines: 57 | Tokens: 483\n",
      "   Preview: ====================  Several people have suggested adding some form of character processing to reStructuredText:  * Some sort of automated replacement of ASCII sequences:    - ``--`` to em-dash (or `...\n",
      "45. sections_content_block_136_to_Superscript Markup\n",
      "   Type: grouped_sections | Level: 1 | Lines: 59 | Tokens: 573\n",
      "   Preview: ===================  * Should ^L (or something else in reST) be defined to mean   force/suggest page breaks in whatever output we have?    A \"break\" or \"page-break\" directive would be easy to add.  A...\n",
      "46. sections_content_block_138_to_Support for Annotations\n",
      "   Type: grouped_sections | Level: 1 | Lines: 62 | Tokens: 499\n",
      "   Preview: ==================  Add ``^superscript^`` inline markup?  The only common non-markup uses of \"^\" I can think of are as short hand for \"superscript\" itself and for describing control characters (\"^C to...\n",
      "47. sections_content_block_144_to_content_block_148\n",
      "   Type: grouped_sections | Level: 1 | Lines: 61 | Tokens: 428\n",
      "   Preview: =======================  Add an \"annotation\" role, as the equivalent of the HTML \"title\" attribute?  This is secondary information that may \"pop up\" when the pointer hovers over the main text.  A corr...\n",
      "\n",
      "🔍 Processing: problems.rst\n",
      "🌳 Processing problems.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 43 chunks\n",
      "Created 11 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_26_to_content_block_32 (1259 tokens)\n",
      "  📦 Sub-chunking sections_content_block_36_to_content_block_40 (1891 tokens)\n",
      "  📦 Sub-chunking sections_Hyperlinks_to_content_block_42 (1164 tokens)\n",
      "Final result: 14 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 14 chunks for problems.rst\n",
      "  ✅ Saved: problems.rst_chunk_001_76ysu3.md\n",
      "  ✅ Saved: problems.rst_chunk_002_76ysu3.md\n",
      "  ✅ Saved: problems.rst_chunk_003_76ysu3.md\n",
      "  ✅ Saved: problems.rst_chunk_004_76ysu3.md\n",
      "  ✅ Saved: problems.rst_chunk_005_76ysu3.md\n",
      "  ✅ Saved: problems.rst_chunk_006_76ysu3.md\n",
      "  ✅ Saved: problems.rst_chunk_007_76ysu3.md\n",
      "  ✅ Saved: problems.rst_chunk_008_76ysu3.md\n",
      "  ✅ Saved: problems.rst_chunk_009_76ysu3.md\n",
      "  ✅ Saved: problems.rst_chunk_010_76ysu3.md\n",
      "  ✅ Saved: problems.rst_chunk_011_76ysu3.md\n",
      "  ✅ Saved: problems.rst_chunk_012_76ysu3.md\n",
      "  ✅ Saved: problems.rst_chunk_013_76ysu3.md\n",
      "  ✅ Saved: problems.rst_chunk_014_76ysu3.md\n",
      "\n",
      "--- RST Chunk Summary for problems.rst ---\n",
      "1. sections_content_block_1_to_Section Structure via Indentation\n",
      "   Type: grouped_sections | Level: 1 | Lines: 65 | Tokens: 466\n",
      "   Preview: .. include:: ../../header2.rst  ==============================   Problems With StructuredText  ============================== :Author: David Goodger :Contact: docutils-develop@lists.sourceforge.net :R...\n",
      "2. sections_content_block_10_to_Character Escaping Mechanism\n",
      "   Type: grouped_sections | Level: 1 | Lines: 46 | Tokens: 446\n",
      "   Preview: =================================  Setext_ required that body text be indented by 2 spaces.  The original StructuredText_ and StructuredTextNG_ require that section structure be indicated through inde...\n",
      "3. content_block_12\n",
      "   Type: content_block | Level: 1 | Lines: 59 | Tokens: 602\n",
      "   Preview: ============================  No matter what characters are chosen for markup, some day someone will want to write documentation *about* that markup or using markup characters in a non-markup context....\n",
      "4. sections_Blank Lines in Lists_to_Enumerated List Markup\n",
      "   Type: grouped_sections | Level: 1 | Lines: 49 | Tokens: 356\n",
      "   Preview: Blank Lines in Lists  ====================  Oft-requested in Doc-SIG (the earliest reference is dated 1996-08-13) is the ability to write lists without requiring blank lines between items.  In docstri...\n",
      "5. sections_content_block_18_to_Definition List Markup\n",
      "   Type: grouped_sections | Level: 1 | Lines: 46 | Tokens: 433\n",
      "   Preview: ======================  StructuredText enumerated lists are allowed to begin with numbers and letters followed by a period or right-parenthesis, then whitespace. This has surprising consequences for w...\n",
      "6. sections_content_block_20_to_Tables\n",
      "   Type: grouped_sections | Level: 1 | Lines: 68 | Tokens: 539\n",
      "   Preview: ======================  StructuredText uses ' -- ' (whitespace, two hyphens, whitespace) on the first line of a paragraph to indicate a definition list item.  The ' -- ' serves to separate the term (o...\n",
      "  7. sections_content_block_26_to_content_block_32_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 92 | Tokens: 1035\n",
      "     Preview: ======  The table markup scheme in classic StructuredText was horrible.  Its omission from StructuredTextNG is welcome, and its markup will not be repeated here.  However, tables themselves are useful...\n",
      "  8. sections_content_block_26_to_content_block_32_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 18 | Tokens: 224\n",
      "     Preview: The table begins with a top border of equals signs with one or more    spaces at each column boundary (regardless of spans).  There must    be at least two columns in the table (to differentiate it...\n",
      "9. sections_Header 2 & 3 Span_to_Delimitation of Inline Markup\n",
      "   Type: grouped_sections | Level: 1 | Lines: 67 | Tokens: 506\n",
      "   Preview: Header 2 & 3 Span                   ------------------        Header 1  Header 2  Header 3        ========  ========  ========        Each      line is   a new row.        Each row  c...\n",
      "  10. sections_content_block_36_to_content_block_40_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 91 | Tokens: 1018\n",
      "     Preview: =============================  StructuredText specifies that inline markup must begin with whitespace, precluding such constructs as parenthesized or quoted emphatic text::      \"**What?**\" she cried....\n",
      "  11. sections_content_block_36_to_content_block_40_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 74 | Tokens: 873\n",
      "     Preview: Does #a[b] = 'c' + \"d\" + `2^3`# work?     8. Some @code@, with a 'quote', \"double\", ain't it grand?        Does @a[b] = 'c' + \"d\" + `2^3`@ work?     9. Some `code`, with a 'quote', \"double\", ai...\n",
      "  12. sections_Hyperlinks_to_content_block_42_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 115 | Tokens: 1034\n",
      "     Preview: Hyperlinks  ==========  There are three forms of hyperlink currently in StructuredText_:  1. (Absolute & relative URIs.)  Text enclosed by double quotes    followed by a colon, a URI, and concluded by...\n",
      "  13. sections_Hyperlinks_to_content_block_42_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 16 | Tokens: 130\n",
      "     Preview: Internal hyperlinks (links from one point to another within a single document) can be expressed by a source link as before, and a target link with a colon but no URI.  In effect, these targets 'map to...\n",
      "14. content_block_43\n",
      "   Type: content_block | Level: 1 | Lines: 101 | Tokens: 940\n",
      "   Preview: .. version:: 1      This is a footnote [1]_.      This internal hyperlink will take us to the footnotes_ area below.      Here is a one-word_ external hyperlink.      Here is `a hyperlink phrase`_...\n",
      "\n",
      "🔍 Processing: demo.rst\n",
      "🌳 Processing demo.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 112 chunks\n",
      "Created 7 grouped chunks\n",
      "Final result: 7 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 7 chunks for demo.rst\n",
      "  ✅ Saved: demo.rst_chunk_001_4fvag2.md\n",
      "  ✅ Saved: demo.rst_chunk_002_4fvag2.md\n",
      "  ✅ Saved: demo.rst_chunk_003_4fvag2.md\n",
      "  ✅ Saved: demo.rst_chunk_004_4fvag2.md\n",
      "  ✅ Saved: demo.rst_chunk_005_4fvag2.md\n",
      "  ✅ Saved: demo.rst_chunk_006_4fvag2.md\n",
      "  ✅ Saved: demo.rst_chunk_007_4fvag2.md\n",
      "\n",
      "--- RST Chunk Summary for demo.rst ---\n",
      "1. sections_content_block_1_to_Inline Markup\n",
      "   Type: grouped_sections | Level: 1 | Lines: 104 | Tokens: 455\n",
      "   Preview: .. This is a comment. Note how any initial comments are moved by    transforms to after the document title, subtitle, and docinfo.  ================================   reStructuredText Demonstration  =...\n",
      "2. sections_content_block_20_to_Field Lists\n",
      "   Type: grouped_sections | Level: 1 | Lines: 103 | Tokens: 592\n",
      "   Preview: `````````````  Paragraphs contain text and may contain inline markup: *emphasis*, **strong emphasis**, ``inline literals``, standalone hyperlinks (https://www.python.org), external hyperlinks (Python_...\n",
      "3. sections_content_block_28_to_Block Quotes\n",
      "   Type: grouped_sections | Level: 1 | Lines: 96 | Tokens: 586\n",
      "   Preview: -----------  :what: Field lists map field names to field bodies, like database        records.  They are often part of an extension syntax.  They are        an unambiguous variant of RFC 2822 fields....\n",
      "4. sections_content_block_36_to_Footnotes\n",
      "   Type: grouped_sections | Level: 1 | Lines: 68 | Tokens: 433\n",
      "   Preview: ------------  Block quotes consist of indented body elements:      My theory by A. Elk.  Brackets Miss, brackets.  This theory goes     as follows and begins now.  All brontosauruses are thin at one...\n",
      "5. sections_content_block_52_to_Document Parts\n",
      "   Type: grouped_sections | Level: 1 | Lines: 94 | Tokens: 561\n",
      "   Preview: ---------  .. [1] A footnote contains body elements, consistently indented by at    least 3 spaces.     This is the footnote's second paragraph.  .. [#label] Footnotes may be numbered, either manually...\n",
      "6. sections_content_block_65_to_content_block_101\n",
      "   Type: grouped_sections | Level: 1 | Lines: 134 | Tokens: 534\n",
      "   Preview: ``````````````  An example of the \"contents\" directive can be seen above this section (a local, untitled table of contents_) and at the beginning of the document (a document-wide `table of contents`_)...\n",
      "7. sections_content_block_102_to_content_block_112\n",
      "   Type: grouped_sections | Level: 1 | Lines: 74 | Tokens: 359\n",
      "   Preview: .. compound::     This paragraph contains a literal block::         Connecting... OK        Transmitting data... OK        Disconnecting... OK     and thus consists of a simple paragraph, a literal bl...\n",
      "\n",
      "🔍 Processing: quickstart.rst\n",
      "🌳 Processing quickstart.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 47 chunks\n",
      "Created 6 grouped chunks\n",
      "Final result: 6 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 6 chunks for quickstart.rst\n",
      "  ✅ Saved: quickstart.rst_chunk_001_9jtc1y.md\n",
      "  ✅ Saved: quickstart.rst_chunk_002_9jtc1y.md\n",
      "  ✅ Saved: quickstart.rst_chunk_003_9jtc1y.md\n",
      "  ✅ Saved: quickstart.rst_chunk_004_9jtc1y.md\n",
      "  ✅ Saved: quickstart.rst_chunk_005_9jtc1y.md\n",
      "  ✅ Saved: quickstart.rst_chunk_006_9jtc1y.md\n",
      "\n",
      "--- RST Chunk Summary for quickstart.rst ---\n",
      "1. sections_content_block_1_to_Text styles\n",
      "   Type: grouped_sections | Level: 1 | Lines: 71 | Tokens: 416\n",
      "   Preview: .. include:: ../../header2.rst   A ReStructuredText Primer  =========================  :Author: Richard Jones :Version: $Revision$ :Copyright: This document has been placed in the public domain.   .....\n",
      "2. sections_content_block_9_to_Lists\n",
      "   Type: grouped_sections | Level: 1 | Lines: 40 | Tokens: 364\n",
      "   Preview: -----------  (quickref__)  __ quickref.html#inline-markup  Inside paragraphs and other bodies of text, you may additionally mark text for *italics* with \"``*italics*``\" or **bold** with \"``**bold**``\"...\n",
      "3. content_block_12\n",
      "   Type: content_block | Level: 1 | Lines: 111 | Tokens: 665\n",
      "   Preview: -----  Lists of items come in three main flavours: **enumerated**, **bulleted** and **definitions**.  In all list cases, you may have as many paragraphs, sublists, etc. as you want, as long as the lef...\n",
      "4. sections_Preformatting (code samples)_to_Chapter 2 Title\n",
      "   Type: grouped_sections | Level: 1 | Lines: 90 | Tokens: 511\n",
      "   Preview: Preformatting (code samples)  ---------------------------- (quickref__)  __ quickref.html#literal-blocks  To just include a chunk of preformatted, never-to-be-fiddled-with text, finish the prior parag...\n",
      "5. sections_content_block_26_to_content_block_43\n",
      "   Type: grouped_sections | Level: 1 | Lines: 116 | Tokens: 555\n",
      "   Preview: ===============  This results in the following structure, illustrated by simplified pseudo-XML::      <section>         <title>             Chapter 1 Title         <section>             <title>...\n",
      "6. sections_content_block_44_to_content_block_47\n",
      "   Type: grouped_sections | Level: 1 | Lines: 42 | Tokens: 325\n",
      "   Preview: .. image:: images/biohazard.png  The ``images/biohazard.png`` part indicates the filename of the image you wish to appear in the document. There's no restriction placed on the image (format, size etc)...\n",
      "\n",
      "🔍 Processing: cheatsheet.rst\n",
      "🌳 Processing cheatsheet.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 18 chunks\n",
      "Created 3 grouped chunks\n",
      "Final result: 3 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 3 chunks for cheatsheet.rst\n",
      "  ✅ Saved: cheatsheet.rst_chunk_001_wyvabn.md\n",
      "  ✅ Saved: cheatsheet.rst_chunk_002_wyvabn.md\n",
      "  ✅ Saved: cheatsheet.rst_chunk_003_wyvabn.md\n",
      "\n",
      "--- RST Chunk Summary for cheatsheet.rst ---\n",
      "1. sections_content_block_1_to_Inline Markup\n",
      "   Type: grouped_sections | Level: 1 | Lines: 78 | Tokens: 586\n",
      "   Preview: =====================================================   The reStructuredText_ Cheat Sheet: Syntax Reminders  ===================================================== :Info: See <https://docutils.sourcefo...\n",
      "2. sections_content_block_14_to_Interpreted Text Role Quick Reference\n",
      "   Type: grouped_sections | Level: 1 | Lines: 53 | Tokens: 512\n",
      "   Preview: ============= *emphasis*; **strong emphasis**; `interpreted text`; `interpreted text with role`:emphasis:; ``inline literal text``; standalone hyperlink, https://docutils.sourceforge.io; named referen...\n",
      "3. content_block_18\n",
      "   Type: content_block | Level: 1 | Lines: 19 | Tokens: 153\n",
      "   Preview: ===================================== See <https://docutils.sourceforge.io/docs/ref/rst/roles.html> for full info.  ================  ============================================================ Role...\n",
      "\n",
      "🔍 Processing: roles.rst\n",
      "🌳 Processing roles.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 56 chunks\n",
      "Created 6 grouped chunks\n",
      "Final result: 6 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 6 chunks for roles.rst\n",
      "  ✅ Saved: roles.rst_chunk_001_c3rmyi.md\n",
      "  ✅ Saved: roles.rst_chunk_002_c3rmyi.md\n",
      "  ✅ Saved: roles.rst_chunk_003_c3rmyi.md\n",
      "  ✅ Saved: roles.rst_chunk_004_c3rmyi.md\n",
      "  ✅ Saved: roles.rst_chunk_005_c3rmyi.md\n",
      "  ✅ Saved: roles.rst_chunk_006_c3rmyi.md\n",
      "\n",
      "--- RST Chunk Summary for roles.rst ---\n",
      "1. sections_content_block_1_to_content_block_15\n",
      "   Type: grouped_sections | Level: 1 | Lines: 102 | Tokens: 547\n",
      "   Preview: .. include:: ../../header2.rst  =========================================   reStructuredText Interpreted Text Roles  =========================================  :Author: David Goodger :Contact: docutil...\n",
      "2. sections_content_block_16_to_content_block_30\n",
      "   Type: grouped_sections | Level: 1 | Lines: 114 | Tokens: 530\n",
      "   Preview: .. role:: latex(code)      :language: latex  Content of the new role is parsed and tagged by the Pygments_ syntax highlighter. See the `\"code\" directive`_ for more info on parsing and display of cod...\n",
      "3. sections_content_block_31_to_content_block_45\n",
      "   Type: grouped_sections | Level: 1 | Lines: 111 | Tokens: 593\n",
      "   Preview: .. class:: field-indent-12em  :Aliases:         \\:RFC: :Doctree Element: `\\<reference>`_  The :rfc-reference: role is used to create an HTTP reference to an RFC (Internet Request for Comments).  The :...\n",
      "4. sections_content_block_46_to_content_block_51\n",
      "   Type: grouped_sections | Level: 1 | Lines: 66 | Tokens: 466\n",
      "   Preview: .. class:: field-indent-12em  :Aliases:         \\:title:, :t: :Doctree Element: `\\<title_reference>`_  The :title-reference: role is used to describe the titles of books, periodicals, and other materi...\n",
      "5. sections_content_block_52_to_Custom Roles\n",
      "   Type: grouped_sections | Level: 1 | Lines: 36 | Tokens: 298\n",
      "   Preview: .. WARNING::    The \"raw\" role is a stop-gap measure allowing the author to bypass    reStructuredText's markup.  It is a \"power-user\" feature that    should not be overused or abused.  The use of \"ra...\n",
      "6. content_block_56\n",
      "   Type: content_block | Level: 1 | Lines: 48 | Tokens: 513\n",
      "   Preview: ============  Custom interpreted text roles can be defined in a document with the `\"role\" directive`_. The new role may be based on an existing role. The \"role\" directive may be called with options_ t...\n",
      "\n",
      "🔍 Processing: directives.rst\n",
      "🌳 Processing directives.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 230 chunks\n",
      "Created 32 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_217_to_content_block_230 (2209 tokens)\n",
      "Final result: 34 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 34 chunks for directives.rst\n",
      "  ✅ Saved: directives.rst_chunk_001_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_002_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_003_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_004_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_005_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_006_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_007_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_008_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_009_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_010_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_011_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_012_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_013_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_014_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_015_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_016_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_017_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_018_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_019_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_020_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_021_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_022_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_023_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_024_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_025_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_026_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_027_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_028_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_029_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_030_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_031_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_032_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_033_yr08mg.md\n",
      "  ✅ Saved: directives.rst_chunk_034_yr08mg.md\n",
      "\n",
      "--- RST Chunk Summary for directives.rst ---\n",
      "1. sections_content_block_1_to_content_block_10\n",
      "   Type: grouped_sections | Level: 1 | Lines: 81 | Tokens: 490\n",
      "   Preview: .. include:: ../../header2.rst  =============================   reStructuredText Directives  ============================= :Author: David Goodger :Contact: docutils-develop@lists.sourceforge.net :Revi...\n",
      "2. sections_content_block_11_to_content_block_21\n",
      "   Type: grouped_sections | Level: 1 | Lines: 94 | Tokens: 589\n",
      "   Preview: .. class:: field-indent-13em  :Directive Types: \"attention\", \"caution\", \"danger\", \"error\", \"hint\",                   \"important\", \"note\", \"tip\", \"warning\" :Doctree Elements: `\\<attention>`_, `\\<cautio...\n",
      "3. sections_content_block_22_to_content_block_24\n",
      "   Type: grouped_sections | Level: 1 | Lines: 61 | Tokens: 574\n",
      "   Preview: .. attention::    It is up to the author to ensure compatibility of the image data format   with the output format or user agent (LaTeX engine, `HTML browser`__, …).  .. _image formats:  =========== =...\n",
      "4. sections_content_block_25_to_content_block_28\n",
      "   Type: grouped_sections | Level: 1 | Lines: 43 | Tokens: 331\n",
      "   Preview: .. class:: field-indent-13em  :Directive Type: \"image\" :Doctree Elements: `\\<image>`_, `\\<reference>`_ (only with option \"target_\") :Directive Arguments: one, required (image URI_) :Directive Options:...\n",
      "5. content_block_29\n",
      "   Type: content_block | Level: 1 | Lines: 81 | Tokens: 799\n",
      "   Preview: .. |green light|  image:: green_light.png        :align: bottom  .. _image options:  The \"image\" directive recognizes the common options `class <class option_>`_ and name_ as well as  ``align`` :...\n",
      "6. sections_Figure_to_| .. image:: peak.png   | Mountain              |\n",
      "   Type: grouped_sections | Level: 1 | Lines: 47 | Tokens: 292\n",
      "   Preview: Figure  ======   .. class:: field-indent-13em  :Directive Type: \"figure\" :Doctree Elements: `\\<figure>`_, `\\<image>`_,                    `\\<caption>`_, `\\<legend>`_ :Directive Arguments: one, require...\n",
      "7. sections_content_block_41_to_content_block_47\n",
      "   Type: grouped_sections | Level: 1 | Lines: 75 | Tokens: 544\n",
      "   Preview: +-----------------------+-----------------------+  There must be blank lines before the caption paragraph and before the legend.  To specify a legend without a caption, use an empty comment (\"....\n",
      "8. sections_content_block_48_to_content_block_56\n",
      "   Type: grouped_sections | Level: 1 | Lines: 84 | Tokens: 561\n",
      "   Preview: .. class:: field-indent-13em  :Directive Type: \"topic\" :Doctree Element: `\\<topic>`_ :Directive Arguments: one, required (topic title) :Directive Options: `class <class option_>`_, name_ :Directive Co...\n",
      "9. sections_content_block_57_to_content_block_64\n",
      "   Type: grouped_sections | Level: 1 | Lines: 77 | Tokens: 563\n",
      "   Preview: .. class:: field-indent-13em  :Directive Type: \"line-block\" :Doctree Element: `\\<line_block>`_ :Directive Arguments: none :Directive Options: `class <class option_>`_, name_ :Directive Content: Become...\n",
      "10. sections_content_block_65_to_content_block_69\n",
      "   Type: grouped_sections | Level: 1 | Lines: 70 | Tokens: 542\n",
      "   Preview: .. class:: field-indent-13em  :Directive Type: \"code\" :Doctree Elements: `\\<literal_block>`_, `inline elements`_ :Directive Arguments: one, optional (formal language) :Directive Options: `see below <c...\n",
      "11. sections_content_block_70_to_content_block_82\n",
      "   Type: grouped_sections | Level: 1 | Lines: 106 | Tokens: 598\n",
      "   Preview: .. math::      α_t(i) = P(O_1, O_2, … O_t, q_t = S_i λ)  Support is limited to a subset of *LaTeX math* by the conversion required for many output formats.  For HTML, the `math_output`_ configuratio...\n",
      "12. sections_content_block_83_to_content_block_91\n",
      "   Type: grouped_sections | Level: 1 | Lines: 92 | Tokens: 579\n",
      "   Preview: .. class:: field-indent-13em  :Directive Type: \"pull-quote\" :Doctree Element: `\\<block_quote>`_ :Directive Arguments: none :Directive Options: none :Directive Content: Interpreted as the body of the b...\n",
      "13. sections_content_block_92_to_content_block_98\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 471\n",
      "   Preview: .. container:: custom         This paragraph might be rendered in a custom way.  Parsing the above results in the following pseudo-XML::      <container classes=\"custom\">         <paragraph>...\n",
      "14. sections_content_block_99_to_content_block_103\n",
      "   Type: grouped_sections | Level: 1 | Lines: 68 | Tokens: 522\n",
      "   Preview: .. class:: field-indent-7ex run-in      :\"auto\": Delegate the determination of column widths to the backend              (LaTeX, the HTML browser, ...).      :\"grid\": Relative column widths match...\n",
      "15. content_block_104\n",
      "   Type: content_block | Level: 1 | Lines: 34 | Tokens: 327\n",
      "   Preview: .. csv-table:: Frozen Delights!        :header: \"Treat\", \"Quantity\", \"Description\"        :widths: 15, 10, 30         \"Albatross\", 2.99, \"On a stick!\"        \"Crunchy Frog\", 1.49, \"If we took the...\n",
      "16. content_block_105\n",
      "   Type: content_block | Level: 1 | Lines: 74 | Tokens: 632\n",
      "   Preview: .. Caution:: Setting ``escape`` to ``\\`` (backslash) interferes with        the reStructuredText `escaping mechanism`_ (applied after CSV        parsing). You will need two backslashes to escape r...\n",
      "17. sections_List Table_to_Document Parts\n",
      "   Type: grouped_sections | Level: 1 | Lines: 84 | Tokens: 590\n",
      "   Preview: List Table  ==========   .. class:: field-indent-13em  :Directive Type: \"list-table\" :Doctree Element: `\\<table>`_ :Directive Arguments: one, optional (table caption) :Directive Options: `see below <l...\n",
      "18. sections_content_block_111_to_content_block_121\n",
      "   Type: grouped_sections | Level: 1 | Lines: 82 | Tokens: 480\n",
      "   Preview: ----------------  .. A ``_contents:`` hyperlink here became id \"contents-1\"    (name clash with the generated ToC)   Table of Contents  =================   .. class:: field-indent-13em  :Directive Typ...\n",
      "19. sections_content_block_122_to_Document Header & Footer\n",
      "   Type: grouped_sections | Level: 1 | Lines: 57 | Tokens: 513\n",
      "   Preview: .. class:: field-indent-13em  :Directive Type: \"sectnum\" or \"section-numbering\" (synonyms) :Doctree Elements: `\\<pending>`_, `\\<generated>`_ :Directive Arguments: none :Directive Options: `see below <...\n",
      "20. sections_content_block_124_to_content_block_133\n",
      "   Type: grouped_sections | Level: 1 | Lines: 85 | Tokens: 574\n",
      "   Preview: ========================  :Directive Types: \"header\" and \"footer\" :Doctree Elements: `\\<decoration>`_, `\\<header>`_, `\\<footer>`_ :Directive Arguments: none :Directive Options: none :Directive Content...\n",
      "21. sections_content_block_134_to_content_block_152\n",
      "   Type: grouped_sections | Level: 1 | Lines: 116 | Tokens: 494\n",
      "   Preview: .. class:: field-indent-13em  :Directive Type: \"footnotes\" :Doctree Elements: `\\<pending>`_, `\\<topic>`_ :Directive Arguments: none? :Directive Options: Possible? :Directive Content: none  @@@    Cita...\n",
      "22. sections_content_block_153_to_content_block_161\n",
      "   Type: grouped_sections | Level: 1 | Lines: 82 | Tokens: 530\n",
      "   Preview: .. class:: field-indent-13em  :Directive Type: \"unicode\" :Doctree Element: Text :Directive Arguments: one or more, required (Unicode character codes,                       optional text, and comments)...\n",
      "23. sections_content_block_162_to_content_block_172\n",
      "   Type: grouped_sections | Level: 1 | Lines: 89 | Tokens: 583\n",
      "   Preview: .. class:: field-indent-13em  :Directive Type: \"date\" :Doctree Element: Text :Directive Arguments: one, optional (date format) :Directive Options: none :Directive Content: none  The \"date\" directive g...\n",
      "24. sections_content_block_173_to_content_block_176\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 551\n",
      "   Preview: .. include:: <isonum.txt>  The current set of standard \"include\" data files consists of sets of substitution definitions.  See `reStructuredText Standard Definition Files`_ for details.  .. _inclu...\n",
      "25. sections_content_block_177_to_content_block_181\n",
      "   Type: grouped_sections | Level: 1 | Lines: 61 | Tokens: 477\n",
      "   Preview: .. class:: field-indent-13em  :Directive Type: \"raw\" :Doctree Element: `\\<raw>`_ :Directive Arguments: one or more, required (output format types) :Directive Options: `see below <raw options_>`__ :Dir...\n",
      "26. sections_content_block_182_to_content_block_191\n",
      "   Type: grouped_sections | Level: 1 | Lines: 93 | Tokens: 521\n",
      "   Preview: .. raw:: html        :file: inclusion.html  Inline equivalents of the \"raw\" directive can be defined via `custom interpreted text roles`_ derived from the `\"raw\" role`_.  .. _raw options:  The \"ra...\n",
      "27. sections_content_block_192_to_content_block_195\n",
      "   Type: grouped_sections | Level: 1 | Lines: 58 | Tokens: 405\n",
      "   Preview: .. class:: special    ..         Special block quote.  results in this doctree_ fragment::      <comment xml:space=\"preserve\">     <block_quote classes=\"special\">         <paragraph>             Sp...\n",
      "28. sections_content_block_196_to_content_block_198\n",
      "   Type: grouped_sections | Level: 1 | Lines: 53 | Tokens: 533\n",
      "   Preview: .. topic:: Rationale:      Identifier keys must be valid in all supported output formats.      For HTML 4.1 + CSS1 compatibility, identifiers should have no     underscores, colons, or periods.  Hyphe...\n",
      "29. sections_content_block_199_to_content_block_202\n",
      "   Type: grouped_sections | Level: 1 | Lines: 75 | Tokens: 458\n",
      "   Preview: .. class:: field-indent-13em  :Directive Type: \"role\" :Doctree Element: none; affects subsequent parsing :Directive Arguments: two; one required (new `role name`_), one optional...\n",
      "30. sections_content_block_203_to_content_block_210\n",
      "   Type: grouped_sections | Level: 1 | Lines: 96 | Tokens: 600\n",
      "   Preview: .. role:: custom          :class: special        :custom:`interpreted text`      is parsed as ::        <paragraph>           <inline classes=\"special\">               interpreted text      The \"...\n",
      "31. sections_content_block_211_to_content_block_216\n",
      "   Type: grouped_sections | Level: 1 | Lines: 72 | Tokens: 556\n",
      "   Preview: .. note:: Data from some `bibliographic fields`_ is automatically    extracted and stored as metadata, too. However, Bibliographic    Fields are also displayed in the document's screen rendering or...\n",
      "  32. sections_content_block_217_to_content_block_230_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 163 | Tokens: 1060\n",
      "     Preview: .. class:: field-indent-13em  :Directive Type: \"title\" :Doctree Element: sets the `\\<document>`_ element's `title attribute`_) :Directive Arguments: one, required (the title text) :Directive Options:...\n",
      "  33. sections_content_block_217_to_content_block_230_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 86 | Tokens: 1075\n",
      "     Preview: .. _grid table: restructuredtext.html#grid-tables .. _hyperlink reference: .. _hyperlink references: restructuredtext.html#hyperlink-references .. _hyperlink targets: .. _hyperlink target: restructure...\n",
      "  34. sections_content_block_217_to_content_block_230_part_3\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 14 | Tokens: 72\n",
      "     Preview: .. _<topic>: ../doctree.html#topic .. _<warning>: ../doctree.html#warning   ..\f Emacs settings     Local Variables:    mode: indented-text    mode: rst    indent-tabs-mode: nil    sentence-end-double-...\n",
      "\n",
      "🔍 Processing: mathematics.rst\n",
      "🌳 Processing mathematics.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 129 chunks\n",
      "Created 26 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_51_to_content_block_54 (1651 tokens)\n",
      "  📦 Sub-chunking sections_Comparison_to_content_block_69 (1248 tokens)\n",
      "Final result: 28 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 28 chunks for mathematics.rst\n",
      "  ✅ Saved: mathematics.rst_chunk_001_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_002_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_003_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_004_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_005_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_006_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_007_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_008_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_009_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_010_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_011_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_012_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_013_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_014_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_015_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_016_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_017_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_018_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_019_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_020_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_021_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_022_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_023_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_024_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_025_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_026_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_027_20r97a.md\n",
      "  ✅ Saved: mathematics.rst_chunk_028_20r97a.md\n",
      "\n",
      "--- RST Chunk Summary for mathematics.rst ---\n",
      "1. sections_content_block_1_to_content_block_14\n",
      "   Type: grouped_sections | Level: 1 | Lines: 72 | Tokens: 490\n",
      "   Preview: .. include:: ../../header2.rst  ============================  LaTeX syntax for mathematics  ============================   .. role:: m(math)  .. default-role:: math  .. |latex| replace:: L\\ :sup:`A`\\...\n",
      "2. sections_content_block_15_to_content_block_18\n",
      "   Type: grouped_sections | Level: 1 | Lines: 43 | Tokens: 343\n",
      "   Preview: .. math::    :name: Fourier transform       (\\mathcal{F}f)(y)       = \\frac{1}{\\sqrt{2\\pi}^{\\ n}}         \\int_{\\mathbb{R}^n} f(x)\\,         e^{-\\mathrm{i} y \\cdot x} \\,\\mathrm{d} x.  The ``:name:`` o...\n",
      "3. sections_content_block_19_to_content_block_23\n",
      "   Type: grouped_sections | Level: 1 | Lines: 49 | Tokens: 380\n",
      "   Preview: .. math::   \\left.     \\begin{aligned}       B' & = -\\partial\\times E         \\\\       E' & =  \\partial\\times B - 4\\pi j     \\end{aligned}   \\;\\right\\}   \\qquad \\text{Maxwell’s equations.}   .. [#math...\n",
      "4. sections_content_block_24_to_content_block_27\n",
      "   Type: grouped_sections | Level: 1 | Lines: 31 | Tokens: 533\n",
      "   Preview: .. class:: colwidths-auto    =========== =============  ============ ==============  ============== ================  ========= ===========   `\\acute{x}` ``\\acute{x}``  `\\dot{t}`    ``\\dot{t}``     `\\...\n",
      "5. content_block_28\n",
      "   Type: content_block | Level: 1 | Lines: 26 | Tokens: 693\n",
      "   Preview: .. class:: colwidths-auto    ================== ====================  ================= ===================  ================== ====================   `*`                ``*``                 `\\circle...\n",
      "6. sections_Extensible delimiters_to_content_block_40\n",
      "   Type: grouped_sections | Level: 1 | Lines: 59 | Tokens: 574\n",
      "   Preview: Extensible delimiters  --------------------- Unless you indicate otherwise, delimiters in math formulas remain at the standard size regardless of the height of the enclosed material. To get adaptable...\n",
      "7. sections_content_block_41_to_content_block_44\n",
      "   Type: grouped_sections | Level: 1 | Lines: 35 | Tokens: 544\n",
      "   Preview: .. class:: colwidths-auto    ===============================  ======================================   `\\uparrow`     ``\\uparrow``      `\\Uparrow`     ``\\Uparrow``   `\\downarrow`   ``\\downarrow``    `...\n",
      "8. sections_content_block_45_to_content_block_47\n",
      "   Type: grouped_sections | Level: 1 | Lines: 18 | Tokens: 92\n",
      "   Preview: .. math:: \\operatorname{sgn}(-3) = -1.  .. TODO: \\operatorname* for function name with limits.  The ``\\DeclareMathOperator`` command can only be used in the `LaTeX preamble`_.  .. _LaTeX preamble: lat...\n",
      "9. sections_content_block_48_to_content_block_50\n",
      "   Type: grouped_sections | Level: 1 | Lines: 33 | Tokens: 600\n",
      "   Preview: .. class:: colwidths-auto    ========== ============  ========== ============  ========== ============  ============== ===============   `\\Gamma`   ``\\Gamma``    `\\alpha`   ``\\alpha``    `\\mu`      ``...\n",
      "  10. sections_content_block_51_to_content_block_54_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 66 | Tokens: 1014\n",
      "     Preview: .. class:: colwidths-auto    ============= ===============  ========== ============  ========== ============  =========== =============   `\\forall`     ``\\forall``      `\\aleph`   ``\\aleph``    `\\hbar...\n",
      "  11. sections_content_block_51_to_content_block_54_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 40 | Tokens: 637\n",
      "     Preview: mathbf:   `\\mathbf{ABCDEFGHIJKLMNOPQRSTUVWXYZ\\ abcdefghijklmnopqrstuvwxyz}`   `\\mathbf{ΓΔΘΛΞΠΣΥΦΨΩ\\ αβγδεζηθικλμνξπρςστυφχψω\\ ϵϑϕϰϱϖϜϝ\\ \\partial∇}`   `\\mathbf{0123456789}` mathit:   `\\mathit{ABCDEFGHI...\n",
      "12. sections_content_block_55_to_content_block_57\n",
      "   Type: grouped_sections | Level: 1 | Lines: 21 | Tokens: 193\n",
      "   Preview: .. math::    V_i x \\pm \\cos(\\alpha) \\approx 3\\Gamma \\quad \\forall x\\in\\mathbb{R}     \\boldsymbol{V_i x \\pm \\cos(\\alpha) \\approx 3\\Gamma \\quad \\forall x\\in\\mathbb{R}}  It is usually ill-advised to appl...\n",
      "13. sections_content_block_58_to_content_block_60\n",
      "   Type: grouped_sections | Level: 1 | Lines: 22 | Tokens: 416\n",
      "   Preview: .. class:: colwidths-auto  ==================== ======================  ================ ==================  ================= =================== `\\#`                 ``\\#``                  `\\clubsu...\n",
      "14. sections_content_block_61_to_content_block_65\n",
      "   Type: grouped_sections | Level: 1 | Lines: 24 | Tokens: 221\n",
      "   Preview: .. class:: colwidths-auto  === =====  ======== ===============  ======== ========== `.` ``.``  `!`      ``!``            `\\vdots` ``\\vdots`` `/` ``/``  `?`      ``?``            `\\dotsb` ``\\dotsb`` `|...\n",
      "15. content_block_66\n",
      "   Type: content_block | Level: 1 | Lines: 37 | Tokens: 833\n",
      "   Preview: .. class:: colwidths-auto    ====================== ========================  ===================== =======================   `\\circlearrowleft`     ``\\circlearrowleft``      `\\circlearrowright`   ``\\...\n",
      "  16. sections_Comparison_to_content_block_69_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 28 | Tokens: 988\n",
      "     Preview: Comparison  ~~~~~~~~~~   .. class:: colwidths-auto  ================ ==================  ============= ===============  ============= ===============  =============== ================= `<`...\n",
      "  17. sections_Comparison_to_content_block_69_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 14 | Tokens: 260\n",
      "     Preview: `\\equiv`         ``\\equiv``          `\\lessgtr`    ``\\lessgtr``     `\\nsucceq`    ``\\nsucceq``     `\\thicksim`     ``\\thicksim`` `\\fallingdotseq` ``\\fallingdotseq``  `\\lesssim`    ``\\lesssim``...\n",
      "18. sections_Miscellaneous relations_to_content_block_71\n",
      "   Type: grouped_sections | Level: 1 | Lines: 3 | Tokens: 8\n",
      "   Preview: Miscellaneous relations  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "19. content_block_72\n",
      "   Type: content_block | Level: 1 | Lines: 37 | Tokens: 921\n",
      "   Preview: .. class:: colwidths-auto    ===================== =======================  =================== =====================  =================== =====================   `\\backepsilon`        ``\\backepsilon`...\n",
      "20. sections_Variable-sized operators_to_content_block_88\n",
      "   Type: grouped_sections | Level: 1 | Lines: 79 | Tokens: 580\n",
      "   Preview: Variable-sized operators  ------------------------  .. class:: colwidths-auto    =========================  =========================  =========================  ===========================   `\\sum`...\n",
      "21. sections_content_block_89_to_content_block_93\n",
      "   Type: grouped_sections | Level: 1 | Lines: 36 | Tokens: 287\n",
      "   Preview: .. math::      \\left ( \\begin{matrix} a & b \\\\ c & d \\end{matrix} \\right)  The environments ``pmatrix``, ``bmatrix``, ``Bmatrix``, ``vmatrix``, and ``Vmatrix`` have (respectively) ( ), [ ], { }, \\| \\|...\n",
      "22. sections_content_block_94_to_content_block_97\n",
      "   Type: grouped_sections | Level: 1 | Lines: 42 | Tokens: 562\n",
      "   Preview: .. class:: colwidths-auto    ======================  ========  =====================  ===================   :m:`3\\qquad 4`                    ``3\\qquad 4``          = 2em   :m:`3\\quad 4`...\n",
      "23. sections_content_block_98_to_content_block_106\n",
      "   Type: grouped_sections | Level: 1 | Lines: 56 | Tokens: 469\n",
      "   Preview: .. class:: colwidths-auto    =========  ===========================  =========================   command    example                      result   =========  ===========================  ==============...\n",
      "24. sections_content_block_107_to_content_block_112\n",
      "   Type: grouped_sections | Level: 1 | Lines: 52 | Tokens: 479\n",
      "   Preview: .. math:: \\frac{x+1}{x-1}  \\quad           \\dfrac{x+1}{x-1} \\quad           \\tfrac{x+1}{x-1}  and in text: `\\frac{x+1}{x-1}`, `\\dfrac{x+1}{x-1}`, `\\tfrac{x+1}{x-1}`.  For binomial expressions such as...\n",
      "25. sections_content_block_113_to_content_block_114\n",
      "   Type: grouped_sections | Level: 1 | Lines: 31 | Tokens: 511\n",
      "   Preview: .. class:: colwidths-auto    =========  ==============  ==============  ==============  ==============  ===============  ===============   Sizing     no              ``\\left``       ``\\bigl``       ``...\n",
      "26. sections_content_block_115_to_content_block_121\n",
      "   Type: grouped_sections | Level: 1 | Lines: 55 | Tokens: 556\n",
      "   Preview: .. math:: \\left((a_1 b_1) - (a_2 b_2)\\right)           \\left((a_2 b_1) + (a_1 b_2)\\right)           \\quad\\text{versus}\\quad           \\bigl((a_1 b_1) - (a_2 b_2)\\bigr)           \\bigl((a_2 b_1) + (a_1...\n",
      "27. sections_content_block_122_to_content_block_128\n",
      "   Type: grouped_sections | Level: 1 | Lines: 57 | Tokens: 531\n",
      "   Preview: .. math:: \\lim_{n\\to\\infty} \\sum_1^n \\frac{1}{n}  move to index positions: `\\lim_{n\\to\\infty} \\sum_1^n \\frac{1}{n}`.    Altering the placement of limits  --------------------------------  The commands...\n",
      "28. content_block_129\n",
      "   Type: content_block | Level: 1 | Lines: 21 | Tokens: 232\n",
      "   Preview: .. math::    \\frac{\\scriptstyle\\sum_{n > 0} z^n}   {\\displaystyle\\prod_{1\\leq k\\leq n} (1-q^k)}   \\text{ instead of the default }   \\frac{\\sum_{n > 0} z^n}   {\\prod_{1\\leq k\\leq n} (1-q^k)}.  .. [#] \"...\n",
      "\n",
      "🔍 Processing: restructuredtext.rst\n",
      "🌳 Processing restructuredtext.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 243 chunks\n",
      "Created 49 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_165_to_content_block_169 (1997 tokens)\n",
      "  📦 Sub-chunking sections_content_block_201_to_content_block_203 (1497 tokens)\n",
      "  📦 Sub-chunking sections_content_block_238_to_content_block_243 (2633 tokens)\n",
      "Final result: 53 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 53 chunks for restructuredtext.rst\n",
      "  ✅ Saved: restructuredtext.rst_chunk_001_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_002_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_003_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_004_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_005_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_006_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_007_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_008_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_009_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_010_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_011_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_012_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_013_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_014_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_015_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_016_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_017_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_018_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_019_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_020_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_021_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_022_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_023_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_024_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_025_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_026_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_027_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_028_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_029_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_030_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_031_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_032_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_033_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_034_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_035_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_036_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_037_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_038_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_039_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_040_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_041_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_042_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_043_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_044_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_045_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_046_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_047_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_048_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_049_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_050_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_051_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_052_6fobll.md\n",
      "  ✅ Saved: restructuredtext.rst_chunk_053_6fobll.md\n",
      "\n",
      "--- RST Chunk Summary for restructuredtext.rst ---\n",
      "1. sections_content_block_1_to_Quick Syntax Overview\n",
      "   Type: grouped_sections | Level: 1 | Lines: 59 | Tokens: 437\n",
      "   Preview: .. include:: ../../header2.rst  =======================================   reStructuredText Markup Specification  =======================================  :Author: David Goodger :Contact: docutils-deve...\n",
      "2. content_block_7\n",
      "   Type: content_block | Level: 1 | Lines: 105 | Tokens: 744\n",
      "   Preview: -----------------------  A reStructuredText document is made up of body or block-level elements, and may be structured into sections.  Sections_ are indicated through title style (underlines & optiona...\n",
      "3. sections_| Header row, column 1   | Header 2   | Header 3 |_to_Blank Lines\n",
      "   Type: grouped_sections | Level: 1 | Lines: 90 | Tokens: 523\n",
      "   Preview: | Header row, column 1   | Header 2   | Header 3 |           +========================+============+==========+           | body row 1, column 1   | column 2   | column 3 |           +-------...\n",
      "4. sections_content_block_21_to_Indentation\n",
      "   Type: grouped_sections | Level: 1 | Lines: 13 | Tokens: 97\n",
      "   Preview: -----------  Blank lines are used to separate paragraphs and other elements. Multiple successive blank lines are equivalent to a single blank line, except within literal blocks (where all whitespace i...\n",
      "5. content_block_23\n",
      "   Type: content_block | Level: 1 | Lines: 89 | Tokens: 666\n",
      "   Preview: -----------  Indentation is used to indicate -- and is only significant in indicating -- block quotes, definitions (in `definition lists`_), and local nested content:  - list item content (multi-line...\n",
      "6. sections_Escaping Mechanism_to_Reference Names\n",
      "   Type: grouped_sections | Level: 1 | Lines: 62 | Tokens: 597\n",
      "   Preview: Escaping Mechanism  ==================  The character set universally available to plaintext documents, 7-bit ASCII, is limited.  No matter what characters are used for markup, they will already have...\n",
      "7. content_block_27\n",
      "   Type: content_block | Level: 1 | Lines: 4 | Tokens: 14\n",
      "   Preview: ===============  `Reference names` identify elements for cross-referencing.\n",
      "8. content_block_28\n",
      "   Type: content_block | Level: 1 | Lines: 65 | Tokens: 601\n",
      "   Preview: .. Note:: References to a target position in external, generated documents    must use the auto-generated `identifier key`_ which may differ from the    `reference name` due to restrictions on identif...\n",
      "9. sections_Document Structure_to_Sections\n",
      "   Type: grouped_sections | Level: 1 | Lines: 50 | Tokens: 340\n",
      "   Preview: Document Structure  ==================   .. raw:: html     <style type=\"text/css\"><!--     dl.field-list {--field-indent: 10em;}     --></style>   Document  --------  :Doctree element: `\\<document>`_...\n",
      "10. sections_content_block_37_to_Section Title\n",
      "   Type: grouped_sections | Level: 1 | Lines: 97 | Tokens: 532\n",
      "   Preview: --------  :Doctree elements: `\\<section>`_, `\\<title>`_  .. _section title: .. _section titles:  Sections are identified through their *titles*, which are marked up with adornment: \"underlines\" below...\n",
      "11. sections_content_block_57_to_content_block_67\n",
      "   Type: grouped_sections | Level: 1 | Lines: 102 | Tokens: 603\n",
      "   Preview: ^^^^^^^^^^^^^  When a title has both an underline and an overline, the title text may be inset, as in the first two examples above.  This is merely aesthetic and not significant.  Underline-only t...\n",
      "12. sections_Bullet Lists_to_Enumerated Lists\n",
      "   Type: grouped_sections | Level: 1 | Lines: 52 | Tokens: 408\n",
      "   Preview: Bullet Lists  ------------  :Doctree elements: `\\<bullet_list>`_, `\\<list_item>`_  A text block which begins with a ``*``, ``+``, ``-``, ``•``, ``‣``, or ``⁃``, followed by whitespace, is a bullet lis...\n",
      "13. content_block_71\n",
      "   Type: content_block | Level: 1 | Lines: 62 | Tokens: 696\n",
      "   Preview: ----------------  :Doctree elements: `\\<enumerated_list>`_, `\\<list_item>`_  Enumerated lists (a.k.a. \"ordered\" lists) are similar to bullet lists, but use enumerators instead of bullets.  An enumerat...\n",
      "14. sections_content_block_72_to_Definition Lists\n",
      "   Type: grouped_sections | Level: 1 | Lines: 33 | Tokens: 194\n",
      "   Preview: .. Caution::     If a single-line paragraph begins with text identical to an enumerator     (``A.``, ``1.``, ``(b)``, ``I)``, etc.), the first character will have     to be escaped in order to have th...\n",
      "15. sections_content_block_74_to_Field Lists\n",
      "   Type: grouped_sections | Level: 1 | Lines: 72 | Tokens: 507\n",
      "   Preview: ----------------  :Doctree elements: `\\<definition_list>`_, `\\<definition_list_item>`_,                    `\\<term>`_, `\\<classifier>`_, `\\<definition>`_  Each definition list item contains a term, op...\n",
      "16. content_block_80\n",
      "   Type: content_block | Level: 1 | Lines: 60 | Tokens: 610\n",
      "   Preview: -----------  :Doctree elements: `\\<field_list>`_, `\\<field>`_,                    `\\<field_name>`_, `\\<field_body>`_  Field lists are used as part of an extension syntax, such as options for directive...\n",
      "17. sections_| (body elements)+                  |_to_content_block_84\n",
      "   Type: grouped_sections | Level: 1 | Lines: 60 | Tokens: 515\n",
      "   Preview: | (body elements)+                  |              +-----------------------------------+  .. [#] Up to Docutils 0.14, field markers were not recognized when    containing a colon.   Biblio...\n",
      "18. sections_content_block_85_to_RCS Keywords\n",
      "   Type: grouped_sections | Level: 1 | Lines: 52 | Tokens: 509\n",
      "   Preview: .. compound::    :name: authors     The **Authors** field may contain     * a single paragraph_ consisting of a list of authors, separated by      ``;`` or ``,`` [#i18n]_ (the semicolon is checked fir...\n",
      "19. sections_content_block_88_to_Option Lists\n",
      "   Type: grouped_sections | Level: 1 | Lines: 27 | Tokens: 231\n",
      "   Preview: ````````````  `Bibliographic fields`_ recognized by the parser are normally checked for RCS [#]_ keywords and cleaned up [#]_.  RCS keywords may be entered into source files as \"$keyword$\", and once s...\n",
      "20. content_block_90\n",
      "   Type: content_block | Level: 1 | Lines: 93 | Tokens: 861\n",
      "   Preview: ------------  :Doctree elements: `\\<option_list>`_, `\\<option_list_item>`_,                    `\\<option_group>`_, `\\<option>`_, `\\<option_string>`_,                    `\\<option_argument>`_, `\\<descr...\n",
      "21. sections_| (body elements)+                 |_to_Quoted Literal Blocks\n",
      "   Type: grouped_sections | Level: 1 | Lines: 100 | Tokens: 579\n",
      "   Preview: | (body elements)+                 |              +----------------------------------+    Literal Blocks  --------------  :Doctree element: `\\<literal_block>`_  A paragraph consisting of t...\n",
      "22. sections_content_block_103_to_Line Blocks\n",
      "   Type: grouped_sections | Level: 1 | Lines: 45 | Tokens: 230\n",
      "   Preview: `````````````````````  Quoted literal blocks are unindented contiguous blocks of text where each line begins with the same non-alphanumeric printable 7-bit ASCII character [#]_.  A blank line ends a q...\n",
      "23. sections_content_block_110_to_Block Quotes\n",
      "   Type: grouped_sections | Level: 1 | Lines: 52 | Tokens: 408\n",
      "   Preview: -----------  :Doctree elements: `\\<line_block>`_, `\\<line>`_  Line blocks are useful for address blocks, verse (poetry, song lyrics), and unadorned lists, where the structure of lines is significant....\n",
      "24. sections_content_block_112_to_Tables\n",
      "   Type: grouped_sections | Level: 1 | Lines: 105 | Tokens: 580\n",
      "   Preview: ------------  :Doctree elements: `\\<block_quote>`_, `\\<attribution>`_  A text block that is indented relative to the preceding text, without preceding markup indicating it to be a literal block or oth...\n",
      "25. sections_content_block_121_to_content_block_129\n",
      "   Type: grouped_sections | Level: 1 | Lines: 59 | Tokens: 591\n",
      "   Preview: ------  :Doctree elements: `\\<table>`_, `\\<tgroup>`_, `\\<colspec>`_, `\\<thead>`_,                    `\\<tbody>`_, `\\<row>`_, `\\<entry>`_  ReStructuredText provides two syntax variants for delineating...\n",
      "26. sections_| body row 4             |            | - body elements.    |_to_Simple Tables\n",
      "   Type: grouped_sections | Level: 1 | Lines: 77 | Tokens: 534\n",
      "   Preview: | body row 4             |            | - body elements.    |      +------------------------+------------+---------------------+  Some care must be taken with grid tables to avoid undesired intera...\n",
      "27. content_block_157\n",
      "   Type: content_block | Level: 1 | Lines: 67 | Tokens: 725\n",
      "   Preview: `````````````  Simple tables provide a compact and easy to type but limited row-oriented table representation for simple data sets.  Cell contents are typically single paragraphs, although arbitrary b...\n",
      "28. sections_content_block_158_to_Footnotes\n",
      "   Type: grouped_sections | Level: 1 | Lines: 78 | Tokens: 596\n",
      "   Preview: .. Tip::     To start a new row in a simple table without text in the first    column in the processed output, use one of these:     * an empty comment (``..``), which may be omitted from the processe...\n",
      "29. sections_content_block_162_to_| (body elements)+        |\n",
      "   Type: grouped_sections | Level: 1 | Lines: 47 | Tokens: 376\n",
      "   Preview: `````````  :Doctree elements: `\\<footnote>`_, `\\<label>`_ :Config settings:  footnote_references_ :See also:         `footnote references`_  Each footnote consists of an explicit markup start (:litera...\n",
      "30. content_block_164\n",
      "   Type: content_block | Level: 1 | Lines: 98 | Tokens: 883\n",
      "   Preview: +-------------------------+   Auto-Numbered Footnotes .......................  A number sign (``#``) may be used as the first character of a footnote label to request automatic numbering o...\n",
      "  31. sections_content_block_165_to_content_block_169_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 126 | Tokens: 1043\n",
      "     Preview: .. Note:: When using auto-symbol footnotes, the choice of output    encoding is important.  Many of the symbols used are not    encodable in 8-bit text encodings such as Latin-1 (ISO 8859-1).    The u...\n",
      "  32. sections_content_block_165_to_content_block_169_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 121 | Tokens: 954\n",
      "     Preview: .. _Python DOC-SIG mailing list archive:        .. _archive:        .. _Doc-SIG: https://mail.python.org/pipermail/doc-sig/     An inline form of internal hyperlink target is available; see...\n",
      "33. sections_|                      |_to_content_block_175\n",
      "   Type: grouped_sections | Level: 1 | Lines: 77 | Tokens: 550\n",
      "   Preview: |                      |              +----------------------+   .. _anonymous:  Anonymous Hyperlinks ....................  The `World Wide Web Consortium`_ recommends in its `HTML Techniq...\n",
      "34. content_block_176\n",
      "   Type: content_block | Level: 1 | Lines: 42 | Tokens: 405\n",
      "   Preview: .. note:: This is a paragraph         - Here is a bullet list.  Directives are indicated by an explicit markup start (:literal:`.. \\ `) followed by the directive type, two colons, and whitespace (...\n",
      "35. sections_content_block_177_to_content_block_181\n",
      "   Type: grouped_sections | Level: 1 | Lines: 57 | Tokens: 464\n",
      "   Preview: .. figure:: larch.png        :scale: 50         The larch.  Simple directives may not require any content.  If a directive that does not employ a content block is followed by indented text anyway,...\n",
      "36. sections_content_block_182_to_content_block_185\n",
      "   Type: grouped_sections | Level: 1 | Lines: 64 | Tokens: 449\n",
      "   Preview: .. |biohazard| image:: biohazard.png  It is an error for a substitution definition block to directly or indirectly contain a circular substitution reference.  `Substitution references`_ are replac...\n",
      "37. sections_content_block_186_to_content_block_190\n",
      "   Type: grouped_sections | Level: 1 | Lines: 62 | Tokens: 516\n",
      "   Preview: .. |Jon|     user:: jhl      Depending on the needs of the site, this may be used to index the     document for later searching, to hyperlink the inline text in     various ways (mailto, homep...\n",
      "38. sections_content_block_191_to_Implicit Hyperlink Targets\n",
      "   Type: grouped_sections | Level: 1 | Lines: 94 | Tokens: 544\n",
      "   Preview: .. style:: disclaimer               All rights reversed.  Reprint what you like.      .. [#] There may be sufficient need for a \"style\" mechanism to        warrant simpler syntax such as an...\n",
      "39. sections_content_block_199_to_Inline Markup\n",
      "   Type: grouped_sections | Level: 1 | Lines: 45 | Tokens: 357\n",
      "   Preview: ==========================  :Doctree element: `\\<target>`_  Implicit hyperlink targets are generated by `section titles`_, auto-numbered footnotes_, and hyperlink references with `embedded URIs and al...\n",
      "  40. sections_content_block_201_to_content_block_203_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 103 | Tokens: 1020\n",
      "     Preview: =============  In reStructuredText, inline markup applies to words or phrases within a text block.  The same whitespace and punctuation that serves to delimit words in written text is used to delimit...\n",
      "  41. sections_content_block_201_to_content_block_203_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 37 | Tokens: 477\n",
      "     Preview: - 2 * x  a ** b  (* BOM32_* ` `` _ __ | (breaks rule 1) - || (breaks rule 3) - \"*\" '|' (*) [*] {*} <*>   ‘*’ ‚*‘ ‘*‚ ’*’ ‚*’   “*” „*“ “*„ ”*” „*”   »*« ›*‹ «*» »*» ›*› (breaks rule 5) - 2*x a**b O(N*...\n",
      "42. sections_Recognition order_to_Strong Emphasis\n",
      "   Type: grouped_sections | Level: 1 | Lines: 83 | Tokens: 556\n",
      "   Preview: Recognition order  -----------------  Inline markup delimiter characters are used for multiple constructs, so to avoid ambiguity there must be a specific recognition order for each character.  The inl...\n",
      "43. sections_content_block_212_to_Interpreted Text\n",
      "   Type: grouped_sections | Level: 1 | Lines: 15 | Tokens: 67\n",
      "   Preview: ---------------  :Doctree element:  `\\<strong>`_ :Start/End string: ``**`` :Standard role:    `:strong:`_  Text enclosed by double-asterisks is emphasized strongly::      This is **strong text**.  Str...\n",
      "44. content_block_214\n",
      "   Type: content_block | Level: 1 | Lines: 67 | Tokens: 610\n",
      "   Preview: ----------------  :Doctree element:  depends on the explicit or implicit role and                    processing :Start/End string: `````\\ :Configuration:    `\"default-role\"`_ directive :See also:...\n",
      "45. sections_Inline Literals_to_content_block_219\n",
      "   Type: grouped_sections | Level: 1 | Lines: 43 | Tokens: 278\n",
      "   Preview: Inline Literals  ---------------  :Doctree element:  `\\<literal>`_ :Start/End string: ``````\\ :Standard role:    `:literal:`_ :See also:         `:code:`_  Text enclosed by double-backquotes is treate...\n",
      "46. sections_content_block_220_to_Embedded URIs and Aliases\n",
      "   Type: grouped_sections | Level: 1 | Lines: 41 | Tokens: 337\n",
      "   Preview: .. table::      :class: borderless      :widths: grid       ===================  ==========  ========  =========       reference type       name        start     end      ===================  ======...\n",
      "47. content_block_222\n",
      "   Type: content_block | Level: 1 | Lines: 72 | Tokens: 640\n",
      "   Preview: `````````````````````````  :Doctree elements:  `\\<reference>`_, `\\<target>`_ :Start/End strings: ``<``   ``>``                     (only recognized inside `hyperlink references`_)  A hyperlink referen...\n",
      "48. sections_content_block_223_to_Citation References\n",
      "   Type: grouped_sections | Level: 1 | Lines: 78 | Tokens: 533\n",
      "   Preview: .. CAUTION::     This construct offers easy authoring and maintenance of hyperlinks    at the expense of general readability.  Inline URIs, especially    long ones, inevitably interrupt the natural fl...\n",
      "49. sections_content_block_230_to_Standalone Hyperlinks\n",
      "   Type: grouped_sections | Level: 1 | Lines: 48 | Tokens: 310\n",
      "   Preview: -------------------  :Doctree element: `\\<citation_reference>`_ :Start/End string: ``[``   ``]_`` :See also:        citations_  Each citation reference consists of a square-bracketed label followed by...\n",
      "50. sections_content_block_234_to_Length Units\n",
      "   Type: grouped_sections | Level: 1 | Lines: 72 | Tokens: 577\n",
      "   Preview: ---------------------  :Doctree element: `\\<reference>`_ :Start/End string: none  A URI [#URI]_ or standalone email address within a text block is treated as a general external hyperlink with the URI...\n",
      "  51. sections_content_block_238_to_content_block_243_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 117 | Tokens: 1057\n",
      "     Preview: ------------  The reStructuredText parser supports the `length units in CSS3`_. [#]_ Unit identifiers are case-sensitive (in contrast to CSS):   .. class:: align-center  ====  =======================...\n",
      "  52. sections_content_block_238_to_content_block_243_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 68 | Tokens: 1054\n",
      "     Preview: .. _`:raw:`: roles.html#raw .. _`:strong:`: roles.html#strong  .. _Document Tree: .. _The Docutils Document Tree: ../doctree.html .. _`<address>`: ../doctree.html#address .. _`<attribution>`: ../doctr...\n",
      "  53. sections_content_block_238_to_content_block_243_part_3\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 52 | Tokens: 520\n",
      "     Preview: .. _`<version>`: ../doctree.html#version .. _\"classes\" attribute:  ../doctree.html#classes .. _identifier key: ../doctree.html#identifiers .. _`measure`: ../doctree.html#measure .. _metadata title: .....\n",
      "\n",
      "🔍 Processing: history.rst\n",
      "🌳 Processing history.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 3 chunks\n",
      "Created 1 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_1_to_content_block_3 (2008 tokens)\n",
      "Final result: 2 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 2 chunks for history.rst\n",
      "  ✅ Saved: history.rst_chunk_001_jwqq7i.md\n",
      "  ✅ Saved: history.rst_chunk_002_jwqq7i.md\n",
      "\n",
      "--- RST Chunk Summary for history.rst ---\n",
      "  1. sections_content_block_1_to_content_block_3_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 92 | Tokens: 1041\n",
      "     Preview: .. include:: ../../header2.rst  =============================   History of reStructuredText  ============================= :Author: David Goodger :Contact: docutils-develop@lists.sourceforge.net :Revi...\n",
      "  2. sections_content_block_1_to_content_block_3_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 78 | Tokens: 966\n",
      "     Preview: the two projects; reStructuredText_ has found its place as one possible choice for a single component of the larger framework.  The project web site and the first project release were rolled out in Ju...\n",
      "\n",
      "🔍 Processing: definitions.rst\n",
      "🌳 Processing definitions.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 22 chunks\n",
      "Created 3 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_1_to_content_block_15 (1774 tokens)\n",
      "Final result: 4 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 4 chunks for definitions.rst\n",
      "  ✅ Saved: definitions.rst_chunk_001_zp5skx.md\n",
      "  ✅ Saved: definitions.rst_chunk_002_zp5skx.md\n",
      "  ✅ Saved: definitions.rst_chunk_003_zp5skx.md\n",
      "  ✅ Saved: definitions.rst_chunk_004_zp5skx.md\n",
      "\n",
      "--- RST Chunk Summary for definitions.rst ---\n",
      "  1. sections_content_block_1_to_content_block_15_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 139 | Tokens: 1072\n",
      "     Preview: .. include:: ../../header2.rst  .. include:: <html-roles.txt>  ============================================   reStructuredText Standard Definition Files  ============================================ :...\n",
      "  2. sections_content_block_1_to_content_block_15_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 39 | Tokens: 701\n",
      "     Preview: basic multilingual plane or BMP (wide-Unicode; code points greater    than U+FFFF).  Before Python 3.3, some distributions were \"narrow\"    and did not support wide-Unicode characters. This should...\n",
      "3. sections_Role Definitions_to_S5/HTML Definitions\n",
      "   Type: grouped_sections | Level: 1 | Lines: 64 | Tokens: 573\n",
      "   Preview: Role Definitions  ================  Role definitions use the `\"role\" directive`_ to provide additional `reStructuredText interpreted text roles`_.  .. _\"role\" directive: directives.html#role .. _reStr...\n",
      "4. content_block_22\n",
      "   Type: content_block | Level: 1 | Lines: 19 | Tokens: 122\n",
      "   Preview: -------------------  The \"s5defs.txt_\" standard definition file contains interpreted text roles (classes) and other definitions for documents destined to become `S5/HTML slide shows`_.  .. _s5defs.txt...\n",
      "\n",
      "🔍 Processing: introduction.rst\n",
      "🌳 Processing introduction.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 5 chunks\n",
      "Created 2 grouped chunks\n",
      "Final result: 2 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 2 chunks for introduction.rst\n",
      "  ✅ Saved: introduction.rst_chunk_001_tzxh4g.md\n",
      "  ✅ Saved: introduction.rst_chunk_002_tzxh4g.md\n",
      "\n",
      "--- RST Chunk Summary for introduction.rst ---\n",
      "1. sections_content_block_1_to_Goals\n",
      "   Type: grouped_sections | Level: 1 | Lines: 48 | Tokens: 438\n",
      "   Preview: .. include:: ../../header2.rst  =====================================   An Introduction to reStructuredText  ===================================== :Author: David Goodger :Contact: docutils-develop@lis...\n",
      "2. content_block_5\n",
      "   Type: content_block | Level: 1 | Lines: 108 | Tokens: 954\n",
      "   Preview: =====  The primary goal of reStructuredText_ is to define a markup syntax for use in Python docstrings and other documentation domains, that is readable and simple, yet powerful enough for non-trivial...\n",
      "\n",
      "🎉 Processing complete!\n",
      "✅ Files processed: 60/60\n",
      "📄 Total chunks created: 635\n",
      "📁 Output directory: /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils_rst_chunks\n",
      "🌳 All chunks generated with tree-sitter (no warning contamination!)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Tree-sitter RST Chunker\n",
    "\n",
    "A clean, robust approach to RST chunking using tree-sitter instead of docutils.\n",
    "No warning injection, clean error handling, syntax-aware chunking.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import secrets\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "try:\n",
    "    from tree_sitter_language_pack import get_language, get_parser\n",
    "    \n",
    "    # Get RST language from language pack\n",
    "    try:\n",
    "        RST_LANGUAGE = tsx_language = get_language('rst')\n",
    "        print(\"✅ Found RST language in tree-sitter-language-pack\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not load RST from language pack: {e}\")\n",
    "        print(\"Will use fallback text-based chunking\")\n",
    "        RST_LANGUAGE = None\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"❌ tree-sitter-language-pack not installed. Install with: pip install tree-sitter-language-pack\")\n",
    "    print(\"Will use fallback text-based chunking\")\n",
    "    RST_LANGUAGE = None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION AND CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "MAX_CHUNK_TOKENS = 1000\n",
    "TARGET_TOKENS = 600\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA STRUCTURES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class RSTChunk:\n",
    "    \"\"\"Represents a semantic chunk of RST content\"\"\"\n",
    "    start_byte: int\n",
    "    end_byte: int\n",
    "    content: str\n",
    "    node_type: str\n",
    "    name: str\n",
    "    depth: int\n",
    "    level: int = 0\n",
    "    token_count: int = 0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.token_count == 0:\n",
    "            self.token_count = count_tokens(self.content)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RSTReference:\n",
    "    \"\"\"Represents a reference/include in RST\"\"\"\n",
    "    reference_type: str\n",
    "    target: str\n",
    "    start_byte: int\n",
    "    end_byte: int\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TOKEN COUNTING\n",
    "# =============================================================================\n",
    "\n",
    "def count_tokens(content: str) -> int:\n",
    "    \"\"\"Count tokens using tiktoken for GPT-4\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "        return len(encoding.encode(content))\n",
    "    except Exception:\n",
    "        return len(content) // 4\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TREE-SITTER PARSING\n",
    "# =============================================================================\n",
    "\n",
    "def parse_rst_with_tree_sitter(content: str) -> Optional[object]:\n",
    "    \"\"\"Parse RST content using tree-sitter - NO WARNING INJECTION!\"\"\"\n",
    "    if not RST_LANGUAGE:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        parser = Parser()\n",
    "        parser.set_language(RST_LANGUAGE)\n",
    "        \n",
    "        # Tree-sitter parsing - clean and simple\n",
    "        tree = parser.parse(content.encode('utf-8'))\n",
    "        \n",
    "        # Check for parse errors (but they don't contaminate content!)\n",
    "        if tree.root_node.has_error:\n",
    "            print(\"⚠️ Parse tree contains error nodes (but content stays clean)\")\n",
    "        \n",
    "        return tree\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Tree-sitter parsing failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_node_text(node: object, source_bytes: bytes) -> str:\n",
    "    \"\"\"Extract clean text from tree-sitter node\"\"\"\n",
    "    return source_bytes[node.start_byte:node.end_byte].decode('utf-8', errors='ignore')\n",
    "\n",
    "\n",
    "def find_rst_sections(node: object, source_bytes: bytes, depth: int = 0) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Find RST sections in the parse tree\"\"\"\n",
    "    sections = []\n",
    "    \n",
    "    # Look for section-like nodes\n",
    "    if node.type in ['section', 'title', 'heading']:\n",
    "        text = extract_node_text(node, source_bytes)\n",
    "        sections.append({\n",
    "            'type': 'section',\n",
    "            'name': text.strip().split('\\n')[0][:50],  # First line as name\n",
    "            'start_byte': node.start_byte,\n",
    "            'end_byte': node.end_byte,\n",
    "            'content': text,\n",
    "            'depth': depth,\n",
    "            'level': depth + 1\n",
    "        })\n",
    "    \n",
    "    # Look for directive nodes (.. note::, .. code-block::, etc.)\n",
    "    elif node.type in ['directive', 'admonition', 'code_block']:\n",
    "        text = extract_node_text(node, source_bytes)\n",
    "        directive_name = text.split('\\n')[0].strip()\n",
    "        sections.append({\n",
    "            'type': f'{node.type}_directive',\n",
    "            'name': directive_name,\n",
    "            'start_byte': node.start_byte,\n",
    "            'end_byte': node.end_byte,\n",
    "            'content': text,\n",
    "            'depth': depth,\n",
    "            'level': 8  # Lower priority for directives\n",
    "        })\n",
    "    \n",
    "    # Recursively check children\n",
    "    for child in node.children:\n",
    "        sections.extend(find_rst_sections(child, source_bytes, depth + 1))\n",
    "    \n",
    "    return sections\n",
    "\n",
    "\n",
    "def extract_references_tree_sitter(node: object, source_bytes: bytes) -> List[RSTReference]:\n",
    "    \"\"\"Extract references using tree-sitter - much cleaner than docutils\"\"\"\n",
    "    references = []\n",
    "    \n",
    "    # Look for reference-like nodes\n",
    "    if node.type in ['reference', 'link', 'image', 'include']:\n",
    "        text = extract_node_text(node, source_bytes)\n",
    "        ref_type = node.type\n",
    "        target = text.strip()\n",
    "        \n",
    "        references.append(RSTReference(\n",
    "            reference_type=ref_type,\n",
    "            target=target,\n",
    "            start_byte=node.start_byte,\n",
    "            end_byte=node.end_byte\n",
    "        ))\n",
    "    \n",
    "    # Recursively check children\n",
    "    for child in node.children:\n",
    "        references.extend(extract_references_tree_sitter(child, source_bytes))\n",
    "    \n",
    "    return references\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FALLBACK TEXT-BASED CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def chunk_rst_by_text_structure(content: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fallback chunking when tree-sitter isn't available.\n",
    "    Uses text patterns to identify RST structures.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    lines = content.split('\\n')\n",
    "    current_chunk_lines = []\n",
    "    current_start = 0\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        line_stripped = line.strip()\n",
    "        \n",
    "        # Check if this looks like a section header\n",
    "        is_section_header = False\n",
    "        if i < len(lines) - 1:\n",
    "            next_line = lines[i + 1].strip()\n",
    "            # RST section headers have underlines of =, -, ~, ^, etc.\n",
    "            if (line_stripped and next_line and \n",
    "                all(c in '=-~^\"\\'`#*+<>' for c in next_line) and\n",
    "                len(next_line) >= len(line_stripped) * 0.8):\n",
    "                is_section_header = True\n",
    "        \n",
    "        # Check if this is a directive\n",
    "        is_directive = line_stripped.startswith('.. ') and '::' in line_stripped\n",
    "        \n",
    "        # If we hit a section or directive, save previous chunk\n",
    "        if (is_section_header or is_directive) and current_chunk_lines:\n",
    "            chunk_content = '\\n'.join(current_chunk_lines)\n",
    "            if chunk_content.strip():\n",
    "                chunks.append({\n",
    "                    'type': 'content_block',\n",
    "                    'name': f'content_block_{len(chunks) + 1}',\n",
    "                    'start_byte': sum(len(l) + 1 for l in lines[:current_start]),\n",
    "                    'end_byte': sum(len(l) + 1 for l in lines[:i]),\n",
    "                    'content': chunk_content,\n",
    "                    'depth': 0,\n",
    "                    'level': 1\n",
    "                })\n",
    "            current_chunk_lines = []\n",
    "            current_start = i\n",
    "        \n",
    "        current_chunk_lines.append(line)\n",
    "        \n",
    "        # If this was a section header, process it\n",
    "        if is_section_header:\n",
    "            section_content = '\\n'.join(current_chunk_lines)\n",
    "            chunks.append({\n",
    "                'type': 'section',\n",
    "                'name': line_stripped,\n",
    "                'start_byte': sum(len(l) + 1 for l in lines[:current_start]),\n",
    "                'end_byte': sum(len(l) + 1 for l in lines[:i + 2]),  # Include underline\n",
    "                'content': section_content,\n",
    "                'depth': 0,\n",
    "                'level': 1\n",
    "            })\n",
    "            current_chunk_lines = []\n",
    "            current_start = i + 2\n",
    "    \n",
    "    # Add final chunk\n",
    "    if current_chunk_lines:\n",
    "        chunk_content = '\\n'.join(current_chunk_lines)\n",
    "        if chunk_content.strip():\n",
    "            chunks.append({\n",
    "                'type': 'content_block',\n",
    "                'name': f'content_block_{len(chunks) + 1}',\n",
    "                'start_byte': sum(len(l) + 1 for l in lines[:current_start]),\n",
    "                'end_byte': len(content),\n",
    "                'content': chunk_content,\n",
    "                'depth': 0,\n",
    "                'level': 1\n",
    "            })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def process_rst_content_tree_sitter(rst_content: str, file_name: str = \"content.rst\") -> List[RSTChunk]:\n",
    "    \"\"\"\n",
    "    Process RST content using tree-sitter approach.\n",
    "    GUARANTEED: No warning injection, clean content extraction.\n",
    "    \"\"\"\n",
    "    print(f\"🌳 Processing {file_name} with tree-sitter approach\")\n",
    "    \n",
    "    # Try tree-sitter parsing first\n",
    "    semantic_nodes = []\n",
    "    \n",
    "    if RST_LANGUAGE:\n",
    "        tree = parse_rst_with_tree_sitter(rst_content)\n",
    "        if tree:\n",
    "            source_bytes = rst_content.encode('utf-8')\n",
    "            \n",
    "            # Extract semantic chunks using tree-sitter\n",
    "            semantic_nodes = find_rst_sections(tree.root_node, source_bytes)\n",
    "            \n",
    "            # Extract references\n",
    "            references = extract_references_tree_sitter(tree.root_node, source_bytes)\n",
    "            \n",
    "            print(f\"✅ Tree-sitter found {len(semantic_nodes)} semantic units\")\n",
    "            print(f\"📎 Found {len(references)} references\")\n",
    "    \n",
    "    # Fallback to text-based chunking if tree-sitter failed\n",
    "    if not semantic_nodes:\n",
    "        print(\"📝 Falling back to text-based structure detection\")\n",
    "        semantic_nodes = chunk_rst_by_text_structure(rst_content)\n",
    "        print(f\"✅ Text-based chunking found {len(semantic_nodes)} chunks\")\n",
    "    \n",
    "    # Convert to RSTChunk objects\n",
    "    chunks = []\n",
    "    for node_info in semantic_nodes:\n",
    "        chunk = RSTChunk(\n",
    "            start_byte=node_info['start_byte'],\n",
    "            end_byte=node_info['end_byte'],\n",
    "            content=node_info['content'],\n",
    "            node_type=node_info['type'],\n",
    "            name=node_info['name'],\n",
    "            depth=node_info['depth'],\n",
    "            level=node_info['level']\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # Group small chunks\n",
    "    chunks = group_small_chunks(chunks, target_tokens=TARGET_TOKENS)\n",
    "    print(f\"Created {len(chunks)} grouped chunks\")\n",
    "    \n",
    "    # Sub-chunk oversized chunks\n",
    "    final_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if chunk.token_count > MAX_CHUNK_TOKENS:\n",
    "            print(f\"  📦 Sub-chunking {chunk.name} ({chunk.token_count} tokens)\")\n",
    "            sub_chunks = sub_chunk_by_bytes(chunk, rst_content)\n",
    "            final_chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            final_chunks.append(chunk)\n",
    "    \n",
    "    print(f\"Final result: {len(final_chunks)} clean chunks (no warnings injected!)\")\n",
    "    \n",
    "    # Verify content cleanliness\n",
    "    warning_patterns = ['(WARNING/', '(ERROR/', '(INFO/', '(SEVERE/']\n",
    "    contaminated_chunks = 0\n",
    "    for chunk in final_chunks:\n",
    "        if any(pattern in chunk.content for pattern in warning_patterns):\n",
    "            contaminated_chunks += 1\n",
    "    \n",
    "    if contaminated_chunks == 0:\n",
    "        print(\"✅ All chunks verified clean - no docutils warnings!\")\n",
    "    else:\n",
    "        print(f\"⚠️ {contaminated_chunks} chunks may contain warnings\")\n",
    "    \n",
    "    return final_chunks\n",
    "\n",
    "\n",
    "def group_small_chunks(chunks: List[RSTChunk], target_tokens: int = 600) -> List[RSTChunk]:\n",
    "    \"\"\"Group small chunks together\"\"\"\n",
    "    if not chunks:\n",
    "        return chunks\n",
    "    \n",
    "    total_tokens = sum(c.token_count for c in chunks)\n",
    "    if total_tokens <= MAX_CHUNK_TOKENS:\n",
    "        combined_content = \"\\n\\n\".join(c.content for c in chunks)\n",
    "        return [RSTChunk(\n",
    "            start_byte=min(c.start_byte for c in chunks),\n",
    "            end_byte=max(c.end_byte for c in chunks),\n",
    "            content=combined_content,\n",
    "            node_type=\"combined_document\",\n",
    "            name=\"complete_document\",\n",
    "            depth=0,\n",
    "            level=0\n",
    "        )]\n",
    "    \n",
    "    grouped_chunks = []\n",
    "    current_group = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if (current_tokens + chunk.token_count > target_tokens and \n",
    "            current_group and chunk.token_count <= MAX_CHUNK_TOKENS):\n",
    "            \n",
    "            if len(current_group) == 1:\n",
    "                grouped_chunks.append(current_group[0])\n",
    "            else:\n",
    "                combined_content = \"\\n\\n\".join(c.content for c in current_group)\n",
    "                grouped_chunk = RSTChunk(\n",
    "                    start_byte=min(c.start_byte for c in current_group),\n",
    "                    end_byte=max(c.end_byte for c in current_group),\n",
    "                    content=combined_content,\n",
    "                    node_type=\"grouped_sections\",\n",
    "                    name=f\"sections_{current_group[0].name}_to_{current_group[-1].name}\",\n",
    "                    depth=min(c.depth for c in current_group),\n",
    "                    level=min(c.level for c in current_group)\n",
    "                )\n",
    "                grouped_chunks.append(grouped_chunk)\n",
    "            \n",
    "            current_group = [chunk]\n",
    "            current_tokens = chunk.token_count\n",
    "        else:\n",
    "            current_group.append(chunk)\n",
    "            current_tokens += chunk.token_count\n",
    "    \n",
    "    if current_group:\n",
    "        if len(current_group) == 1:\n",
    "            grouped_chunks.append(current_group[0])\n",
    "        else:\n",
    "            combined_content = \"\\n\\n\".join(c.content for c in current_group)\n",
    "            grouped_chunk = RSTChunk(\n",
    "                start_byte=min(c.start_byte for c in current_group),\n",
    "                end_byte=max(c.end_byte for c in current_group),\n",
    "                content=combined_content,\n",
    "                node_type=\"grouped_sections\",\n",
    "                name=f\"sections_{current_group[0].name}_to_{current_group[-1].name}\",\n",
    "                depth=min(c.depth for c in current_group),\n",
    "                level=min(c.level for c in current_group)\n",
    "            )\n",
    "            grouped_chunks.append(grouped_chunk)\n",
    "    \n",
    "    return grouped_chunks\n",
    "\n",
    "\n",
    "def sub_chunk_by_bytes(chunk: RSTChunk, rst_content: str) -> List[RSTChunk]:\n",
    "    \"\"\"Sub-chunk oversized chunks by byte boundaries\"\"\"\n",
    "    if chunk.token_count <= MAX_CHUNK_TOKENS:\n",
    "        return [chunk]\n",
    "    \n",
    "    content = chunk.content\n",
    "    lines = content.split('\\n')\n",
    "    sub_chunks = []\n",
    "    current_lines = []\n",
    "    current_tokens = 0\n",
    "    part_num = 1\n",
    "    \n",
    "    for line in lines:\n",
    "        line_tokens = count_tokens(line)\n",
    "        \n",
    "        if current_tokens + line_tokens > MAX_CHUNK_TOKENS and current_lines:\n",
    "            sub_content = '\\n'.join(current_lines)\n",
    "            if sub_content.strip():\n",
    "                sub_chunk = RSTChunk(\n",
    "                    start_byte=chunk.start_byte,\n",
    "                    end_byte=chunk.start_byte + len(sub_content.encode('utf-8')),\n",
    "                    content=sub_content,\n",
    "                    node_type=f\"{chunk.node_type}_part\",\n",
    "                    name=f\"{chunk.name}_part_{part_num}\",\n",
    "                    depth=chunk.depth + 1,\n",
    "                    level=chunk.level\n",
    "                )\n",
    "                sub_chunks.append(sub_chunk)\n",
    "                part_num += 1\n",
    "            \n",
    "            current_lines = [line]\n",
    "            current_tokens = line_tokens\n",
    "        else:\n",
    "            current_lines.append(line)\n",
    "            current_tokens += line_tokens\n",
    "    \n",
    "    if current_lines:\n",
    "        sub_content = '\\n'.join(current_lines)\n",
    "        if sub_content.strip():\n",
    "            sub_chunk = RSTChunk(\n",
    "                start_byte=chunk.end_byte - len(sub_content.encode('utf-8')),\n",
    "                end_byte=chunk.end_byte,\n",
    "                content=sub_content,\n",
    "                node_type=f\"{chunk.node_type}_part\",\n",
    "                name=f\"{chunk.name}_part_{part_num}\",\n",
    "                depth=chunk.depth + 1,\n",
    "                level=chunk.level\n",
    "            )\n",
    "            sub_chunks.append(sub_chunk)\n",
    "    \n",
    "    return sub_chunks if sub_chunks else [chunk]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FILE PROCESSING AND DIRECTORY HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def process_rst_file(file_path: Path) -> List[RSTChunk]:\n",
    "    \"\"\"Process a single RST file and return chunks\"\"\"\n",
    "    print(f\"\\n🔍 Processing: {file_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            rst_content = f.read()\n",
    "        \n",
    "        chunks = process_rst_content_tree_sitter(rst_content, file_path.name)\n",
    "        \n",
    "        if chunks:\n",
    "            print(f\"✅ Generated {len(chunks)} chunks for {file_path.name}\")\n",
    "        else:\n",
    "            print(f\"⚠️ No chunks generated for {file_path.name}\")\n",
    "            \n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def save_chunks_to_files(chunks: List[RSTChunk], \n",
    "                        original_file_path: Path, \n",
    "                        input_directory: Path,\n",
    "                        output_base: Path,\n",
    "                        references: List[RSTReference]) -> List[str]:\n",
    "    \"\"\"Save chunks as markdown files maintaining directory structure\"\"\"\n",
    "    if not chunks:\n",
    "        return []\n",
    "    \n",
    "    # Calculate relative path from input directory\n",
    "    try:\n",
    "        rel_path = original_file_path.relative_to(input_directory)\n",
    "    except ValueError:\n",
    "        # If file is not under input directory, use just the filename\n",
    "        rel_path = original_file_path.name\n",
    "    \n",
    "    # Create output directory structure\n",
    "    output_dir = output_base / rel_path.parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate unique ID for this file\n",
    "    file_unique_id = generate_unique_id()\n",
    "    \n",
    "    saved_files = []\n",
    "    \n",
    "    # Add chunk index to each chunk and save\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        # Create chunk filename\n",
    "        chunk_filename = f\"{original_file_path.name}_chunk_{i:03d}_{file_unique_id}.md\"\n",
    "        \n",
    "        # Create markdown content\n",
    "        markdown_content = create_chunk_markdown(\n",
    "            chunk, \n",
    "            str(rel_path), \n",
    "            references\n",
    "        )\n",
    "        \n",
    "        # Write to file\n",
    "        chunk_file_path = output_dir / chunk_filename\n",
    "        try:\n",
    "            with open(chunk_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "            \n",
    "            saved_files.append(str(chunk_file_path))\n",
    "            print(f\"  ✅ Saved: {chunk_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error saving {chunk_filename}: {e}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "\n",
    "def print_chunk_summary(chunks: List[RSTChunk], file_name: str):\n",
    "    \"\"\"Print detailed summary of chunks\"\"\"\n",
    "    print(f\"\\n--- RST Chunk Summary for {file_name} ---\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        indent = \"  \" * chunk.depth\n",
    "        content_lines = len(chunk.content.split('\\n'))\n",
    "        \n",
    "        print(f\"{indent}{i}. {chunk.name}\")\n",
    "        print(f\"{indent}   Type: {chunk.node_type} | Level: {chunk.level} | Lines: {content_lines} | Tokens: {chunk.token_count}\")\n",
    "        \n",
    "        # Show content preview\n",
    "        preview = chunk.content[:200].replace('\\n', ' ').strip()\n",
    "        if len(chunk.content) > 200:\n",
    "            preview += \"...\"\n",
    "        print(f\"{indent}   Preview: {preview}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING - SAME INTERFACE AS ORIGINAL\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for RST semantic chunking - SAME INTERFACE AS ORIGINAL\"\"\"\n",
    "    print(\"🚀 RST (reStructuredText) Semantic Chunking with Tree-sitter\")\n",
    "    print(f\"Max chunk tokens: {MAX_CHUNK_TOKENS}\")\n",
    "    print(f\"Target tokens for grouping: {TARGET_TOKENS}\")\n",
    "    print(\"🌳 Using tree-sitter for clean, warning-free parsing\")\n",
    "    \n",
    "    # Get directory from user or use current directory - SAME AS ORIGINAL\n",
    "    directory = input(\"\\nEnter source directory path (or press Enter for current directory): \").strip()\n",
    "    if not directory:\n",
    "        directory = \".\"\n",
    "    \n",
    "    target_dir = Path(directory).resolve()\n",
    "    if not target_dir.exists():\n",
    "        print(f\"❌ Directory not found: {directory}\")\n",
    "        return\n",
    "    \n",
    "    # Create output directory parallel to source directory - SAME AS ORIGINAL\n",
    "    output_dir = target_dir.parent / f\"{target_dir.name}_rst_chunks\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    print(f\"📁 Output directory: {output_dir}\")\n",
    "    \n",
    "    target_path = target_dir\n",
    "    input_directory = target_dir\n",
    "    \n",
    "    # Collect RST files - SAME AS ORIGINAL\n",
    "    rst_files = []\n",
    "    for ext in ['*.rst', '*.txt']:\n",
    "        rst_files.extend(target_path.rglob(ext))\n",
    "    \n",
    "    # Filter to actual RST files by checking content - SAME AS ORIGINAL\n",
    "    actual_rst_files = []\n",
    "    for file in rst_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read(1000)  # Check first 1000 chars\n",
    "                # Simple heuristic: look for RST-like content\n",
    "                if any(marker in content for marker in ['===', '---', '~~~', '^^^', '.. ', '::']):\n",
    "                    actual_rst_files.append(file)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    rst_files = actual_rst_files\n",
    "    \n",
    "    if not rst_files:\n",
    "        print(f\"❌ No RST files found in {directory}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"🔍 Found {len(rst_files)} RST file(s)\")\n",
    "    \n",
    "    # Group by directory for display - SAME AS ORIGINAL\n",
    "    by_dir = {}\n",
    "    for f in rst_files:\n",
    "        dir_path = str(f.parent.relative_to(input_directory)) if f.parent != input_directory else '.'\n",
    "        if dir_path not in by_dir:\n",
    "            by_dir[dir_path] = []\n",
    "        by_dir[dir_path].append(f)\n",
    "    \n",
    "    # Show files found - SAME AS ORIGINAL\n",
    "    for dir_path, files in sorted(by_dir.items()):\n",
    "        print(f\"  📂 {dir_path}:\")\n",
    "        for f in files:\n",
    "            print(f\"    - {f.name}\")\n",
    "    \n",
    "    proceed = input(f\"\\nProcess all {len(rst_files)} files? (y/n): \").strip().lower()\n",
    "    if proceed != 'y':\n",
    "        print(\"❌ Processing cancelled\")\n",
    "        return\n",
    "    \n",
    "    # Process all files - SAME AS ORIGINAL\n",
    "    total_chunks = 0\n",
    "    processed_files = 0\n",
    "    \n",
    "    for file_path in rst_files:\n",
    "        try:\n",
    "            chunks = process_rst_file(file_path)\n",
    "            \n",
    "            if chunks:\n",
    "                # Extract references for this file\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                    references = extract_references_from_source(content)\n",
    "                except:\n",
    "                    references = []\n",
    "                \n",
    "                # Save chunks - SAME AS ORIGINAL\n",
    "                saved_files = save_chunks_to_files(\n",
    "                    chunks, file_path, input_directory, output_dir, references\n",
    "                )\n",
    "                \n",
    "                total_chunks += len(chunks)\n",
    "                processed_files += 1\n",
    "                \n",
    "                print_chunk_summary(chunks, file_path.name)\n",
    "            else:\n",
    "                print(f\"⚠️ No chunks generated for {file_path.name}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Processing complete!\")\n",
    "    print(f\"✅ Files processed: {processed_files}/{len(rst_files)}\")\n",
    "    print(f\"📄 Total chunks created: {total_chunks}\")\n",
    "    print(f\"📁 Output directory: {output_dir}\")\n",
    "    print(\"🌳 All chunks generated with tree-sitter (no warning contamination!)\")\n",
    "\n",
    "\n",
    "def extract_references_from_source(rst_content: str) -> List[RSTReference]:\n",
    "    \"\"\"Extract references from source text as fallback\"\"\"\n",
    "    references = []\n",
    "    lines = rst_content.split('\\n')\n",
    "    \n",
    "    for line_num, line in enumerate(lines, 1):\n",
    "        line = line.strip()\n",
    "        if line.startswith('..'):\n",
    "            for ref_type in ['include', 'literalinclude', 'image', 'figure', 'csv-table']:\n",
    "                if f'.. {ref_type}::' in line:\n",
    "                    parts = line.split('::', 1)\n",
    "                    if len(parts) > 1:\n",
    "                        target = parts[1].strip()\n",
    "                        references.append(RSTReference(\n",
    "                            reference_type=ref_type,\n",
    "                            target=target,\n",
    "                            start_byte=0,  # Approximate for text-based extraction\n",
    "                            end_byte=len(line)\n",
    "                        ))\n",
    "                    break\n",
    "    \n",
    "    return references\n",
    "\n",
    "def generate_unique_id(length: int = 6) -> str:\n",
    "    \"\"\"Generate a random unique ID\"\"\"\n",
    "    alphabet = string.ascii_lowercase + string.digits\n",
    "    return ''.join(secrets.choice(alphabet) for _ in range(length))\n",
    "\n",
    "\n",
    "def create_chunk_markdown(chunk: RSTChunk, source_file_path: str, references: List[RSTReference]) -> str:\n",
    "    \"\"\"Create markdown content with YAML frontmatter\"\"\"\n",
    "    unique_id = generate_unique_id()\n",
    "    \n",
    "    # Filter references that might apply to this chunk\n",
    "    chunk_references = []\n",
    "    for ref in references:\n",
    "        if chunk.start_byte <= ref.start_byte <= chunk.end_byte:\n",
    "            chunk_references.append(f\"{ref.reference_type}: {ref.target}\")\n",
    "    \n",
    "    frontmatter = f\"\"\"---\n",
    "file_path: \"{source_file_path}\"\n",
    "chunk_id: \"{unique_id}\"\n",
    "chunk_type: \"{chunk.node_type}\"\n",
    "chunk_name: \"{chunk.name}\"\n",
    "start_byte: {chunk.start_byte}\n",
    "end_byte: {chunk.end_byte}\n",
    "token_count: {chunk.token_count}\n",
    "depth: {chunk.depth}\n",
    "level: {chunk.level}\n",
    "language: \"rst\"\n",
    "references: {chunk_references}\n",
    "parser: \"tree-sitter\"\n",
    "---\n",
    "\n",
    "# {chunk.name}\n",
    "\n",
    "**Type:** {chunk.node_type}  \n",
    "**Tokens:** {chunk.token_count}  \n",
    "**Depth:** {chunk.depth}  \n",
    "**Level:** {chunk.level}\n",
    "\n",
    "```rst\n",
    "{chunk.content}\n",
    "```\n",
    "\"\"\"\n",
    "    return frontmatter\n",
    "\n",
    "\n",
    "def save_chunks_as_markdown(chunks: List[RSTChunk], output_dir: str = \"rst_chunks_tree_sitter\") -> None:\n",
    "    \"\"\"Save chunks as markdown files\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    file_unique_id = generate_unique_id()\n",
    "    saved_count = 0\n",
    "    \n",
    "    print(f\"\\n💾 Saving {len(chunks)} chunks to {output_path}/\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        chunk_filename = f\"chunk_{i:03d}_{file_unique_id}.md\"\n",
    "        markdown_content = create_chunk_markdown(chunk, \"notebook_content.rst\", [])\n",
    "        \n",
    "        chunk_file_path = output_path / chunk_filename\n",
    "        try:\n",
    "            with open(chunk_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "            \n",
    "            print(f\"  ✅ Saved: {chunk_filename}\")\n",
    "            saved_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error saving {chunk_filename}: {e}\")\n",
    "    \n",
    "    print(f\"✅ Successfully saved {saved_count} clean chunk files\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# NOTEBOOK INTERFACE (for direct content processing)\n",
    "# =============================================================================\n",
    "\n",
    "def save_chunks_as_markdown(chunks: List[RSTChunk], output_dir: str = \"rst_chunks_tree_sitter\") -> None:\n",
    "    \"\"\"Save chunks as markdown files (notebook version)\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    file_unique_id = generate_unique_id()\n",
    "    saved_count = 0\n",
    "    \n",
    "    print(f\"\\n💾 Saving {len(chunks)} chunks to {output_path}/\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        chunk_filename = f\"chunk_{i:03d}_{file_unique_id}.md\"\n",
    "        markdown_content = create_chunk_markdown(chunk, \"notebook_content.rst\", [])\n",
    "        \n",
    "        chunk_file_path = output_path / chunk_filename\n",
    "        try:\n",
    "            with open(chunk_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "            \n",
    "            print(f\"  ✅ Saved: {chunk_filename}\")\n",
    "            saved_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error saving {chunk_filename}: {e}\")\n",
    "    \n",
    "    print(f\"✅ Successfully saved {saved_count} clean chunk files (no warnings!)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82267468",
   "metadata": {},
   "outputs": [],
   "source": [
    "tttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6c022a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found RST language in tree-sitter-language-pack\n",
      "🚀 RST (reStructuredText) Semantic Chunking with Tree-sitter\n",
      "Max chunk tokens: 1000\n",
      "Target tokens for grouping: 600\n",
      "🌳 Using tree-sitter for clean, warning-free parsing\n",
      "📁 Output directory: /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils_rst_chunks\n",
      "🔍 Found 60 RST file(s)\n",
      "  📂 docs:\n",
      "    - index.rst\n",
      "    - header2.rst\n",
      "    - header0.rst\n",
      "    - header.rst\n",
      "  📂 docs/api:\n",
      "    - runtime-settings.rst\n",
      "    - publisher.rst\n",
      "    - transforms.rst\n",
      "  📂 docs/dev:\n",
      "    - policies.rst\n",
      "    - website.rst\n",
      "    - distributing.rst\n",
      "    - runtime-settings-processing.rst\n",
      "    - pysource.rst\n",
      "    - testing.rst\n",
      "    - todo.rst\n",
      "    - hacking.rst\n",
      "    - semantics.rst\n",
      "    - enthought-plan.rst\n",
      "    - enthought-rfp.rst\n",
      "    - repository.rst\n",
      "    - release.rst\n",
      "  📂 docs/dev/rst:\n",
      "    - alternatives.rst\n",
      "    - problems.rst\n",
      "  📂 docs/eps:\n",
      "    - index.rst\n",
      "    - ep-template.rst\n",
      "    - header.rst\n",
      "    - ep-001.rst\n",
      "    - ep-010.rst\n",
      "  📂 docs/howto:\n",
      "    - rst-directives.rst\n",
      "    - html-stylesheets.rst\n",
      "    - i18n.rst\n",
      "    - rst-roles.rst\n",
      "    - cmdline-tool.rst\n",
      "    - security.rst\n",
      "  📂 docs/peps:\n",
      "    - pep-0287.rst\n",
      "    - pep-0256.rst\n",
      "    - pep-0257.rst\n",
      "    - pep-0258.rst\n",
      "  📂 docs/ref:\n",
      "    - doctree.rst\n",
      "  📂 docs/ref/rst:\n",
      "    - roles.rst\n",
      "    - directives.rst\n",
      "    - mathematics.rst\n",
      "    - restructuredtext.rst\n",
      "    - history.rst\n",
      "    - definitions.rst\n",
      "    - introduction.rst\n",
      "  📂 docs/user:\n",
      "    - todo-lists.rst\n",
      "    - slide-shows.rst\n",
      "    - tools.rst\n",
      "    - config.rst\n",
      "    - html.rst\n",
      "    - smartquotes.rst\n",
      "    - manpage.rst\n",
      "    - emacs.rst\n",
      "    - mailing-lists.rst\n",
      "    - latex.rst\n",
      "    - odt.rst\n",
      "    - links.rst\n",
      "  📂 docs/user/rst:\n",
      "    - demo.rst\n",
      "    - quickstart.rst\n",
      "    - cheatsheet.rst\n",
      "\n",
      "🔍 Processing: index.rst\n",
      "🌳 Processing index.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 23 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/index.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for index.rst\n",
      "\n",
      "🔍 Processing: header2.rst\n",
      "🌳 Processing header2.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 2 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for header2.rst\n",
      "  ✅ Saved: header2.rst_chunk_001_6p3v86.md\n",
      "\n",
      "--- RST Chunk Summary for header2.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 17 | Tokens: 108\n",
      "   Preview: .. Minimal menu bar for inclusion in documentation sources    in ``docutils/docs/*/`` sub-sub-diretories.     Attention: this is not a standalone document.   .. header::    Docutils__ | Overview__ | A...\n",
      "\n",
      "🔍 Processing: header0.rst\n",
      "🌳 Processing header0.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 2 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for header0.rst\n",
      "  ✅ Saved: header0.rst_chunk_001_re9cog.md\n",
      "\n",
      "--- RST Chunk Summary for header0.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 16 | Tokens: 105\n",
      "   Preview: .. Minimal menu bar for inclusion in documentation sources    in the ``docutils/`` parent diretory.     Attention: this is not a standalone document.   .. header::    Docutils__ | Overview__ | About__...\n",
      "\n",
      "🔍 Processing: header.rst\n",
      "🌳 Processing header.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 2 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for header.rst\n",
      "  ✅ Saved: header.rst_chunk_001_fia6pz.md\n",
      "\n",
      "--- RST Chunk Summary for header.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 16 | Tokens: 107\n",
      "   Preview: .. Minimal menu bar for inclusion in documentation sources    in ``docutils/docs/*/`` sub-diretories.     Attention: this is not a standalone document.   .. header::    Docutils__ | Overview__ | About...\n",
      "\n",
      "🔍 Processing: doctree.rst\n",
      "🌳 Processing doctree.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 719 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/ref/doctree.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for doctree.rst\n",
      "\n",
      "🔍 Processing: pep-0287.rst\n",
      "🌳 Processing pep-0287.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 33 chunks\n",
      "Created 10 grouped chunks\n",
      "  📦 Sub-chunking sections_Rationale_to_content_block_9 (1040 tokens)\n",
      "  📦 Sub-chunking sections_Specification_to_content_block_13 (1396 tokens)\n",
      "  📦 Sub-chunking sections_content_block_14_to_content_block_23 (1325 tokens)\n",
      "  📦 Sub-chunking sections_Abstract_to_content_block_27 (1525 tokens)\n",
      "Final result: 13 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 13 chunks for pep-0287.rst\n",
      "  ✅ Saved: pep-0287.rst_chunk_001_872sy2.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_002_872sy2.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_003_872sy2.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_004_872sy2.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_005_872sy2.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_006_872sy2.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_007_872sy2.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_008_872sy2.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_009_872sy2.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_010_872sy2.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_011_872sy2.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_012_872sy2.md\n",
      "  ✅ Saved: pep-0287.rst_chunk_013_872sy2.md\n",
      "\n",
      "--- RST Chunk Summary for pep-0287.rst ---\n",
      "1. sections_content_block_1_to_Benefits\n",
      "   Type: grouped_sections | Level: 1 | Lines: 37 | Tokens: 287\n",
      "   Preview: PEP: 287 Title: reStructuredText Docstring Format Version: $Revision$ Last-Modified: $Date$ Author: David Goodger <goodger@python.org> Discussions-To: <doc-sig@python.org> Status: Draft Type: Informat...\n",
      "2. content_block_5\n",
      "   Type: content_block | Level: 1 | Lines: 54 | Tokens: 606\n",
      "   Preview: ========  Programmers are by nature a lazy breed.  We reuse code with functions, classes, modules, and subsystems.  Through its docstring syntax, Python allows us to document our code from within.  Th...\n",
      "3. Goals\n",
      "   Type: section | Level: 1 | Lines: 1 | Tokens: 1\n",
      "   Preview: Goals\n",
      "4. content_block_7\n",
      "   Type: content_block | Level: 1 | Lines: 72 | Tokens: 621\n",
      "   Preview: =====  These are the generally accepted goals for a docstring format, as discussed in the Doc-SIG:  1. It must be readable in source form by the casual observer.  2. It must be easy to type with any s...\n",
      "  5. sections_Rationale_to_content_block_9_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 110 | Tokens: 1040\n",
      "     Preview: Rationale  =========  The lack of a standard syntax for docstrings has hampered the development of standard tools for extracting and converting docstrings into documentation in standard formats (e.g.,...\n",
      "  6. sections_Specification_to_content_block_13_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 128 | Tokens: 1038\n",
      "     Preview: Specification  =============  The specification and user documentaton for reStructuredText is quite extensive.  Rather than repeating or summarizing it all here, links to the originals are provided....\n",
      "  7. sections_Specification_to_content_block_13_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 37 | Tokens: 358\n",
      "     Preview: The intention of the markup is that there should be little need to   use explicit roles; their use is to be kept to an absolute minimum.  - Markup for \"tagged lists\" or \"label lists\": field lists....\n",
      "  8. sections_content_block_14_to_content_block_23_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 121 | Tokens: 1043\n",
      "     Preview: .. image:: mylogo.png    Substitution definitions allow the power and flexibility of   block-level directives to be shared by inline text.  For example::        The |biohazard| symbol must be us...\n",
      "  9. sections_content_block_14_to_content_block_23_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 31 | Tokens: 281\n",
      "     Preview: deleted.  It is painful to renumber the references, since it has to    be done in two places and can have a cascading effect (insert a    single new reference 1, and every other reference has to be...\n",
      "  10. sections_Abstract_to_content_block_27_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 103 | Tokens: 1043\n",
      "     Preview: Abstract         ========         This PEP proposes adding `frungible doodads`_ to the core.  It        extends PEP 9876 [#pep9876]_ via the BCA [#]_ mechanism.         ...          References...\n",
      "  11. sections_Abstract_to_content_block_27_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 51 | Tokens: 481\n",
      "     Preview: reStructuredText`_:         Unambiguous.  The rules for markup must not be open for        interpretation.  For any given input, there should be one and        only one possible output (including e...\n",
      "12. sections_References & Footnotes_to_Acknowledgements\n",
      "   Type: grouped_sections | Level: 1 | Lines: 77 | Tokens: 571\n",
      "   Preview: References & Footnotes  ======================  .. [#PEP-1] PEP 1, PEP Guidelines, Warsaw, Hylton    (http://www.python.org/peps/pep-0001.html)  .. [#PEP-9] PEP 9, Sample PEP Template, Warsaw    (http...\n",
      "13. content_block_33\n",
      "   Type: content_block | Level: 1 | Lines: 18 | Tokens: 92\n",
      "   Preview: ================  Some text is borrowed from PEP 216, Docstring Format [#PEP-216]_, by Moshe Zadka.  Special thanks to all members past & present of the Python Doc-SIG_.   ..\f Emacs settings     Local...\n",
      "\n",
      "🔍 Processing: pep-0256.rst\n",
      "🌳 Processing pep-0256.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 21 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/peps/pep-0256.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for pep-0256.rst\n",
      "\n",
      "🔍 Processing: pep-0257.rst\n",
      "🌳 Processing pep-0257.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 21 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/peps/pep-0257.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for pep-0257.rst\n",
      "\n",
      "🔍 Processing: pep-0258.rst\n",
      "🌳 Processing pep-0258.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 58 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/peps/pep-0258.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for pep-0258.rst\n",
      "\n",
      "🔍 Processing: todo-lists.rst\n",
      "🌳 Processing todo-lists.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 27 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for todo-lists.rst\n",
      "  ✅ Saved: todo-lists.rst_chunk_001_rrcg6q.md\n",
      "\n",
      "--- RST Chunk Summary for todo-lists.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 182 | Tokens: 983\n",
      "   Preview: .. include:: ../header.rst  ===================  Docutils TODO lists  ===================  TODO lists allow you to create a list of items with checkboxes. In `extended Markdown`_, they are called `tas...\n",
      "\n",
      "🔍 Processing: slide-shows.rst\n",
      "🌳 Processing slide-shows.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 182 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/user/slide-shows.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for slide-shows.rst\n",
      "\n",
      "🔍 Processing: tools.rst\n",
      "🌳 Processing tools.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 63 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/user/tools.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for tools.rst\n",
      "\n",
      "🔍 Processing: config.rst\n",
      "🌳 Processing config.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 420 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/user/config.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for config.rst\n",
      "\n",
      "🔍 Processing: html.rst\n",
      "🌳 Processing html.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 20 chunks\n",
      "Created 5 grouped chunks\n",
      "Final result: 5 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 5 chunks for html.rst\n",
      "  ✅ Saved: html.rst_chunk_001_dud52a.md\n",
      "  ✅ Saved: html.rst_chunk_002_dud52a.md\n",
      "  ✅ Saved: html.rst_chunk_003_dud52a.md\n",
      "  ✅ Saved: html.rst_chunk_004_dud52a.md\n",
      "  ✅ Saved: html.rst_chunk_005_dud52a.md\n",
      "\n",
      "--- RST Chunk Summary for html.rst ---\n",
      "1. sections_content_block_1_to_html5\n",
      "   Type: grouped_sections | Level: 1 | Lines: 41 | Tokens: 234\n",
      "   Preview: .. include:: ../header.rst  =====================  Docutils HTML writers  =====================   .. contents::  This document describes the HTML writers provided by Docutils.  The default `length uni...\n",
      "2. sections_content_block_8_to_html4css1\n",
      "   Type: grouped_sections | Level: 1 | Lines: 50 | Tokens: 591\n",
      "   Preview: -----  :aliases:   _`html5_polyglot`, xhtml :front-end: rst2html5_ :config:    `[html5 writer]`_  The *html5* writer generates valid XML that conforms to the `HTML standard`_ (`polyglot HTML`_). [#saf...\n",
      "3. sections_content_block_10_to_s5_html\n",
      "   Type: grouped_sections | Level: 1 | Lines: 55 | Tokens: 561\n",
      "   Preview: ---------  :aliases:   html4, html_, xhtml10 :front-end: rst2html4_ :config:    `[html4css1 writer]`_  The HTML Writer module, ``docutils/writers/html4css1.py``, was the first Docutils writer and up t...\n",
      "4. sections_content_block_14_to_.. other references\n",
      "   Type: grouped_sections | Level: 1 | Lines: 71 | Tokens: 573\n",
      "   Preview: ~~~~~~~  :alias:     s5 :front-end: rst2s5_ :config:    `[s5_html writer]`_, `[html4css1 writer]`_  The `s5` writer inherits from html4css1_. It produces XHTML for use with S5_, the “Simple Standards-...\n",
      "5. content_block_20\n",
      "   Type: content_block | Level: 1 | Lines: 42 | Tokens: 433\n",
      "   Preview: ----------------  .. _HTML Compatibility Guidelines: https://www.w3.org/TR/xhtml1/#guidelines .. _transitional version:     https://www.w3.org/TR/xhtml1/#a_dtd_XHTML-1.0-Transitional  .. _polyglot...\n",
      "\n",
      "🔍 Processing: smartquotes.rst\n",
      "🌳 Processing smartquotes.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 22 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/user/smartquotes.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for smartquotes.rst\n",
      "\n",
      "🔍 Processing: manpage.rst\n",
      "🌳 Processing manpage.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 22 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/user/manpage.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for manpage.rst\n",
      "\n",
      "🔍 Processing: emacs.rst\n",
      "🌳 Processing emacs.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 102 chunks\n",
      "Created 15 grouped chunks\n",
      "Final result: 15 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 15 chunks for emacs.rst\n",
      "  ✅ Saved: emacs.rst_chunk_001_w4whl5.md\n",
      "  ✅ Saved: emacs.rst_chunk_002_w4whl5.md\n",
      "  ✅ Saved: emacs.rst_chunk_003_w4whl5.md\n",
      "  ✅ Saved: emacs.rst_chunk_004_w4whl5.md\n",
      "  ✅ Saved: emacs.rst_chunk_005_w4whl5.md\n",
      "  ✅ Saved: emacs.rst_chunk_006_w4whl5.md\n",
      "  ✅ Saved: emacs.rst_chunk_007_w4whl5.md\n",
      "  ✅ Saved: emacs.rst_chunk_008_w4whl5.md\n",
      "  ✅ Saved: emacs.rst_chunk_009_w4whl5.md\n",
      "  ✅ Saved: emacs.rst_chunk_010_w4whl5.md\n",
      "  ✅ Saved: emacs.rst_chunk_011_w4whl5.md\n",
      "  ✅ Saved: emacs.rst_chunk_012_w4whl5.md\n",
      "  ✅ Saved: emacs.rst_chunk_013_w4whl5.md\n",
      "  ✅ Saved: emacs.rst_chunk_014_w4whl5.md\n",
      "  ✅ Saved: emacs.rst_chunk_015_w4whl5.md\n",
      "\n",
      "--- RST Chunk Summary for emacs.rst ---\n",
      "1. sections_content_block_1_to_Checking situation\n",
      "   Type: grouped_sections | Level: 1 | Lines: 57 | Tokens: 414\n",
      "   Preview: .. -*- coding: utf-8 -*-   .. include:: ../header.rst  ========================================     Emacs Support for reStructuredText  ========================================  :Authors: Stefan Merte...\n",
      "2. sections_content_block_11_to_Switching ``rst-mode`` on\n",
      "   Type: grouped_sections | Level: 1 | Lines: 81 | Tokens: 559\n",
      "   Preview: ------------------  Here are some steps to check your situation:  #. In Emacs_ switch to an empty buffer and try ::       M-x rst-mode     If this works you have ``rst.el`` installed somewhere. You ca...\n",
      "3. sections_content_block_15_to_content_block_17\n",
      "   Type: grouped_sections | Level: 1 | Lines: 80 | Tokens: 540\n",
      "   Preview: -------------------------  By default ``rst-mode`` is switched on for files ending in ``.rst`` or ``.rest``. If in a buffer you want to switch ``rst-mode`` on manually use ::    M-x rst-mode  If you w...\n",
      "4. sections_content_block_18_to_Promoting and Demoting Many Sections\n",
      "   Type: grouped_sections | Level: 1 | Lines: 59 | Tokens: 525\n",
      "   Preview: .. note:: The key bindings have been completely revamped in ``rst.el``           V1.0.0. This was necessary to make room for new           functionality. Some of the old bindings still work but give...\n",
      "5. sections_content_block_24_to_Movements and Selection for Text Blocks\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 554\n",
      "   Preview: ------------------------------------  When you are re-organizing the structure of a document, it can be useful to change the level of a number of section titles. The same key binding can be used to do...\n",
      "6. sections_content_block_36_to_content_block_42\n",
      "   Type: grouped_sections | Level: 1 | Lines: 81 | Tokens: 536\n",
      "   Preview: ---------------------------------------  The understanding of reStructuredText_ of ``rst-mode`` is used to set all the variables influencing Emacs' understanding of paragraphs. Thus all operations on...\n",
      "7. sections_content_block_43_to_Straightening Existing Bullet List Hierarchies\n",
      "   Type: grouped_sections | Level: 1 | Lines: 108 | Tokens: 575\n",
      "   Preview: .. note:: Since Emacs_ V24.4 ``electric-indent-mode`` is globally on.           This breaks indentation in ``rst-mode`` and renders           ``rst-mode`` mostly useless. This is fixed in V1.4.1 of...\n",
      "8. sections_content_block_51_to_Converting Documents from Emacs\n",
      "   Type: grouped_sections | Level: 1 | Lines: 78 | Tokens: 433\n",
      "   Preview: ----------------------------------------------  If you invoke ``rst-straighten-bullets-region`` (``C-c C-l C-s``), the existing bullets in the active region will be replaced to reflect their respectiv...\n",
      "9. sections_content_block_61_to_Navigating Using the Table of Contents\n",
      "   Type: grouped_sections | Level: 1 | Lines: 74 | Tokens: 593\n",
      "   Preview: ===============================  ``rst-mode`` provides a number of functions for running documents being edited through the docutils tools. The key bindings for these commands start with ``C-c C-c``....\n",
      "10. sections_content_block_71_to_content_block_73\n",
      "   Type: grouped_sections | Level: 1 | Lines: 56 | Tokens: 482\n",
      "   Preview: --------------------------------------  When you are editing long documents, it can be a bit difficult to orient yourself in the structure of your text. To that effect, a function is provided that pre...\n",
      "11. sections_content_block_74_to_Customizing Section Title Formatting\n",
      "   Type: grouped_sections | Level: 1 | Lines: 68 | Tokens: 479\n",
      "   Preview: .. contents::   ..       1  Introduction       2  Debugging Solution Patterns         2.1  Recognize That a Bug Exists         2.2  Subdivide and Isolate         2.3  Identify and Verify Assumptions...\n",
      "12. sections_content_block_82_to_Customizing Faces\n",
      "   Type: grouped_sections | Level: 1 | Lines: 53 | Tokens: 476\n",
      "   Preview: ------------------------------------  For a couple of things the reStructuredText_ syntax offers a choice of options on how to do things exactly. Some of these choices influence the operation of ``rst...\n",
      "13. sections_content_block_86_to_Editing Tables: Emacs table mode\n",
      "   Type: grouped_sections | Level: 1 | Lines: 84 | Tokens: 582\n",
      "   Preview: -----------------  The faces used for font-locking can be defined in the ``rst-faces`` customization group. The customization options ending in ``-face`` are only there for backward compatibility so p...\n",
      "14. sections_content_block_98_to_Credits\n",
      "   Type: grouped_sections | Level: 1 | Lines: 66 | Tokens: 599\n",
      "   Preview: --------------------------------  You may want to check out `Emacs table mode`_ to create an edit tables, it allows creating ASCII tables compatible with reStructuredText_.  .. _Emacs table mode: http...\n",
      "15. content_block_102\n",
      "   Type: content_block | Level: 1 | Lines: 28 | Tokens: 261\n",
      "   Preview: =======  Part of the original code of ``rst.el`` has been written by Martin Blais and David Goodger and Wei-Wei Guo. The font-locking came from Stefan Merten.  Most of the code has been modified, enha...\n",
      "\n",
      "🔍 Processing: mailing-lists.rst\n",
      "🌳 Processing mailing-lists.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 12 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/user/mailing-lists.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for mailing-lists.rst\n",
      "\n",
      "🔍 Processing: latex.rst\n",
      "🌳 Processing latex.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 181 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/user/latex.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for latex.rst\n",
      "\n",
      "🔍 Processing: odt.rst\n",
      "🌳 Processing odt.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 93 chunks\n",
      "Created 19 grouped chunks\n",
      "Final result: 19 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 19 chunks for odt.rst\n",
      "  ✅ Saved: odt.rst_chunk_001_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_002_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_003_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_004_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_005_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_006_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_007_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_008_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_009_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_010_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_011_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_012_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_013_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_014_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_015_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_016_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_017_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_018_y4mvqc.md\n",
      "  ✅ Saved: odt.rst_chunk_019_y4mvqc.md\n",
      "\n",
      "--- RST Chunk Summary for odt.rst ---\n",
      "1. sections_content_block_1_to_Command line options\n",
      "   Type: grouped_sections | Level: 1 | Lines: 100 | Tokens: 531\n",
      "   Preview: .. include:: ../header.rst  =======================  ODT Writer for Docutils  =======================  :Author: Dave Kuhlman :Contact: docutils-develop@lists.sourceforge.net :Revision: $Revision$ :Dat...\n",
      "2. content_block_15\n",
      "   Type: content_block | Level: 1 | Lines: 63 | Tokens: 620\n",
      "   Preview: --------------------  The following command line options are specific to ``odtwriter``:  --stylesheet=<URL>      Specify a stylesheet URL, used verbatim.                         Default: writers/odf_o...\n",
      "3. sections_Styles and Classes_to_Styles used by odtwriter\n",
      "   Type: grouped_sections | Level: 1 | Lines: 54 | Tokens: 574\n",
      "   Preview: Styles and Classes  ==================  ``odtwriter`` uses a number of styles that are defined in ``styles.xml`` in the default ``styles.odt``.  This section describes those styles.  Note that with th...\n",
      "4. sections_content_block_19_to_Paragraph styles\n",
      "   Type: grouped_sections | Level: 1 | Lines: 30 | Tokens: 253\n",
      "   Preview: ------------------------  This section describes the styles used by ``odtwriter``.  Note that we do not describe the \"look\" of these styles.  That can be easily changed by using ``oowriter`` to edit t...\n",
      "5. content_block_21\n",
      "   Type: content_block | Level: 1 | Lines: 87 | Tokens: 608\n",
      "   Preview: ~~~~~~~~~~~~~~~~  rststyle-attribution     The style for attributions, for example, the attribution in a     ``.. epigraph::`` directive.  Derived from     ``rststyle-blockquote``.  rststyle-blockinde...\n",
      "6. sections_Character styles_to_Admonition styles\n",
      "   Type: grouped_sections | Level: 1 | Lines: 98 | Tokens: 563\n",
      "   Preview: Character styles  ~~~~~~~~~~~~~~~~  rststyle-emphasis     Emphasis.  Normally rendered as italics.  rststyle-inlineliteral     An inline literal.  rststyle-strong     Strong emphasis.  Normally render...\n",
      "7. sections_content_block_27_to_Table styles\n",
      "   Type: grouped_sections | Level: 1 | Lines: 89 | Tokens: 537\n",
      "   Preview: ~~~~~~~~~~~~~~~~~  rststyle-admon-attention-hdr     The style for the attention admonition header/title.  rststyle-admon-attention-body     The style for the attention admonition body/paragraph.  rsts...\n",
      "8. content_block_32\n",
      "   Type: content_block | Level: 1 | Lines: 41 | Tokens: 407\n",
      "   Preview: ~~~~~~~~~~~~  A table style is generated by ``oowriter`` for each table that you create.  Therefore, ``odtwriter`` attempts to do something similar. These styles are created in the ``content.xml`` doc...\n",
      "9. sections_content_block_33_to_Heading and title styles\n",
      "   Type: grouped_sections | Level: 1 | Lines: 81 | Tokens: 564\n",
      "   Preview: .. class:: rststyle-table-vegetabledata  The default table style will be applied to all tables for which you do not specify a style with the \"..  class::\" directive.  Customize the table prop...\n",
      "10. sections_content_block_39_to_Why custom style names\n",
      "   Type: grouped_sections | Level: 1 | Lines: 74 | Tokens: 412\n",
      "   Preview: ~~~~~~~~~~~~~~~~~~~~~~~~~  rststyle-heading{1|2|3|4|5}     The styles for headings (section titles and sub-titles).  Five     levels of sub-headings are provided: rststyle-heading1 through     rststyl...\n",
      "11. sections_content_block_49_to_How to use custom style names\n",
      "   Type: grouped_sections | Level: 1 | Lines: 30 | Tokens: 289\n",
      "   Preview: ~~~~~~~~~~~~~~~~~~~~~~  Here are a few reasons and ideas:  - Suppose that your organization has a standard set of styles in   OOo ``oowriter`` and suppose that the use of these styles is   required. Y...\n",
      "12. sections_content_block_51_to_Roles\n",
      "   Type: grouped_sections | Level: 1 | Lines: 65 | Tokens: 481\n",
      "   Preview: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  In order to define custom style names and to generate documents that contain them, do the following:   1. Create a configuration file containing a \"Formats\" section.  Th...\n",
      "13. sections_content_block_57_to_Syntax highlighting\n",
      "   Type: grouped_sections | Level: 1 | Lines: 73 | Tokens: 550\n",
      "   Preview: -------  You can use a Docutils custom interpreted text role to attach a character style to an inline area of text.  This capability also enables you to attach a new character style (with a new name)...\n",
      "14. sections_content_block_65_to_content_block_67\n",
      "   Type: grouped_sections | Level: 1 | Lines: 70 | Tokens: 439\n",
      "   Preview: -------------------  ``odtwriter`` can add syntax highlighting to code in code blocks.  In order to activate this, do all of the following:  1. Install `Pygments`_ and ...  2. Use the command line opt...\n",
      "15. sections_content_block_68_to_Images and figures\n",
      "   Type: grouped_sections | Level: 1 | Lines: 82 | Tokens: 488\n",
      "   Preview: .. container:: style-1 style-2 style-3          a block of text  - Only ``style-1`` is used; ``style-2`` and ``style-3`` are ignored.  - ``rststyle-style-1`` must be defined.  It should be an exis...\n",
      "16. sections_content_block_76_to_content_block_81\n",
      "   Type: grouped_sections | Level: 1 | Lines: 66 | Tokens: 579\n",
      "   Preview: ------------------  The ODT Writer only supports fixed `length units`_ (\"cm\", \"mm\", \"in\", \"pc\", \"pt\", \"px) for the size attributes \"width\", and \"height\". The fallback unit (used for attribute values w...\n",
      "17. sections_content_block_82_to_Custom header/footers: inserting page numbers, date, time, etc\n",
      "   Type: grouped_sections | Level: 1 | Lines: 68 | Tokens: 513\n",
      "   Preview: .. meta::        :keywords: reStructuredText, docutils, formatting        :description lang=en: A reST document, contains formatted            text in a formatted style.        :custom_var: Value...\n",
      "18. sections_content_block_89_to_Credits\n",
      "   Type: grouped_sections | Level: 1 | Lines: 74 | Tokens: 407\n",
      "   Preview: ----------------------------------------------------------------  You can specify custom headers and footers for your document from the command line.  These headers and footers can be used to insert f...\n",
      "19. content_block_93\n",
      "   Type: content_block | Level: 1 | Lines: 31 | Tokens: 230\n",
      "   Preview: =======  Stefan Merten designed and implemented the custom style names capability.  Thank you, Stefan.  Michael Schutte supports the Debian GNU/Linux distribution of ``odtwriter``.  Thank you, Michael...\n",
      "\n",
      "🔍 Processing: links.rst\n",
      "🌳 Processing links.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 37 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/user/links.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for links.rst\n",
      "\n",
      "🔍 Processing: runtime-settings.rst\n",
      "🌳 Processing runtime-settings.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 19 chunks\n",
      "Created 3 grouped chunks\n",
      "Final result: 3 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 3 chunks for runtime-settings.rst\n",
      "  ✅ Saved: runtime-settings.rst_chunk_001_yrkzwi.md\n",
      "  ✅ Saved: runtime-settings.rst_chunk_002_yrkzwi.md\n",
      "  ✅ Saved: runtime-settings.rst_chunk_003_yrkzwi.md\n",
      "\n",
      "--- RST Chunk Summary for runtime-settings.rst ---\n",
      "1. sections_content_block_1_to_attributes\n",
      "   Type: grouped_sections | Level: 1 | Lines: 87 | Tokens: 422\n",
      "   Preview: .. include:: ../header.rst  ===========================   Docutils Runtime Settings  ===========================  :Author: David Goodger, Günter Milde :Contact: docutils-develop@lists.sourceforge.net...\n",
      "2. sections_content_block_13_to_settings_spec\n",
      "   Type: grouped_sections | Level: 1 | Lines: 79 | Tokens: 427\n",
      "   Preview: ----------  .. _SettingsSpec.settings_spec:  `settings_spec`    a sequence of     1. option group title (string or None)     2. description (string or None)     3. option tuples with        a) help te...\n",
      "3. content_block_19\n",
      "   Type: content_block | Level: 1 | Lines: 33 | Tokens: 288\n",
      "   Preview: -------------  The name ``settings_spec`` may refer to  a) an instance of the SettingsSpec_ class, b) the data structure `SettingsSpec.settings_spec`_ which is used to    store settings details, or c)...\n",
      "\n",
      "🔍 Processing: publisher.rst\n",
      "🌳 Processing publisher.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 54 chunks\n",
      "Created 10 grouped chunks\n",
      "Final result: 10 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 10 chunks for publisher.rst\n",
      "  ✅ Saved: publisher.rst_chunk_001_47nxt0.md\n",
      "  ✅ Saved: publisher.rst_chunk_002_47nxt0.md\n",
      "  ✅ Saved: publisher.rst_chunk_003_47nxt0.md\n",
      "  ✅ Saved: publisher.rst_chunk_004_47nxt0.md\n",
      "  ✅ Saved: publisher.rst_chunk_005_47nxt0.md\n",
      "  ✅ Saved: publisher.rst_chunk_006_47nxt0.md\n",
      "  ✅ Saved: publisher.rst_chunk_007_47nxt0.md\n",
      "  ✅ Saved: publisher.rst_chunk_008_47nxt0.md\n",
      "  ✅ Saved: publisher.rst_chunk_009_47nxt0.md\n",
      "  ✅ Saved: publisher.rst_chunk_010_47nxt0.md\n",
      "\n",
      "--- RST Chunk Summary for publisher.rst ---\n",
      "1. sections_content_block_1_to_content_block_11\n",
      "   Type: grouped_sections | Level: 1 | Lines: 86 | Tokens: 557\n",
      "   Preview: .. include:: ../header.rst  ========================   The Docutils Publisher  ========================  :Author: David Goodger :Contact: docutils-develop@lists.sourceforge.net :Date: $Date$ :Revision...\n",
      "2. sections_content_block_12_to_publish_doctree()\n",
      "   Type: grouped_sections | Level: 1 | Lines: 76 | Tokens: 601\n",
      "   Preview: .. parsed-literal::      def publish_file(source__\\ =None, source_path__\\ =None,                      destination_\\ =None, destination_path__\\ =None,                      reader_\\ =None, reader_name=N...\n",
      "3. sections_content_block_18_to_content_block_25\n",
      "   Type: grouped_sections | Level: 1 | Lines: 83 | Tokens: 578\n",
      "   Preview: -----------------  For programmatic use with `string input`_. Parse input into a `Docutils Document Tree`_ data structure. Return a `nodes.document`_ instance.   .. parsed-literal::      publish_doctr...\n",
      "4. sections_content_block_26_to_HTML4 Writer\n",
      "   Type: grouped_sections | Level: 1 | Lines: 46 | Tokens: 264\n",
      "   Preview: .. contents::    :local:  Example:   post-process the output document with a custom function   ``post_process()`` before encoding with user-customizable   encoding and errors::         def publish_byt...\n",
      "5. content_block_32\n",
      "   Type: content_block | Level: 1 | Lines: 124 | Tokens: 894\n",
      "   Preview: ^^^^^^^^^^^^  _`\"body\"`     Equivalent to `\"fragment\"`_.  It is *not* equivalent to `\"html_body\"`_.  _`\"body_prefix\"`     Contains ::          </head>         <body>         <div class=\"document\" ...>...\n",
      "6. sections_PEP/HTML Writer_to_Parts Provided by the LaTeX Writers\n",
      "   Type: grouped_sections | Level: 1 | Lines: 33 | Tokens: 175\n",
      "   Preview: PEP/HTML Writer  ^^^^^^^^^^^^^^^  The PEP/HTML writer provides the same parts as the `HTML4 writer`_, plus the following:  _`\"pepnum\"`     The PEP number (extracted from the `header preamble`__)....\n",
      "7. sections_content_block_40_to_Entry Point Functions\n",
      "   Type: grouped_sections | Level: 1 | Lines: 84 | Tokens: 580\n",
      "   Preview: ```````````````````````````````````  All parts returned by the LaTeX writers (\"latex\" writer and \"xelatex\" writer) are of data type `str`.  _`\"abstract\"`     Formatted content of the \"abstract\" `bibli...\n",
      "8. sections_content_block_44_to_String I/O\n",
      "   Type: grouped_sections | Level: 1 | Lines: 66 | Tokens: 414\n",
      "   Preview: ---------------------  The functions rst2html(), rst2html4(), rst2html5(), rst2latex(), rst2man(), rst2odt(), rst2pseudoxml(), rst2s5(), rst2xetex(), and rst2xml() are wrappers around `publish_cmdline...\n",
      "9. sections_content_block_50_to_Settings Specification\n",
      "   Type: grouped_sections | Level: 1 | Lines: 66 | Tokens: 450\n",
      "   Preview: ----------  The functions `publish_string()`_, `publish_doctree()`_, `publish_from_doctree()`_, and `publish_parts()`_ use the string I/O interface provided by the `docutils.io.StringInput` and `docut...\n",
      "10. content_block_54\n",
      "   Type: content_block | Level: 1 | Lines: 79 | Tokens: 605\n",
      "   Preview: ----------------------  See also `Runtime Settings`_.  _`settings` : docutils.frontend.Values   Runtime settings object.   If `settings` is passed, it's assumed to be the end result of   `runtime sett...\n",
      "\n",
      "🔍 Processing: transforms.rst\n",
      "🌳 Processing transforms.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 64 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/api/transforms.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for transforms.rst\n",
      "\n",
      "🔍 Processing: index.rst\n",
      "🌳 Processing index.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 5 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for index.rst\n",
      "  ✅ Saved: index.rst_chunk_001_9qu1to.md\n",
      "\n",
      "--- RST Chunk Summary for index.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 30 | Tokens: 159\n",
      "   Preview: .. include:: ../header.rst  ================================   Docutils Enhancement Proposals  ================================  A framework for proposing major new features, collecting community inpu...\n",
      "\n",
      "🔍 Processing: ep-template.rst\n",
      "🌳 Processing ep-template.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 26 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for ep-template.rst\n",
      "  ✅ Saved: ep-template.rst_chunk_001_a06ph3.md\n",
      "\n",
      "--- RST Chunk Summary for ep-template.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 123 | Tokens: 461\n",
      "   Preview: .. include:: header.rst  ==========================  <EP number> — <title>  ==========================  :Author:  <name and optional e-mail; use ``:Authors:`` for a list> :Discussions-To: <current dis...\n",
      "\n",
      "🔍 Processing: header.rst\n",
      "🌳 Processing header.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 2 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for header.rst\n",
      "  ✅ Saved: header.rst_chunk_001_37wxgn.md\n",
      "\n",
      "--- RST Chunk Summary for header.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 14 | Tokens: 70\n",
      "   Preview: .. Minimal menu bar for inclusion in enhancement proposal sources    in ``docutils/docs/eps/``.     Attention: this is not a standalone document.   .. header::    Docutils__ | Overview__ | `Enhancemen...\n",
      "\n",
      "🔍 Processing: ep-001.rst\n",
      "🌳 Processing ep-001.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 39 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/eps/ep-001.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for ep-001.rst\n",
      "\n",
      "🔍 Processing: ep-010.rst\n",
      "🌳 Processing ep-010.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 25 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/eps/ep-010.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for ep-010.rst\n",
      "\n",
      "🔍 Processing: policies.rst\n",
      "🌳 Processing policies.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 53 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/dev/policies.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for policies.rst\n",
      "\n",
      "🔍 Processing: website.rst\n",
      "🌳 Processing website.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 14 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for website.rst\n",
      "  ✅ Saved: website.rst_chunk_001_cocldj.md\n",
      "\n",
      "--- RST Chunk Summary for website.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 106 | Tokens: 608\n",
      "   Preview: .. include:: ../header.rst  ===================   Docutils Web Site  ===================  :Author: David Goodger; open to all Docutils developers :Contact: docutils-develop@lists.sourceforge.net :Date...\n",
      "\n",
      "🔍 Processing: distributing.rst\n",
      "🌳 Processing distributing.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 20 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/dev/distributing.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for distributing.rst\n",
      "\n",
      "🔍 Processing: runtime-settings-processing.rst\n",
      "🌳 Processing runtime-settings-processing.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 11 chunks\n",
      "Created 3 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_1_to_content_block_9 (1599 tokens)\n",
      "Final result: 4 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 4 chunks for runtime-settings-processing.rst\n",
      "  ✅ Saved: runtime-settings-processing.rst_chunk_001_rlj6ml.md\n",
      "  ✅ Saved: runtime-settings-processing.rst_chunk_002_rlj6ml.md\n",
      "  ✅ Saved: runtime-settings-processing.rst_chunk_003_rlj6ml.md\n",
      "  ✅ Saved: runtime-settings-processing.rst_chunk_004_rlj6ml.md\n",
      "\n",
      "--- RST Chunk Summary for runtime-settings-processing.rst ---\n",
      "  1. sections_content_block_1_to_content_block_9_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 161 | Tokens: 1050\n",
      "     Preview: .. include:: ../header.rst  =============================   Runtime Settings Processing  =============================  :Author: David Goodger, Günter Milde :Contact: docutils-develop@lists.sourceforg...\n",
      "  2. sections_content_block_1_to_content_block_9_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 63 | Tokens: 548\n",
      "     Preview: the `defaults` argument passed to ``OptionParser(…)`` in step 5.     This means that the `settings_overrides` argument of the    `convenience functions`_ has priority over all    ``SettingsSpec.set...\n",
      "3. Runtime settings processing for other applications\n",
      "   Type: section | Level: 1 | Lines: 1 | Tokens: 6\n",
      "   Preview: Runtime settings processing for other applications\n",
      "4. content_block_11\n",
      "   Type: content_block | Level: 1 | Lines: 85 | Tokens: 712\n",
      "   Preview: ==================================================  The `convenience functions`_ , ``core.publish_file()``, ``core.publish_string()``, or ``core.publish_parts()`` do not parse the command line for set...\n",
      "\n",
      "🔍 Processing: pysource.rst\n",
      "🌳 Processing pysource.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 10 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/dev/pysource.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for pysource.rst\n",
      "\n",
      "🔍 Processing: testing.rst\n",
      "🌳 Processing testing.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 23 chunks\n",
      "Created 5 grouped chunks\n",
      "Final result: 5 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 5 chunks for testing.rst\n",
      "  ✅ Saved: testing.rst_chunk_001_50672k.md\n",
      "  ✅ Saved: testing.rst_chunk_002_50672k.md\n",
      "  ✅ Saved: testing.rst_chunk_003_50672k.md\n",
      "  ✅ Saved: testing.rst_chunk_004_50672k.md\n",
      "  ✅ Saved: testing.rst_chunk_005_50672k.md\n",
      "\n",
      "--- RST Chunk Summary for testing.rst ---\n",
      "1. sections_content_block_1_to_Testing across multiple Python versions\n",
      "   Type: grouped_sections | Level: 1 | Lines: 64 | Tokens: 436\n",
      "   Preview: .. include:: ../header.rst  ===================   Docutils_ Testing  ===================  :Authors: Lea Wiemann <LeWiemann@gmail.com>;           David Goodger <goodger@python.org>;           Docutils...\n",
      "2. sections_content_block_9_to_Writing New Tests\n",
      "   Type: grouped_sections | Level: 1 | Lines: 78 | Tokens: 567\n",
      "   Preview: ---------------------------------------  A Docutils release has a commitment to support a minimum Python version and beyond (see dependencies__ in README.rst). Before a release is cut, tests must pass...\n",
      "3. sections_content_block_14_to_The Testing Process\n",
      "   Type: grouped_sections | Level: 1 | Lines: 101 | Tokens: 577\n",
      "   Preview: -----------------  When writing new tests, it very often helps to see how a similar test is implemented.  For example, the files in the ``test_parsers/test_rst/`` directory all look very similar.  So...\n",
      "4. sections_content_block_21_to_Creating New Tests\n",
      "   Type: grouped_sections | Level: 1 | Lines: 39 | Tokens: 366\n",
      "   Preview: -------------------  When running ``test_functional.py``, all config files in ``functional/tests/`` are processed.  (Config files whose names begin with an underscore are ignored.)  The current workin...\n",
      "5. content_block_23\n",
      "   Type: content_block | Level: 1 | Lines: 26 | Tokens: 250\n",
      "   Preview: ------------------  In order to create a new test, put the input test file into ``functional/input/``.  Then create a config file in ``functional/tests/`` which sets at least input and output file nam...\n",
      "\n",
      "🔍 Processing: todo.rst\n",
      "🌳 Processing todo.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 136 chunks\n",
      "Created 36 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_8_to_content_block_12 (2091 tokens)\n",
      "  📦 Sub-chunking sections_content_block_17_to_content_block_20 (1472 tokens)\n",
      "  📦 Sub-chunking sections_reStructuredText Parser_to_content_block_42 (2855 tokens)\n",
      "  📦 Sub-chunking sections_content_block_94_to_content_block_96 (1727 tokens)\n",
      "Final result: 42 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 42 chunks for todo.rst\n",
      "  ✅ Saved: todo.rst_chunk_001_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_002_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_003_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_004_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_005_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_006_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_007_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_008_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_009_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_010_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_011_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_012_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_013_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_014_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_015_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_016_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_017_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_018_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_019_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_020_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_021_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_022_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_023_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_024_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_025_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_026_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_027_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_028_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_029_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_030_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_031_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_032_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_033_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_034_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_035_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_036_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_037_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_038_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_039_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_040_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_041_lh5rxl.md\n",
      "  ✅ Saved: todo.rst_chunk_042_lh5rxl.md\n",
      "\n",
      "--- RST Chunk Summary for todo.rst ---\n",
      "1. sections_content_block_1_to_Repository\n",
      "   Type: grouped_sections | Level: 1 | Lines: 69 | Tokens: 397\n",
      "   Preview: .. include:: ../header.rst  ======================   Docutils_ To Do List  ======================  :Author: David Goodger (with input from many); open to all Docutils          developers :Contact: doc...\n",
      "  2. sections_content_block_8_to_content_block_12_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 120 | Tokens: 1045\n",
      "     Preview: ==========  Move to a Git repository.  See `feature requests #58`__ (with pointers to Sphinx issues and discussion).  __ https://sourceforge.net/p/docutils/feature-requests/58/  * From a `post by Davi...\n",
      "  3. sections_content_block_8_to_content_block_12_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 106 | Tokens: 1033\n",
      "     Preview: * Move some general-interest sandboxes out of individuals'   directories, into subprojects?  * Add option for file (and URL) access restriction to make Docutils   usable in Wikis and similar applicati...\n",
      "  4. sections_content_block_8_to_content_block_12_part_3\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 4 | Tokens: 13\n",
      "     Preview: Aahz's idea:        First the ToC::\n",
      "5. sections_content_block_13_to_content_block_15\n",
      "   Type: grouped_sections | Level: 1 | Lines: 38 | Tokens: 241\n",
      "   Preview: .. ToC-list::               Introduction.rst               Objects.rst               Data.rst               Control.rst        Then a sample use::             .. include:: ToC.rst...\n",
      "6. content_block_16\n",
      "   Type: content_block | Level: 1 | Lines: 73 | Tokens: 715\n",
      "   Preview: .. include:: manifest.rst        As I said earlier in chapter :chapter:`objects`, the       reference count gets increased every time a binding is made.  * Add support for _`multiple output file...\n",
      "  7. sections_content_block_17_to_content_block_20_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 118 | Tokens: 1031\n",
      "     Preview: .. |version| include:: version.rst      This document describes version |version| of ...    (cf. Grzegorz Adam Hankiewicz's post from 2014-10-01 in docutils-devel)  * Add an ``:optional: <replacem...\n",
      "  8. sections_content_block_17_to_content_block_20_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 43 | Tokens: 441\n",
      "     Preview: I may have used them instead of rolling my own.)    .. _traits: http://code.enthought.com/traits/   .. _SciPy: http://www.scipy.org/  * Add support for _`plugins`.  * _`Config directories`: Currentl...\n",
      "9. sections_object numbering and object references_to_content_block_24\n",
      "   Type: grouped_sections | Level: 1 | Lines: 70 | Tokens: 543\n",
      "   Preview: object numbering and object references  --------------------------------------  For equations, tables & figures.  These would be the equivalent of DocBook's \"formal\" elements.  In LaTeX, automatic cou...\n",
      "10. sections_content_block_25_to_Python Source Reader\n",
      "   Type: grouped_sections | Level: 1 | Lines: 106 | Tokens: 583\n",
      "   Preview: .. fignum::            :prefix-ref: \"Figure \"            :prefix-caption: \"Fig. \"            :suffix-caption: :      The position of the role (prefix or suffix) could also be utilized    .. _O...\n",
      "11. content_block_37\n",
      "   Type: content_block | Level: 1 | Lines: 105 | Tokens: 770\n",
      "   Preview: ====================  General:  * Analyze Tony Ibbs' PySource code.  * Analyze Doug Hellmann's HappyDoc project.  * Investigate how POD handles literate programming.  * Take the best ideas and integra...\n",
      "  12. sections_reStructuredText Parser_to_content_block_42_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 115 | Tokens: 1028\n",
      "     Preview: reStructuredText Parser  =======================  Also see the `... Or Not To Do?`__ list.  __ rst/alternatives.html#or-not-to-do    Misc  ----  * A list problem::        * foo             * bar...\n",
      "  13. sections_reStructuredText Parser_to_content_block_42_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 129 | Tokens: 1042\n",
      "     Preview: disabled at run-time.  Subclassing is probably not enough because it   makes it difficult to apply multiple extensions.  * Generalize the \"doctest block\" construct (which is overly   Python-centric)...\n",
      "  14. sections_reStructuredText Parser_to_content_block_42_part_3\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 86 | Tokens: 784\n",
      "     Preview: The initial colon (\":\") can be thought of as a type of bullet    We could even have segment titles::        :: title  : title   : title       : segment : segment : segment       : segment : segment...\n",
      "15. sections_Adaptable file extensions_to_Proposals\n",
      "   Type: grouped_sections | Level: 1 | Lines: 58 | Tokens: 419\n",
      "   Preview: Adaptable file extensions  -------------------------   Questions  `````````  Should Docutils support adaptable file extensions in hyperlinks?    In the rST source, sister documents are \".rst\" files. I...\n",
      "16. content_block_48\n",
      "   Type: content_block | Level: 1 | Lines: 39 | Tokens: 400\n",
      "   Preview: `````````  How about using \".*\" to indicate \"choose the most appropriate filename extension\"?  For example::      .. _Another Document: another.*  * My point about using ``.*`` is that any other mecha...\n",
      "17. sections_content_block_49_to_Math Markup\n",
      "   Type: grouped_sections | Level: 1 | Lines: 63 | Tokens: 495\n",
      "   Preview: .. role:: link(rewrite)        :transform: .rst|.html    and then to use it::      for more information see :link:`README.rst`    it would be useful if it supported an additional option   ``:forma...\n",
      "18. sections_content_block_51_to_alternative input formats\n",
      "   Type: grouped_sections | Level: 1 | Lines: 32 | Tokens: 251\n",
      "   Preview: -----------  * Use a \"Transform\" for math format conversions as extensively discussed   in the `math directive issues`__ thread in May 2008?    __ http://osdir.com/ml/text.docutils.devel/2008-05/threa...\n",
      "19. content_block_53\n",
      "   Type: content_block | Level: 1 | Lines: 66 | Tokens: 657\n",
      "   Preview: `````````````````````````  Use a directive option to specify an alternative input format, e.g. (but not limited to):  MathML_   Not for hand-written code but maybe useful when pasted in (or included...\n",
      "20. sections_LaTeX output_to_OpenOffice output\n",
      "   Type: grouped_sections | Level: 1 | Lines: 71 | Tokens: 525\n",
      "   Preview: LaTeX output  ````````````  Which equation environments should be supported by the math directive?  * one line:    + numbered: `equation`   + unnumbered: `equation*`  * multiline (test for ``\\\\`` outs...\n",
      "21. sections_content_block_59_to_Directives\n",
      "   Type: grouped_sections | Level: 1 | Lines: 22 | Tokens: 184\n",
      "   Preview: `````````````````  * The `OpenDocument standard`_ version 1.1 says:      Mathematical content is represented by MathML 2.0    However, putting MathML into an ODP file seems tricky as these   (maybe ou...\n",
      "22. content_block_61\n",
      "   Type: content_block | Level: 1 | Lines: 92 | Tokens: 900\n",
      "   Preview: ----------  Directives below are often referred to as \"module.directive\", the directive function.  The \"module.\" is not part of the directive name when used in a document.  * Allow for field lists in...\n",
      "23. content_block_62\n",
      "   Type: content_block | Level: 1 | Lines: 12 | Tokens: 106\n",
      "   Preview: .. include::              :url: https://www.example.org/inclusion.rst      - Strip blank lines from begin and end of a literal included file or       file section. This would correspond to t...\n",
      "24. content_block_63\n",
      "   Type: content_block | Level: 1 | Lines: 68 | Tokens: 598\n",
      "   Preview: .. raw:: html            :destination: head             <link ...>      It needs thought & discussion though, to come up with a consistent     set of destination labels and consistent behavior...\n",
      "25. sections_content_block_64_to_content_block_83\n",
      "   Type: grouped_sections | Level: 1 | Lines: 98 | Tokens: 511\n",
      "   Preview: .. directive:: incr              .. class:: incremental           .. incr::          \"``.. incr::``\" above is equivalent to \"``.. class:: incremental``\".      Another example::           .. di...\n",
      "26. sections_content_block_84_to_Interpreted Text\n",
      "   Type: grouped_sections | Level: 1 | Lines: 64 | Tokens: 487\n",
      "   Preview: .. eqn::             .EQ            delim %%            .EN            %sum from i=o to inf c sup i~=~lim from {m -> inf}            sum from i=0 to m sup i%            .EQ            delim of...\n",
      "27. sections_content_block_87_to_content_block_88\n",
      "   Type: grouped_sections | Level: 1 | Lines: 51 | Tokens: 447\n",
      "   Preview: ----------------  Interpreted text is entirely a reStructuredText markup construct, a way to get around built-in limitations of the medium.  Some roles are intended to introduce new doctree elements,...\n",
      "28. content_block_89\n",
      "   Type: content_block | Level: 1 | Lines: 60 | Tokens: 462\n",
      "   Preview: .. role:: red(raw-formatting)            :prefix:                :html: <font color=\"red\">                :latex: {\\color{red}            :suffix:                :html: </font>...\n",
      "29. sections_content_block_90_to_Doctree pruning\n",
      "   Type: grouped_sections | Level: 1 | Lines: 67 | Tokens: 498\n",
      "   Preview: .. acronyms::                reST                   reStructuredText               DPS                   Docstring Processing System         Would this list remain in the document as a glos...\n",
      "  30. sections_content_block_94_to_content_block_96_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 115 | Tokens: 1023\n",
      "     Preview: ---------------  [DG 2017-01-02: These are not definitive to-dos, just one developer's opinion. Added 2009-10-13 by Günter Milde, in r6178.] [Updated by GM 2017-02-04]  The number of doctree nodes can...\n",
      "  31. sections_content_block_94_to_content_block_96_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 83 | Tokens: 703\n",
      "     Preview: unique; and/or   * duplicate footnote numbers that need to be renumbered.    Should this be done before or after reference-resolving transforms   are applied?  What about references from within on...\n",
      "32. HTML Writer\n",
      "   Type: section | Level: 1 | Lines: 1 | Tokens: 2\n",
      "   Preview: HTML Writer\n",
      "33. content_block_98\n",
      "   Type: content_block | Level: 1 | Lines: 69 | Tokens: 635\n",
      "   Preview: ===========  * Make the _`list compacting` logic more generic: For example, allow   for literal blocks or line blocks inside of compact list items.    This is not implementable as long as list compact...\n",
      "34. sections_PEP/HTML Writer_to_content_block_102\n",
      "   Type: grouped_sections | Level: 1 | Lines: 41 | Tokens: 309\n",
      "   Preview: PEP/HTML Writer  ===============  * Remove the generic style information (duplicated from html4css1.css)   from pep.css to avoid redundancy.    Set ``stylesheet-path`` to \"html4css.css,pep.css\" and th...\n",
      "35. sections_content_block_103_to_Bug fixes\n",
      "   Type: grouped_sections | Level: 1 | Lines: 71 | Tokens: 566\n",
      "   Preview: .. Note:: This item and the following items are partially      accomplished by the S5 1.2 code (currently in alpha), which has      not yet been integrated into Docutils.  * Speaker's notes -- how t...\n",
      "36. sections_content_block_109_to_Footnotes\n",
      "   Type: grouped_sections | Level: 1 | Lines: 64 | Tokens: 387\n",
      "   Preview: ---------  * Too deeply nested lists fail: generate a warning and provide   a workaround.    2017-02-09 this is fixed for enumeration in 0.13.1    for others, cf. sandbox/latex-variants/tests/rst-leve...\n",
      "37. content_block_116\n",
      "   Type: content_block | Level: 1 | Lines: 60 | Tokens: 592\n",
      "   Preview: `````````  + True footnotes with LaTeX auto-numbering (as option ``--latex-footnotes``)   (also for target-footnotes):   Write ``\\footnote{<footnote content>}`` at the place of the   ``<footnote_refer...\n",
      "38. sections_Other LaTeX constructs and packages instead of re-implementations_to_Tables\n",
      "   Type: grouped_sections | Level: 1 | Lines: 51 | Tokens: 310\n",
      "   Preview: Other LaTeX constructs and packages instead of re-implementations  `````````````````````````````````````````````````````````````````  * Check the generated source with package `nag`.  * enumitem_ (tex...\n",
      "39. sections_content_block_122_to_Image and figure directives\n",
      "   Type: grouped_sections | Level: 1 | Lines: 57 | Tokens: 511\n",
      "   Preview: ``````  * Improve/simplify logic to set the column width in the output.    + Assumed reST line length for table width setting configurable, or   + use `ltxtable` (a combination of `tabularx` (auto-wid...\n",
      "40. sections_content_block_124_to_problematic URLs\n",
      "   Type: grouped_sections | Level: 1 | Lines: 80 | Tokens: 549\n",
      "   Preview: ```````````````````````````  * compare the test case in:    + `<../../test/functional/input/data/standard.rst>`__   + `<../../test/functional/expected/standalone_rst_html4css1.html>`__   + `<../../tes...\n",
      "41. sections_content_block_132_to_Front-End Tools\n",
      "   Type: grouped_sections | Level: 1 | Lines: 72 | Tokens: 526\n",
      "   Preview: ````````````````  * ^^ LaTeX's special syntax for characters results in \"strange\" replacements   (both with \\href and \\url).    `file with ^^ <../strange^^name>`__:   `<../strange^^name>`__  * Unbalan...\n",
      "42. content_block_136\n",
      "   Type: content_block | Level: 1 | Lines: 31 | Tokens: 179\n",
      "   Preview: ===============  * Parameterize help text & defaults somehow?  Perhaps a callback?  Or   initialize ``settings_spec`` in ``__init__`` or ``init_options``?  * Disable common options that don't apply?...\n",
      "\n",
      "🔍 Processing: hacking.rst\n",
      "🌳 Processing hacking.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 25 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/dev/hacking.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for hacking.rst\n",
      "\n",
      "🔍 Processing: semantics.rst\n",
      "🌳 Processing semantics.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 7 chunks\n",
      "Created 3 grouped chunks\n",
      "Final result: 3 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 3 chunks for semantics.rst\n",
      "  ✅ Saved: semantics.rst_chunk_001_607jvb.md\n",
      "  ✅ Saved: semantics.rst_chunk_002_607jvb.md\n",
      "  ✅ Saved: semantics.rst_chunk_003_607jvb.md\n",
      "\n",
      "--- RST Chunk Summary for semantics.rst ---\n",
      "1. sections_content_block_1_to_PythonDoc\n",
      "   Type: grouped_sections | Level: 1 | Lines: 36 | Tokens: 328\n",
      "   Preview: .. include:: ../header.rst  =====================   Docstring Semantics  ===================== :Author: David Goodger :Contact: docutils-develop@lists.sourceforge.net :Revision: $Revision$ :Date: $Dat...\n",
      "2. sections_content_block_5_to_Other Ideas\n",
      "   Type: grouped_sections | Level: 1 | Lines: 51 | Tokens: 437\n",
      "   Preview: =========  (Not to be confused with Daniel Larsson's pythondoc_ project.)  A Python version of the JavaDoc_ semantics (not syntax).  A set of conventions which are understood by the Docutils.  What Ja...\n",
      "3. content_block_7\n",
      "   Type: content_block | Level: 1 | Lines: 40 | Tokens: 316\n",
      "   Preview: ===========  - Can we extract comments from parsed modules?  Could be handy for   documenting function/method parameters::        def method(self,                  source,        # path of input file...\n",
      "\n",
      "🔍 Processing: enthought-plan.rst\n",
      "🌳 Processing enthought-plan.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 31 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/dev/enthought-plan.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for enthought-plan.rst\n",
      "\n",
      "🔍 Processing: enthought-rfp.rst\n",
      "🌳 Processing enthought-rfp.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 19 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for enthought-rfp.rst\n",
      "  ✅ Saved: enthought-rfp.rst_chunk_001_g7zlux.md\n",
      "\n",
      "--- RST Chunk Summary for enthought-rfp.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 165 | Tokens: 950\n",
      "   Preview: ==================================   Enthought API Documentation Tool  ================================== -----------------------   Request for Proposals  -----------------------  :Author: Janet Swish...\n",
      "\n",
      "🔍 Processing: repository.rst\n",
      "🌳 Processing repository.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 24 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/dev/repository.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for repository.rst\n",
      "\n",
      "🔍 Processing: release.rst\n",
      "🌳 Processing release.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 5 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/dev/release.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for release.rst\n",
      "\n",
      "🔍 Processing: rst-directives.rst\n",
      "🌳 Processing rst-directives.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 20 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/howto/rst-directives.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for rst-directives.rst\n",
      "\n",
      "🔍 Processing: html-stylesheets.rst\n",
      "🌳 Processing html-stylesheets.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 3 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for html-stylesheets.rst\n",
      "  ✅ Saved: html-stylesheets.rst_chunk_001_gecf0x.md\n",
      "\n",
      "--- RST Chunk Summary for html-stylesheets.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 100 | Tokens: 836\n",
      "   Preview: .. include:: ../header.rst  ==============================================   Writing HTML (CSS) Stylesheets for Docutils_  ==============================================  :Author: Lea Wiemann :Contact...\n",
      "\n",
      "🔍 Processing: i18n.rst\n",
      "🌳 Processing i18n.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 17 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/howto/i18n.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for i18n.rst\n",
      "\n",
      "🔍 Processing: rst-roles.rst\n",
      "🌳 Processing rst-roles.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 16 chunks\n",
      "Created 5 grouped chunks\n",
      "Final result: 5 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 5 chunks for rst-roles.rst\n",
      "  ✅ Saved: rst-roles.rst_chunk_001_zlkg0p.md\n",
      "  ✅ Saved: rst-roles.rst_chunk_002_zlkg0p.md\n",
      "  ✅ Saved: rst-roles.rst_chunk_003_zlkg0p.md\n",
      "  ✅ Saved: rst-roles.rst_chunk_004_zlkg0p.md\n",
      "  ✅ Saved: rst-roles.rst_chunk_005_zlkg0p.md\n",
      "\n",
      "--- RST Chunk Summary for rst-roles.rst ---\n",
      "1. sections_content_block_1_to_Define the Role Function\n",
      "   Type: grouped_sections | Level: 1 | Lines: 35 | Tokens: 212\n",
      "   Preview: .. include:: ../header.rst  ==================================================   Creating reStructuredText Interpreted Text Roles  ==================================================  :Authors: David G...\n",
      "2. sections_content_block_6_to_Specify Role Function Options and Content\n",
      "   Type: grouped_sections | Level: 1 | Lines: 53 | Tokens: 408\n",
      "   Preview: ========================  The role function creates and returns inline elements (nodes) and does any additional processing required.  Its signature is as follows::      def role_fn(name, rawtext, text...\n",
      "3. sections_content_block_8_to_Examples\n",
      "   Type: grouped_sections | Level: 1 | Lines: 71 | Tokens: 593\n",
      "   Preview: =========================================  Function attributes are for customization, and are interpreted by the `\"role\" directive`_.  If unspecified, role function attributes are assumed to have the...\n",
      "4. sections_content_block_12_to_RFC Reference Role\n",
      "   Type: grouped_sections | Level: 1 | Lines: 26 | Tokens: 150\n",
      "   Preview: ========  For the most direct and accurate information, \"Use the Source, Luke!\". All standard roles are documented in `reStructuredText Interpreted Text Roles`_, and the source code implementing them...\n",
      "5. content_block_16\n",
      "   Type: content_block | Level: 1 | Lines: 64 | Tokens: 619\n",
      "   Preview: ------------------  This role allows easy references to RFCs_ (Request For Comments documents) by automatically providing the base URL, http://www.faqs.org/rfcs/, and appending the RFC document itself...\n",
      "\n",
      "🔍 Processing: cmdline-tool.rst\n",
      "🌳 Processing cmdline-tool.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 4 chunks\n",
      "Created 1 grouped chunks\n",
      "Final result: 1 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 1 chunks for cmdline-tool.rst\n",
      "  ✅ Saved: cmdline-tool.rst_chunk_001_4yt2mt.md\n",
      "\n",
      "--- RST Chunk Summary for cmdline-tool.rst ---\n",
      "1. complete_document\n",
      "   Type: combined_document | Level: 0 | Lines: 81 | Tokens: 560\n",
      "   Preview: .. include:: ../header.rst  ===============================================   Inside A Docutils Command-Line Front-End Tool  ===============================================  :Author: David Goodger :Co...\n",
      "\n",
      "🔍 Processing: security.rst\n",
      "🌳 Processing security.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 26 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/howto/security.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for security.rst\n",
      "\n",
      "🔍 Processing: alternatives.rst\n",
      "🌳 Processing alternatives.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 148 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/dev/rst/alternatives.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for alternatives.rst\n",
      "\n",
      "🔍 Processing: problems.rst\n",
      "🌳 Processing problems.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 43 chunks\n",
      "Created 11 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_26_to_content_block_32 (1259 tokens)\n",
      "  📦 Sub-chunking sections_content_block_36_to_content_block_40 (1891 tokens)\n",
      "  📦 Sub-chunking sections_Hyperlinks_to_content_block_42 (1164 tokens)\n",
      "Final result: 14 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 14 chunks for problems.rst\n",
      "  ✅ Saved: problems.rst_chunk_001_hx8uwb.md\n",
      "  ✅ Saved: problems.rst_chunk_002_hx8uwb.md\n",
      "  ✅ Saved: problems.rst_chunk_003_hx8uwb.md\n",
      "  ✅ Saved: problems.rst_chunk_004_hx8uwb.md\n",
      "  ✅ Saved: problems.rst_chunk_005_hx8uwb.md\n",
      "  ✅ Saved: problems.rst_chunk_006_hx8uwb.md\n",
      "  ✅ Saved: problems.rst_chunk_007_hx8uwb.md\n",
      "  ✅ Saved: problems.rst_chunk_008_hx8uwb.md\n",
      "  ✅ Saved: problems.rst_chunk_009_hx8uwb.md\n",
      "  ✅ Saved: problems.rst_chunk_010_hx8uwb.md\n",
      "  ✅ Saved: problems.rst_chunk_011_hx8uwb.md\n",
      "  ✅ Saved: problems.rst_chunk_012_hx8uwb.md\n",
      "  ✅ Saved: problems.rst_chunk_013_hx8uwb.md\n",
      "  ✅ Saved: problems.rst_chunk_014_hx8uwb.md\n",
      "\n",
      "--- RST Chunk Summary for problems.rst ---\n",
      "1. sections_content_block_1_to_Section Structure via Indentation\n",
      "   Type: grouped_sections | Level: 1 | Lines: 65 | Tokens: 466\n",
      "   Preview: .. include:: ../../header2.rst  ==============================   Problems With StructuredText  ============================== :Author: David Goodger :Contact: docutils-develop@lists.sourceforge.net :R...\n",
      "2. sections_content_block_10_to_Character Escaping Mechanism\n",
      "   Type: grouped_sections | Level: 1 | Lines: 46 | Tokens: 446\n",
      "   Preview: =================================  Setext_ required that body text be indented by 2 spaces.  The original StructuredText_ and StructuredTextNG_ require that section structure be indicated through inde...\n",
      "3. content_block_12\n",
      "   Type: content_block | Level: 1 | Lines: 59 | Tokens: 602\n",
      "   Preview: ============================  No matter what characters are chosen for markup, some day someone will want to write documentation *about* that markup or using markup characters in a non-markup context....\n",
      "4. sections_Blank Lines in Lists_to_Enumerated List Markup\n",
      "   Type: grouped_sections | Level: 1 | Lines: 49 | Tokens: 356\n",
      "   Preview: Blank Lines in Lists  ====================  Oft-requested in Doc-SIG (the earliest reference is dated 1996-08-13) is the ability to write lists without requiring blank lines between items.  In docstri...\n",
      "5. sections_content_block_18_to_Definition List Markup\n",
      "   Type: grouped_sections | Level: 1 | Lines: 46 | Tokens: 433\n",
      "   Preview: ======================  StructuredText enumerated lists are allowed to begin with numbers and letters followed by a period or right-parenthesis, then whitespace. This has surprising consequences for w...\n",
      "6. sections_content_block_20_to_Tables\n",
      "   Type: grouped_sections | Level: 1 | Lines: 68 | Tokens: 539\n",
      "   Preview: ======================  StructuredText uses ' -- ' (whitespace, two hyphens, whitespace) on the first line of a paragraph to indicate a definition list item.  The ' -- ' serves to separate the term (o...\n",
      "  7. sections_content_block_26_to_content_block_32_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 92 | Tokens: 1035\n",
      "     Preview: ======  The table markup scheme in classic StructuredText was horrible.  Its omission from StructuredTextNG is welcome, and its markup will not be repeated here.  However, tables themselves are useful...\n",
      "  8. sections_content_block_26_to_content_block_32_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 18 | Tokens: 224\n",
      "     Preview: The table begins with a top border of equals signs with one or more    spaces at each column boundary (regardless of spans).  There must    be at least two columns in the table (to differentiate it...\n",
      "9. sections_Header 2 & 3 Span_to_Delimitation of Inline Markup\n",
      "   Type: grouped_sections | Level: 1 | Lines: 67 | Tokens: 506\n",
      "   Preview: Header 2 & 3 Span                   ------------------        Header 1  Header 2  Header 3        ========  ========  ========        Each      line is   a new row.        Each row  c...\n",
      "  10. sections_content_block_36_to_content_block_40_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 91 | Tokens: 1018\n",
      "     Preview: =============================  StructuredText specifies that inline markup must begin with whitespace, precluding such constructs as parenthesized or quoted emphatic text::      \"**What?**\" she cried....\n",
      "  11. sections_content_block_36_to_content_block_40_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 74 | Tokens: 873\n",
      "     Preview: Does #a[b] = 'c' + \"d\" + `2^3`# work?     8. Some @code@, with a 'quote', \"double\", ain't it grand?        Does @a[b] = 'c' + \"d\" + `2^3`@ work?     9. Some `code`, with a 'quote', \"double\", ai...\n",
      "  12. sections_Hyperlinks_to_content_block_42_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 115 | Tokens: 1034\n",
      "     Preview: Hyperlinks  ==========  There are three forms of hyperlink currently in StructuredText_:  1. (Absolute & relative URIs.)  Text enclosed by double quotes    followed by a colon, a URI, and concluded by...\n",
      "  13. sections_Hyperlinks_to_content_block_42_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 16 | Tokens: 130\n",
      "     Preview: Internal hyperlinks (links from one point to another within a single document) can be expressed by a source link as before, and a target link with a colon but no URI.  In effect, these targets 'map to...\n",
      "14. content_block_43\n",
      "   Type: content_block | Level: 1 | Lines: 101 | Tokens: 940\n",
      "   Preview: .. version:: 1      This is a footnote [1]_.      This internal hyperlink will take us to the footnotes_ area below.      Here is a one-word_ external hyperlink.      Here is `a hyperlink phrase`_...\n",
      "\n",
      "🔍 Processing: demo.rst\n",
      "🌳 Processing demo.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 112 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/user/rst/demo.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for demo.rst\n",
      "\n",
      "🔍 Processing: quickstart.rst\n",
      "🌳 Processing quickstart.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 47 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/user/rst/quickstart.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for quickstart.rst\n",
      "\n",
      "🔍 Processing: cheatsheet.rst\n",
      "🌳 Processing cheatsheet.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 18 chunks\n",
      "Created 3 grouped chunks\n",
      "Final result: 3 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 3 chunks for cheatsheet.rst\n",
      "  ✅ Saved: cheatsheet.rst_chunk_001_yqovja.md\n",
      "  ✅ Saved: cheatsheet.rst_chunk_002_yqovja.md\n",
      "  ✅ Saved: cheatsheet.rst_chunk_003_yqovja.md\n",
      "\n",
      "--- RST Chunk Summary for cheatsheet.rst ---\n",
      "1. sections_content_block_1_to_Inline Markup\n",
      "   Type: grouped_sections | Level: 1 | Lines: 78 | Tokens: 586\n",
      "   Preview: =====================================================   The reStructuredText_ Cheat Sheet: Syntax Reminders  ===================================================== :Info: See <https://docutils.sourcefo...\n",
      "2. sections_content_block_14_to_Interpreted Text Role Quick Reference\n",
      "   Type: grouped_sections | Level: 1 | Lines: 53 | Tokens: 512\n",
      "   Preview: ============= *emphasis*; **strong emphasis**; `interpreted text`; `interpreted text with role`:emphasis:; ``inline literal text``; standalone hyperlink, https://docutils.sourceforge.io; named referen...\n",
      "3. content_block_18\n",
      "   Type: content_block | Level: 1 | Lines: 19 | Tokens: 153\n",
      "   Preview: ===================================== See <https://docutils.sourceforge.io/docs/ref/rst/roles.html> for full info.  ================  ============================================================ Role...\n",
      "\n",
      "🔍 Processing: roles.rst\n",
      "🌳 Processing roles.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 56 chunks\n",
      "Created 6 grouped chunks\n",
      "Final result: 6 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 6 chunks for roles.rst\n",
      "  ✅ Saved: roles.rst_chunk_001_3j9tlg.md\n",
      "  ✅ Saved: roles.rst_chunk_002_3j9tlg.md\n",
      "  ✅ Saved: roles.rst_chunk_003_3j9tlg.md\n",
      "  ✅ Saved: roles.rst_chunk_004_3j9tlg.md\n",
      "  ✅ Saved: roles.rst_chunk_005_3j9tlg.md\n",
      "  ✅ Saved: roles.rst_chunk_006_3j9tlg.md\n",
      "\n",
      "--- RST Chunk Summary for roles.rst ---\n",
      "1. sections_content_block_1_to_content_block_15\n",
      "   Type: grouped_sections | Level: 1 | Lines: 102 | Tokens: 547\n",
      "   Preview: .. include:: ../../header2.rst  =========================================   reStructuredText Interpreted Text Roles  =========================================  :Author: David Goodger :Contact: docutil...\n",
      "2. sections_content_block_16_to_content_block_30\n",
      "   Type: grouped_sections | Level: 1 | Lines: 114 | Tokens: 530\n",
      "   Preview: .. role:: latex(code)      :language: latex  Content of the new role is parsed and tagged by the Pygments_ syntax highlighter. See the `\"code\" directive`_ for more info on parsing and display of cod...\n",
      "3. sections_content_block_31_to_content_block_45\n",
      "   Type: grouped_sections | Level: 1 | Lines: 111 | Tokens: 593\n",
      "   Preview: .. class:: field-indent-12em  :Aliases:         \\:RFC: :Doctree Element: `\\<reference>`_  The :rfc-reference: role is used to create an HTTP reference to an RFC (Internet Request for Comments).  The :...\n",
      "4. sections_content_block_46_to_content_block_51\n",
      "   Type: grouped_sections | Level: 1 | Lines: 66 | Tokens: 466\n",
      "   Preview: .. class:: field-indent-12em  :Aliases:         \\:title:, :t: :Doctree Element: `\\<title_reference>`_  The :title-reference: role is used to describe the titles of books, periodicals, and other materi...\n",
      "5. sections_content_block_52_to_Custom Roles\n",
      "   Type: grouped_sections | Level: 1 | Lines: 36 | Tokens: 298\n",
      "   Preview: .. WARNING::    The \"raw\" role is a stop-gap measure allowing the author to bypass    reStructuredText's markup.  It is a \"power-user\" feature that    should not be overused or abused.  The use of \"ra...\n",
      "6. content_block_56\n",
      "   Type: content_block | Level: 1 | Lines: 48 | Tokens: 513\n",
      "   Preview: ============  Custom interpreted text roles can be defined in a document with the `\"role\" directive`_. The new role may be based on an existing role. The \"role\" directive may be called with options_ t...\n",
      "\n",
      "🔍 Processing: directives.rst\n",
      "🌳 Processing directives.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 230 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/ref/rst/directives.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for directives.rst\n",
      "\n",
      "🔍 Processing: mathematics.rst\n",
      "🌳 Processing mathematics.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 129 chunks\n",
      "Created 26 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_51_to_content_block_54 (1651 tokens)\n",
      "  📦 Sub-chunking sections_Comparison_to_content_block_69 (1248 tokens)\n",
      "Final result: 28 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 28 chunks for mathematics.rst\n",
      "  ✅ Saved: mathematics.rst_chunk_001_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_002_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_003_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_004_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_005_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_006_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_007_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_008_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_009_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_010_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_011_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_012_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_013_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_014_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_015_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_016_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_017_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_018_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_019_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_020_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_021_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_022_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_023_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_024_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_025_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_026_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_027_rqvy0p.md\n",
      "  ✅ Saved: mathematics.rst_chunk_028_rqvy0p.md\n",
      "\n",
      "--- RST Chunk Summary for mathematics.rst ---\n",
      "1. sections_content_block_1_to_content_block_14\n",
      "   Type: grouped_sections | Level: 1 | Lines: 72 | Tokens: 490\n",
      "   Preview: .. include:: ../../header2.rst  ============================  LaTeX syntax for mathematics  ============================   .. role:: m(math)  .. default-role:: math  .. |latex| replace:: L\\ :sup:`A`\\...\n",
      "2. sections_content_block_15_to_content_block_18\n",
      "   Type: grouped_sections | Level: 1 | Lines: 43 | Tokens: 343\n",
      "   Preview: .. math::    :name: Fourier transform       (\\mathcal{F}f)(y)       = \\frac{1}{\\sqrt{2\\pi}^{\\ n}}         \\int_{\\mathbb{R}^n} f(x)\\,         e^{-\\mathrm{i} y \\cdot x} \\,\\mathrm{d} x.  The ``:name:`` o...\n",
      "3. sections_content_block_19_to_content_block_23\n",
      "   Type: grouped_sections | Level: 1 | Lines: 49 | Tokens: 380\n",
      "   Preview: .. math::   \\left.     \\begin{aligned}       B' & = -\\partial\\times E         \\\\       E' & =  \\partial\\times B - 4\\pi j     \\end{aligned}   \\;\\right\\}   \\qquad \\text{Maxwell’s equations.}   .. [#math...\n",
      "4. sections_content_block_24_to_content_block_27\n",
      "   Type: grouped_sections | Level: 1 | Lines: 31 | Tokens: 533\n",
      "   Preview: .. class:: colwidths-auto    =========== =============  ============ ==============  ============== ================  ========= ===========   `\\acute{x}` ``\\acute{x}``  `\\dot{t}`    ``\\dot{t}``     `\\...\n",
      "5. content_block_28\n",
      "   Type: content_block | Level: 1 | Lines: 26 | Tokens: 693\n",
      "   Preview: .. class:: colwidths-auto    ================== ====================  ================= ===================  ================== ====================   `*`                ``*``                 `\\circle...\n",
      "6. sections_Extensible delimiters_to_content_block_40\n",
      "   Type: grouped_sections | Level: 1 | Lines: 59 | Tokens: 574\n",
      "   Preview: Extensible delimiters  --------------------- Unless you indicate otherwise, delimiters in math formulas remain at the standard size regardless of the height of the enclosed material. To get adaptable...\n",
      "7. sections_content_block_41_to_content_block_44\n",
      "   Type: grouped_sections | Level: 1 | Lines: 35 | Tokens: 544\n",
      "   Preview: .. class:: colwidths-auto    ===============================  ======================================   `\\uparrow`     ``\\uparrow``      `\\Uparrow`     ``\\Uparrow``   `\\downarrow`   ``\\downarrow``    `...\n",
      "8. sections_content_block_45_to_content_block_47\n",
      "   Type: grouped_sections | Level: 1 | Lines: 18 | Tokens: 92\n",
      "   Preview: .. math:: \\operatorname{sgn}(-3) = -1.  .. TODO: \\operatorname* for function name with limits.  The ``\\DeclareMathOperator`` command can only be used in the `LaTeX preamble`_.  .. _LaTeX preamble: lat...\n",
      "9. sections_content_block_48_to_content_block_50\n",
      "   Type: grouped_sections | Level: 1 | Lines: 33 | Tokens: 600\n",
      "   Preview: .. class:: colwidths-auto    ========== ============  ========== ============  ========== ============  ============== ===============   `\\Gamma`   ``\\Gamma``    `\\alpha`   ``\\alpha``    `\\mu`      ``...\n",
      "  10. sections_content_block_51_to_content_block_54_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 66 | Tokens: 1014\n",
      "     Preview: .. class:: colwidths-auto    ============= ===============  ========== ============  ========== ============  =========== =============   `\\forall`     ``\\forall``      `\\aleph`   ``\\aleph``    `\\hbar...\n",
      "  11. sections_content_block_51_to_content_block_54_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 40 | Tokens: 637\n",
      "     Preview: mathbf:   `\\mathbf{ABCDEFGHIJKLMNOPQRSTUVWXYZ\\ abcdefghijklmnopqrstuvwxyz}`   `\\mathbf{ΓΔΘΛΞΠΣΥΦΨΩ\\ αβγδεζηθικλμνξπρςστυφχψω\\ ϵϑϕϰϱϖϜϝ\\ \\partial∇}`   `\\mathbf{0123456789}` mathit:   `\\mathit{ABCDEFGHI...\n",
      "12. sections_content_block_55_to_content_block_57\n",
      "   Type: grouped_sections | Level: 1 | Lines: 21 | Tokens: 193\n",
      "   Preview: .. math::    V_i x \\pm \\cos(\\alpha) \\approx 3\\Gamma \\quad \\forall x\\in\\mathbb{R}     \\boldsymbol{V_i x \\pm \\cos(\\alpha) \\approx 3\\Gamma \\quad \\forall x\\in\\mathbb{R}}  It is usually ill-advised to appl...\n",
      "13. sections_content_block_58_to_content_block_60\n",
      "   Type: grouped_sections | Level: 1 | Lines: 22 | Tokens: 416\n",
      "   Preview: .. class:: colwidths-auto  ==================== ======================  ================ ==================  ================= =================== `\\#`                 ``\\#``                  `\\clubsu...\n",
      "14. sections_content_block_61_to_content_block_65\n",
      "   Type: grouped_sections | Level: 1 | Lines: 24 | Tokens: 221\n",
      "   Preview: .. class:: colwidths-auto  === =====  ======== ===============  ======== ========== `.` ``.``  `!`      ``!``            `\\vdots` ``\\vdots`` `/` ``/``  `?`      ``?``            `\\dotsb` ``\\dotsb`` `|...\n",
      "15. content_block_66\n",
      "   Type: content_block | Level: 1 | Lines: 37 | Tokens: 833\n",
      "   Preview: .. class:: colwidths-auto    ====================== ========================  ===================== =======================   `\\circlearrowleft`     ``\\circlearrowleft``      `\\circlearrowright`   ``\\...\n",
      "  16. sections_Comparison_to_content_block_69_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 28 | Tokens: 988\n",
      "     Preview: Comparison  ~~~~~~~~~~   .. class:: colwidths-auto  ================ ==================  ============= ===============  ============= ===============  =============== ================= `<`...\n",
      "  17. sections_Comparison_to_content_block_69_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 14 | Tokens: 260\n",
      "     Preview: `\\equiv`         ``\\equiv``          `\\lessgtr`    ``\\lessgtr``     `\\nsucceq`    ``\\nsucceq``     `\\thicksim`     ``\\thicksim`` `\\fallingdotseq` ``\\fallingdotseq``  `\\lesssim`    ``\\lesssim``...\n",
      "18. sections_Miscellaneous relations_to_content_block_71\n",
      "   Type: grouped_sections | Level: 1 | Lines: 3 | Tokens: 8\n",
      "   Preview: Miscellaneous relations  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "19. content_block_72\n",
      "   Type: content_block | Level: 1 | Lines: 37 | Tokens: 921\n",
      "   Preview: .. class:: colwidths-auto    ===================== =======================  =================== =====================  =================== =====================   `\\backepsilon`        ``\\backepsilon`...\n",
      "20. sections_Variable-sized operators_to_content_block_88\n",
      "   Type: grouped_sections | Level: 1 | Lines: 79 | Tokens: 580\n",
      "   Preview: Variable-sized operators  ------------------------  .. class:: colwidths-auto    =========================  =========================  =========================  ===========================   `\\sum`...\n",
      "21. sections_content_block_89_to_content_block_93\n",
      "   Type: grouped_sections | Level: 1 | Lines: 36 | Tokens: 287\n",
      "   Preview: .. math::      \\left ( \\begin{matrix} a & b \\\\ c & d \\end{matrix} \\right)  The environments ``pmatrix``, ``bmatrix``, ``Bmatrix``, ``vmatrix``, and ``Vmatrix`` have (respectively) ( ), [ ], { }, \\| \\|...\n",
      "22. sections_content_block_94_to_content_block_97\n",
      "   Type: grouped_sections | Level: 1 | Lines: 42 | Tokens: 562\n",
      "   Preview: .. class:: colwidths-auto    ======================  ========  =====================  ===================   :m:`3\\qquad 4`                    ``3\\qquad 4``          = 2em   :m:`3\\quad 4`...\n",
      "23. sections_content_block_98_to_content_block_106\n",
      "   Type: grouped_sections | Level: 1 | Lines: 56 | Tokens: 469\n",
      "   Preview: .. class:: colwidths-auto    =========  ===========================  =========================   command    example                      result   =========  ===========================  ==============...\n",
      "24. sections_content_block_107_to_content_block_112\n",
      "   Type: grouped_sections | Level: 1 | Lines: 52 | Tokens: 479\n",
      "   Preview: .. math:: \\frac{x+1}{x-1}  \\quad           \\dfrac{x+1}{x-1} \\quad           \\tfrac{x+1}{x-1}  and in text: `\\frac{x+1}{x-1}`, `\\dfrac{x+1}{x-1}`, `\\tfrac{x+1}{x-1}`.  For binomial expressions such as...\n",
      "25. sections_content_block_113_to_content_block_114\n",
      "   Type: grouped_sections | Level: 1 | Lines: 31 | Tokens: 511\n",
      "   Preview: .. class:: colwidths-auto    =========  ==============  ==============  ==============  ==============  ===============  ===============   Sizing     no              ``\\left``       ``\\bigl``       ``...\n",
      "26. sections_content_block_115_to_content_block_121\n",
      "   Type: grouped_sections | Level: 1 | Lines: 55 | Tokens: 556\n",
      "   Preview: .. math:: \\left((a_1 b_1) - (a_2 b_2)\\right)           \\left((a_2 b_1) + (a_1 b_2)\\right)           \\quad\\text{versus}\\quad           \\bigl((a_1 b_1) - (a_2 b_2)\\bigr)           \\bigl((a_2 b_1) + (a_1...\n",
      "27. sections_content_block_122_to_content_block_128\n",
      "   Type: grouped_sections | Level: 1 | Lines: 57 | Tokens: 531\n",
      "   Preview: .. math:: \\lim_{n\\to\\infty} \\sum_1^n \\frac{1}{n}  move to index positions: `\\lim_{n\\to\\infty} \\sum_1^n \\frac{1}{n}`.    Altering the placement of limits  --------------------------------  The commands...\n",
      "28. content_block_129\n",
      "   Type: content_block | Level: 1 | Lines: 21 | Tokens: 232\n",
      "   Preview: .. math::    \\frac{\\scriptstyle\\sum_{n > 0} z^n}   {\\displaystyle\\prod_{1\\leq k\\leq n} (1-q^k)}   \\text{ instead of the default }   \\frac{\\sum_{n > 0} z^n}   {\\prod_{1\\leq k\\leq n} (1-q^k)}.  .. [#] \"...\n",
      "\n",
      "🔍 Processing: restructuredtext.rst\n",
      "🌳 Processing restructuredtext.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 243 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/ref/rst/restructuredtext.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for restructuredtext.rst\n",
      "\n",
      "🔍 Processing: history.rst\n",
      "🌳 Processing history.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 3 chunks\n",
      "❌ Error processing /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils/docs/ref/rst/history.rst: 'RSTChunk' object has no attribute 'start_byte'\n",
      "⚠️ No chunks generated for history.rst\n",
      "\n",
      "🔍 Processing: definitions.rst\n",
      "🌳 Processing definitions.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 22 chunks\n",
      "Created 3 grouped chunks\n",
      "  📦 Sub-chunking sections_content_block_1_to_content_block_15 (1774 tokens)\n",
      "Final result: 4 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 4 chunks for definitions.rst\n",
      "  ✅ Saved: definitions.rst_chunk_001_gh4b3a.md\n",
      "  ✅ Saved: definitions.rst_chunk_002_gh4b3a.md\n",
      "  ✅ Saved: definitions.rst_chunk_003_gh4b3a.md\n",
      "  ✅ Saved: definitions.rst_chunk_004_gh4b3a.md\n",
      "\n",
      "--- RST Chunk Summary for definitions.rst ---\n",
      "  1. sections_content_block_1_to_content_block_15_part_1\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 139 | Tokens: 1072\n",
      "     Preview: .. include:: ../../header2.rst  .. include:: <html-roles.txt>  ============================================   reStructuredText Standard Definition Files  ============================================ :...\n",
      "  2. sections_content_block_1_to_content_block_15_part_2\n",
      "     Type: grouped_sections_part | Level: 1 | Lines: 39 | Tokens: 701\n",
      "     Preview: basic multilingual plane or BMP (wide-Unicode; code points greater    than U+FFFF).  Before Python 3.3, some distributions were \"narrow\"    and did not support wide-Unicode characters. This should...\n",
      "3. sections_Role Definitions_to_S5/HTML Definitions\n",
      "   Type: grouped_sections | Level: 1 | Lines: 64 | Tokens: 573\n",
      "   Preview: Role Definitions  ================  Role definitions use the `\"role\" directive`_ to provide additional `reStructuredText interpreted text roles`_.  .. _\"role\" directive: directives.html#role .. _reStr...\n",
      "4. content_block_22\n",
      "   Type: content_block | Level: 1 | Lines: 19 | Tokens: 122\n",
      "   Preview: -------------------  The \"s5defs.txt_\" standard definition file contains interpreted text roles (classes) and other definitions for documents destined to become `S5/HTML slide shows`_.  .. _s5defs.txt...\n",
      "\n",
      "🔍 Processing: introduction.rst\n",
      "🌳 Processing introduction.rst with tree-sitter approach\n",
      "❌ Tree-sitter parsing failed: 'tree_sitter.Parser' object has no attribute 'set_language'\n",
      "📝 Falling back to text-based structure detection\n",
      "✅ Text-based chunking found 5 chunks\n",
      "Created 2 grouped chunks\n",
      "Final result: 2 clean chunks (no warnings injected!)\n",
      "✅ All chunks verified clean - no docutils warnings!\n",
      "✅ Generated 2 chunks for introduction.rst\n",
      "  ✅ Saved: introduction.rst_chunk_001_ke7gp8.md\n",
      "  ✅ Saved: introduction.rst_chunk_002_ke7gp8.md\n",
      "\n",
      "--- RST Chunk Summary for introduction.rst ---\n",
      "1. sections_content_block_1_to_Goals\n",
      "   Type: grouped_sections | Level: 1 | Lines: 48 | Tokens: 438\n",
      "   Preview: .. include:: ../../header2.rst  =====================================   An Introduction to reStructuredText  ===================================== :Author: David Goodger :Contact: docutils-develop@lis...\n",
      "2. content_block_5\n",
      "   Type: content_block | Level: 1 | Lines: 108 | Tokens: 954\n",
      "   Preview: =====  The primary goal of reStructuredText_ is to define a markup syntax for use in Python docstrings and other documentation domains, that is readable and simple, yet powerful enough for non-trivial...\n",
      "\n",
      "🎉 Processing complete!\n",
      "✅ Files processed: 28/60\n",
      "📄 Total chunks created: 192\n",
      "📁 Output directory: /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/docs/rst/docutils_rst_chunks\n",
      "🌳 All chunks generated with tree-sitter (no warning contamination!)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Tree-sitter RST Chunker\n",
    "\n",
    "A clean, robust approach to RST chunking using tree-sitter instead of docutils.\n",
    "No warning injection, clean error handling, syntax-aware chunking.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import secrets\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "try:\n",
    "    from tree_sitter_language_pack import get_language, get_parser\n",
    "    from tree_sitter import Parser\n",
    "    \n",
    "    # Get RST language from language pack\n",
    "    try:\n",
    "        RST_LANGUAGE = get_language('rst')\n",
    "        print(\"✅ Found RST language in tree-sitter-language-pack\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not load RST from language pack: {e}\")\n",
    "        print(\"Will use fallback text-based chunking\")\n",
    "        RST_LANGUAGE = None\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"❌ tree-sitter-language-pack not installed. Install with: pip install tree-sitter-language-pack\")\n",
    "    print(\"Will use fallback text-based chunking\")\n",
    "    RST_LANGUAGE = None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION AND CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "MAX_CHUNK_TOKENS = 1000\n",
    "TARGET_TOKENS = 600\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA STRUCTURES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class RSTChunk:\n",
    "    \"\"\"Represents a semantic chunk of RST content\"\"\"\n",
    "    start_line: int\n",
    "    end_line: int\n",
    "    content: str\n",
    "    node_type: str\n",
    "    name: str\n",
    "    depth: int\n",
    "    level: int = 0\n",
    "    token_count: int = 0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.token_count == 0:\n",
    "            self.token_count = count_tokens(self.content)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RSTReference:\n",
    "    \"\"\"Represents a reference/include in RST\"\"\"\n",
    "    reference_type: str\n",
    "    target: str\n",
    "    start_line: int\n",
    "    end_line: int\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TOKEN COUNTING\n",
    "# =============================================================================\n",
    "\n",
    "def count_tokens(content: str) -> int:\n",
    "    \"\"\"Count tokens using tiktoken for GPT-4\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "        return len(encoding.encode(content))\n",
    "    except Exception:\n",
    "        return len(content) // 4\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TREE-SITTER PARSING\n",
    "# =============================================================================\n",
    "\n",
    "def parse_rst_with_tree_sitter(content: str) -> Optional[object]:\n",
    "    \"\"\"Parse RST content using tree-sitter - NO WARNING INJECTION!\"\"\"\n",
    "    if not RST_LANGUAGE:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        parser = Parser()\n",
    "        parser.set_language(RST_LANGUAGE)\n",
    "        \n",
    "        # Tree-sitter parsing - clean and simple\n",
    "        tree = parser.parse(content.encode('utf-8'))\n",
    "        \n",
    "        # Check for parse errors (but they don't contaminate content!)\n",
    "        if tree.root_node.has_error:\n",
    "            print(\"⚠️ Parse tree contains error nodes (but content stays clean)\")\n",
    "        \n",
    "        return tree\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Tree-sitter parsing failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_node_text(node: object, source_bytes: bytes) -> str:\n",
    "    \"\"\"Extract clean text from tree-sitter node\"\"\"\n",
    "    return source_bytes[node.start_byte:node.end_byte].decode('utf-8', errors='ignore')\n",
    "\n",
    "\n",
    "def bytes_to_line_numbers(start_byte: int, end_byte: int, source_lines: List[str]) -> Tuple[int, int]:\n",
    "    \"\"\"Convert byte positions to line numbers\"\"\"\n",
    "    current_byte = 0\n",
    "    start_line = 1\n",
    "    end_line = 1\n",
    "    \n",
    "    for line_num, line in enumerate(source_lines, 1):\n",
    "        line_bytes = len(line.encode('utf-8')) + 1  # +1 for newline\n",
    "        \n",
    "        if current_byte <= start_byte < current_byte + line_bytes:\n",
    "            start_line = line_num\n",
    "        if current_byte < end_byte <= current_byte + line_bytes:\n",
    "            end_line = line_num\n",
    "            break\n",
    "            \n",
    "        current_byte += line_bytes\n",
    "    \n",
    "    return start_line, end_line\n",
    "\n",
    "\n",
    "def find_rst_sections(node: object, source_bytes: bytes, depth: int = 0) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Find RST sections in the parse tree\"\"\"\n",
    "    sections = []\n",
    "    source_lines = source_bytes.decode('utf-8', errors='ignore').split('\\n')\n",
    "    \n",
    "    # Look for section-like nodes\n",
    "    if node.type in ['section', 'title', 'heading']:\n",
    "        text = extract_node_text(node, source_bytes)\n",
    "        start_line, end_line = bytes_to_line_numbers(node.start_byte, node.end_byte, source_lines)\n",
    "        \n",
    "        sections.append({\n",
    "            'type': 'section',\n",
    "            'name': text.strip().split('\\n')[0][:50],  # First line as name\n",
    "            'start_line': start_line,\n",
    "            'end_line': end_line,\n",
    "            'content': text,\n",
    "            'depth': depth,\n",
    "            'level': depth + 1\n",
    "        })\n",
    "    \n",
    "    # Look for directive nodes (.. note::, .. code-block::, etc.)\n",
    "    elif node.type in ['directive', 'admonition', 'code_block']:\n",
    "        text = extract_node_text(node, source_bytes)\n",
    "        directive_name = text.split('\\n')[0].strip()\n",
    "        start_line, end_line = bytes_to_line_numbers(node.start_byte, node.end_byte, source_lines)\n",
    "        \n",
    "        sections.append({\n",
    "            'type': f'{node.type}_directive',\n",
    "            'name': directive_name,\n",
    "            'start_line': start_line,\n",
    "            'end_line': end_line,\n",
    "            'content': text,\n",
    "            'depth': depth,\n",
    "            'level': 8  # Lower priority for directives\n",
    "        })\n",
    "    \n",
    "    # Recursively check children\n",
    "    for child in node.children:\n",
    "        sections.extend(find_rst_sections(child, source_bytes, depth + 1))\n",
    "    \n",
    "    return sections\n",
    "\n",
    "\n",
    "def extract_references_tree_sitter(node: object, source_bytes: bytes) -> List[RSTReference]:\n",
    "    \"\"\"Extract references using tree-sitter - much cleaner than docutils\"\"\"\n",
    "    references = []\n",
    "    source_lines = source_bytes.decode('utf-8', errors='ignore').split('\\n')\n",
    "    \n",
    "    # Look for reference-like nodes\n",
    "    if node.type in ['reference', 'link', 'image', 'include']:\n",
    "        text = extract_node_text(node, source_bytes)\n",
    "        ref_type = node.type\n",
    "        target = text.strip()\n",
    "        start_line, end_line = bytes_to_line_numbers(node.start_byte, node.end_byte, source_lines)\n",
    "        \n",
    "        references.append(RSTReference(\n",
    "            reference_type=ref_type,\n",
    "            target=target,\n",
    "            start_line=start_line,\n",
    "            end_line=end_line\n",
    "        ))\n",
    "    \n",
    "    # Recursively check children\n",
    "    for child in node.children:\n",
    "        references.extend(extract_references_tree_sitter(child, source_bytes))\n",
    "    \n",
    "    return references\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FALLBACK TEXT-BASED CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def chunk_rst_by_text_structure(content: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fallback chunking when tree-sitter isn't available.\n",
    "    Uses text patterns to identify RST structures.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    lines = content.split('\\n')\n",
    "    current_chunk_lines = []\n",
    "    current_start_line = 1\n",
    "    \n",
    "    for i, line in enumerate(lines, 1):\n",
    "        line_stripped = line.strip()\n",
    "        \n",
    "        # Check if this looks like a section header\n",
    "        is_section_header = False\n",
    "        if i < len(lines):\n",
    "            next_line = lines[i].strip() if i < len(lines) else \"\"\n",
    "            # RST section headers have underlines of =, -, ~, ^, etc.\n",
    "            if (line_stripped and next_line and \n",
    "                all(c in '=-~^\"\\'`#*+<>' for c in next_line) and\n",
    "                len(next_line) >= len(line_stripped) * 0.8):\n",
    "                is_section_header = True\n",
    "        \n",
    "        # Check if this is a directive\n",
    "        is_directive = line_stripped.startswith('.. ') and '::' in line_stripped\n",
    "        \n",
    "        # If we hit a section or directive, save previous chunk\n",
    "        if (is_section_header or is_directive) and current_chunk_lines:\n",
    "            chunk_content = '\\n'.join(current_chunk_lines)\n",
    "            if chunk_content.strip():\n",
    "                chunks.append({\n",
    "                    'type': 'content_block',\n",
    "                    'name': f'content_block_{len(chunks) + 1}',\n",
    "                    'start_line': current_start_line,\n",
    "                    'end_line': i - 1,\n",
    "                    'content': chunk_content,\n",
    "                    'depth': 0,\n",
    "                    'level': 1\n",
    "                })\n",
    "            current_chunk_lines = []\n",
    "            current_start_line = i\n",
    "        \n",
    "        current_chunk_lines.append(line)\n",
    "        \n",
    "        # If this was a section header, process it\n",
    "        if is_section_header:\n",
    "            section_content = '\\n'.join(current_chunk_lines)\n",
    "            chunks.append({\n",
    "                'type': 'section',\n",
    "                'name': line_stripped,\n",
    "                'start_line': current_start_line,\n",
    "                'end_line': i + 1,  # Include underline\n",
    "                'content': section_content,\n",
    "                'depth': 0,\n",
    "                'level': 1\n",
    "            })\n",
    "            current_chunk_lines = []\n",
    "            current_start_line = i + 2\n",
    "    \n",
    "    # Add final chunk\n",
    "    if current_chunk_lines:\n",
    "        chunk_content = '\\n'.join(current_chunk_lines)\n",
    "        if chunk_content.strip():\n",
    "            chunks.append({\n",
    "                'type': 'content_block',\n",
    "                'name': f'content_block_{len(chunks) + 1}',\n",
    "                'start_line': current_start_line,\n",
    "                'end_line': len(lines),\n",
    "                'content': chunk_content,\n",
    "                'depth': 0,\n",
    "                'level': 1\n",
    "            })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def process_rst_content_tree_sitter(rst_content: str, file_name: str = \"content.rst\") -> List[RSTChunk]:\n",
    "    \"\"\"\n",
    "    Process RST content using tree-sitter approach.\n",
    "    GUARANTEED: No warning injection, clean content extraction.\n",
    "    \"\"\"\n",
    "    print(f\"🌳 Processing {file_name} with tree-sitter approach\")\n",
    "    \n",
    "    # Try tree-sitter parsing first\n",
    "    semantic_nodes = []\n",
    "    \n",
    "    if RST_LANGUAGE:\n",
    "        tree = parse_rst_with_tree_sitter(rst_content)\n",
    "        if tree:\n",
    "            source_bytes = rst_content.encode('utf-8')\n",
    "            \n",
    "            # Extract semantic chunks using tree-sitter\n",
    "            semantic_nodes = find_rst_sections(tree.root_node, source_bytes)\n",
    "            \n",
    "            # Extract references\n",
    "            references = extract_references_tree_sitter(tree.root_node, source_bytes)\n",
    "            \n",
    "            print(f\"✅ Tree-sitter found {len(semantic_nodes)} semantic units\")\n",
    "            print(f\"📎 Found {len(references)} references\")\n",
    "    \n",
    "    # Fallback to text-based chunking if tree-sitter failed\n",
    "    if not semantic_nodes:\n",
    "        print(\"📝 Falling back to text-based structure detection\")\n",
    "        semantic_nodes = chunk_rst_by_text_structure(rst_content)\n",
    "        print(f\"✅ Text-based chunking found {len(semantic_nodes)} chunks\")\n",
    "    \n",
    "    # Convert to RSTChunk objects\n",
    "    chunks = []\n",
    "    for node_info in semantic_nodes:\n",
    "        chunk = RSTChunk(\n",
    "            start_line=node_info['start_line'],\n",
    "            end_line=node_info['end_line'],\n",
    "            content=node_info['content'],\n",
    "            node_type=node_info['type'],\n",
    "            name=node_info['name'],\n",
    "            depth=node_info['depth'],\n",
    "            level=node_info['level']\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # Group small chunks\n",
    "    chunks = group_small_chunks(chunks, target_tokens=TARGET_TOKENS)\n",
    "    print(f\"Created {len(chunks)} grouped chunks\")\n",
    "    \n",
    "    # Sub-chunk oversized chunks\n",
    "    final_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if chunk.token_count > MAX_CHUNK_TOKENS:\n",
    "            print(f\"  📦 Sub-chunking {chunk.name} ({chunk.token_count} tokens)\")\n",
    "            sub_chunks = sub_chunk_by_bytes(chunk, rst_content)\n",
    "            final_chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            final_chunks.append(chunk)\n",
    "    \n",
    "    print(f\"Final result: {len(final_chunks)} clean chunks (no warnings injected!)\")\n",
    "    \n",
    "    # Verify content cleanliness\n",
    "    warning_patterns = ['(WARNING/', '(ERROR/', '(INFO/', '(SEVERE/']\n",
    "    contaminated_chunks = 0\n",
    "    for chunk in final_chunks:\n",
    "        if any(pattern in chunk.content for pattern in warning_patterns):\n",
    "            contaminated_chunks += 1\n",
    "    \n",
    "    if contaminated_chunks == 0:\n",
    "        print(\"✅ All chunks verified clean - no docutils warnings!\")\n",
    "    else:\n",
    "        print(f\"⚠️ {contaminated_chunks} chunks may contain warnings\")\n",
    "    \n",
    "    return final_chunks\n",
    "\n",
    "\n",
    "def group_small_chunks(chunks: List[RSTChunk], target_tokens: int = 600) -> List[RSTChunk]:\n",
    "    \"\"\"Group small chunks together\"\"\"\n",
    "    if not chunks:\n",
    "        return chunks\n",
    "    \n",
    "    total_tokens = sum(c.token_count for c in chunks)\n",
    "    if total_tokens <= MAX_CHUNK_TOKENS:\n",
    "        combined_content = \"\\n\\n\".join(c.content for c in chunks)\n",
    "        return [RSTChunk(\n",
    "            start_line=min(c.start_line for c in chunks),\n",
    "            end_line=max(c.end_line for c in chunks),\n",
    "            content=combined_content,\n",
    "            node_type=\"combined_document\",\n",
    "            name=\"complete_document\",\n",
    "            depth=0,\n",
    "            level=0\n",
    "        )]\n",
    "    \n",
    "    grouped_chunks = []\n",
    "    current_group = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if (current_tokens + chunk.token_count > target_tokens and \n",
    "            current_group and chunk.token_count <= MAX_CHUNK_TOKENS):\n",
    "            \n",
    "            if len(current_group) == 1:\n",
    "                grouped_chunks.append(current_group[0])\n",
    "            else:\n",
    "                combined_content = \"\\n\\n\".join(c.content for c in current_group)\n",
    "                grouped_chunk = RSTChunk(\n",
    "                    start_line=min(c.start_line for c in current_group),\n",
    "                    end_line=max(c.end_line for c in current_group),\n",
    "                    content=combined_content,\n",
    "                    node_type=\"grouped_sections\",\n",
    "                    name=f\"sections_{current_group[0].name}_to_{current_group[-1].name}\",\n",
    "                    depth=min(c.depth for c in current_group),\n",
    "                    level=min(c.level for c in current_group)\n",
    "                )\n",
    "                grouped_chunks.append(grouped_chunk)\n",
    "            \n",
    "            current_group = [chunk]\n",
    "            current_tokens = chunk.token_count\n",
    "        else:\n",
    "            current_group.append(chunk)\n",
    "            current_tokens += chunk.token_count\n",
    "    \n",
    "    if current_group:\n",
    "        if len(current_group) == 1:\n",
    "            grouped_chunks.append(current_group[0])\n",
    "        else:\n",
    "            combined_content = \"\\n\\n\".join(c.content for c in current_group)\n",
    "            grouped_chunk = RSTChunk(\n",
    "                start_byte=min(c.start_byte for c in current_group),\n",
    "                end_byte=max(c.end_byte for c in current_group),\n",
    "                content=combined_content,\n",
    "                node_type=\"grouped_sections\",\n",
    "                name=f\"sections_{current_group[0].name}_to_{current_group[-1].name}\",\n",
    "                depth=min(c.depth for c in current_group),\n",
    "                level=min(c.level for c in current_group)\n",
    "            )\n",
    "            grouped_chunks.append(grouped_chunk)\n",
    "    \n",
    "    return grouped_chunks\n",
    "\n",
    "\n",
    "def sub_chunk_by_bytes(chunk: RSTChunk, rst_content: str) -> List[RSTChunk]:\n",
    "    \"\"\"Sub-chunk oversized chunks by line boundaries\"\"\"\n",
    "    if chunk.token_count <= MAX_CHUNK_TOKENS:\n",
    "        return [chunk]\n",
    "    \n",
    "    content = chunk.content\n",
    "    lines = content.split('\\n')\n",
    "    sub_chunks = []\n",
    "    current_lines = []\n",
    "    current_tokens = 0\n",
    "    part_num = 1\n",
    "    current_line_num = chunk.start_line\n",
    "    \n",
    "    for line in lines:\n",
    "        line_tokens = count_tokens(line)\n",
    "        \n",
    "        if current_tokens + line_tokens > MAX_CHUNK_TOKENS and current_lines:\n",
    "            sub_content = '\\n'.join(current_lines)\n",
    "            if sub_content.strip():\n",
    "                sub_chunk = RSTChunk(\n",
    "                    start_line=current_line_num,\n",
    "                    end_line=current_line_num + len(current_lines) - 1,\n",
    "                    content=sub_content,\n",
    "                    node_type=f\"{chunk.node_type}_part\",\n",
    "                    name=f\"{chunk.name}_part_{part_num}\",\n",
    "                    depth=chunk.depth + 1,\n",
    "                    level=chunk.level\n",
    "                )\n",
    "                sub_chunks.append(sub_chunk)\n",
    "                part_num += 1\n",
    "                current_line_num += len(current_lines)\n",
    "            \n",
    "            current_lines = [line]\n",
    "            current_tokens = line_tokens\n",
    "        else:\n",
    "            current_lines.append(line)\n",
    "            current_tokens += line_tokens\n",
    "    \n",
    "    if current_lines:\n",
    "        sub_content = '\\n'.join(current_lines)\n",
    "        if sub_content.strip():\n",
    "            sub_chunk = RSTChunk(\n",
    "                start_line=current_line_num,\n",
    "                end_line=chunk.end_line,\n",
    "                content=sub_content,\n",
    "                node_type=f\"{chunk.node_type}_part\",\n",
    "                name=f\"{chunk.name}_part_{part_num}\",\n",
    "                depth=chunk.depth + 1,\n",
    "                level=chunk.level\n",
    "            )\n",
    "            sub_chunks.append(sub_chunk)\n",
    "    \n",
    "    return sub_chunks if sub_chunks else [chunk]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FILE PROCESSING AND DIRECTORY HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def process_rst_file(file_path: Path) -> List[RSTChunk]:\n",
    "    \"\"\"Process a single RST file and return chunks\"\"\"\n",
    "    print(f\"\\n🔍 Processing: {file_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            rst_content = f.read()\n",
    "        \n",
    "        chunks = process_rst_content_tree_sitter(rst_content, file_path.name)\n",
    "        \n",
    "        if chunks:\n",
    "            print(f\"✅ Generated {len(chunks)} chunks for {file_path.name}\")\n",
    "        else:\n",
    "            print(f\"⚠️ No chunks generated for {file_path.name}\")\n",
    "            \n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def save_chunks_to_files(chunks: List[RSTChunk], \n",
    "                        original_file_path: Path, \n",
    "                        input_directory: Path,\n",
    "                        output_base: Path,\n",
    "                        references: List[RSTReference]) -> List[str]:\n",
    "    \"\"\"Save chunks as markdown files maintaining directory structure\"\"\"\n",
    "    if not chunks:\n",
    "        return []\n",
    "    \n",
    "    # Calculate relative path from input directory\n",
    "    try:\n",
    "        rel_path = original_file_path.relative_to(input_directory)\n",
    "    except ValueError:\n",
    "        # If file is not under input directory, use just the filename\n",
    "        rel_path = original_file_path.name\n",
    "    \n",
    "    # Create output directory structure\n",
    "    output_dir = output_base / rel_path.parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate unique ID for this file\n",
    "    file_unique_id = generate_unique_id()\n",
    "    \n",
    "    saved_files = []\n",
    "    \n",
    "    # Add chunk index to each chunk and save\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        # Create chunk filename\n",
    "        chunk_filename = f\"{original_file_path.name}_chunk_{i:03d}_{file_unique_id}.md\"\n",
    "        \n",
    "        # Create markdown content\n",
    "        markdown_content = create_chunk_markdown(\n",
    "            chunk, \n",
    "            str(rel_path), \n",
    "            references\n",
    "        )\n",
    "        \n",
    "        # Write to file\n",
    "        chunk_file_path = output_dir / chunk_filename\n",
    "        try:\n",
    "            with open(chunk_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "            \n",
    "            saved_files.append(str(chunk_file_path))\n",
    "            print(f\"  ✅ Saved: {chunk_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error saving {chunk_filename}: {e}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "\n",
    "def print_chunk_summary(chunks: List[RSTChunk], file_name: str):\n",
    "    \"\"\"Print detailed summary of chunks\"\"\"\n",
    "    print(f\"\\n--- RST Chunk Summary for {file_name} ---\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        indent = \"  \" * chunk.depth\n",
    "        content_lines = len(chunk.content.split('\\n'))\n",
    "        \n",
    "        print(f\"{indent}{i}. {chunk.name}\")\n",
    "        print(f\"{indent}   Type: {chunk.node_type} | Level: {chunk.level} | Lines: {content_lines} | Tokens: {chunk.token_count}\")\n",
    "        \n",
    "        # Show content preview\n",
    "        preview = chunk.content[:200].replace('\\n', ' ').strip()\n",
    "        if len(chunk.content) > 200:\n",
    "            preview += \"...\"\n",
    "        print(f\"{indent}   Preview: {preview}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING - SAME INTERFACE AS ORIGINAL\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for RST semantic chunking - SAME INTERFACE AS ORIGINAL\"\"\"\n",
    "    print(\"🚀 RST (reStructuredText) Semantic Chunking with Tree-sitter\")\n",
    "    print(f\"Max chunk tokens: {MAX_CHUNK_TOKENS}\")\n",
    "    print(f\"Target tokens for grouping: {TARGET_TOKENS}\")\n",
    "    print(\"🌳 Using tree-sitter for clean, warning-free parsing\")\n",
    "    \n",
    "    # Get directory from user or use current directory - SAME AS ORIGINAL\n",
    "    directory = input(\"\\nEnter source directory path (or press Enter for current directory): \").strip()\n",
    "    if not directory:\n",
    "        directory = \".\"\n",
    "    \n",
    "    target_dir = Path(directory).resolve()\n",
    "    if not target_dir.exists():\n",
    "        print(f\"❌ Directory not found: {directory}\")\n",
    "        return\n",
    "    \n",
    "    # Create output directory parallel to source directory - SAME AS ORIGINAL\n",
    "    output_dir = target_dir.parent / f\"{target_dir.name}_rst_chunks\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    print(f\"📁 Output directory: {output_dir}\")\n",
    "    \n",
    "    target_path = target_dir\n",
    "    input_directory = target_dir\n",
    "    \n",
    "    # Collect RST files - SAME AS ORIGINAL\n",
    "    rst_files = []\n",
    "    for ext in ['*.rst', '*.txt']:\n",
    "        rst_files.extend(target_path.rglob(ext))\n",
    "    \n",
    "    # Filter to actual RST files by checking content - SAME AS ORIGINAL\n",
    "    actual_rst_files = []\n",
    "    for file in rst_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read(1000)  # Check first 1000 chars\n",
    "                # Simple heuristic: look for RST-like content\n",
    "                if any(marker in content for marker in ['===', '---', '~~~', '^^^', '.. ', '::']):\n",
    "                    actual_rst_files.append(file)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    rst_files = actual_rst_files\n",
    "    \n",
    "    if not rst_files:\n",
    "        print(f\"❌ No RST files found in {directory}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"🔍 Found {len(rst_files)} RST file(s)\")\n",
    "    \n",
    "    # Group by directory for display - SAME AS ORIGINAL\n",
    "    by_dir = {}\n",
    "    for f in rst_files:\n",
    "        dir_path = str(f.parent.relative_to(input_directory)) if f.parent != input_directory else '.'\n",
    "        if dir_path not in by_dir:\n",
    "            by_dir[dir_path] = []\n",
    "        by_dir[dir_path].append(f)\n",
    "    \n",
    "    # Show files found - SAME AS ORIGINAL\n",
    "    for dir_path, files in sorted(by_dir.items()):\n",
    "        print(f\"  📂 {dir_path}:\")\n",
    "        for f in files:\n",
    "            print(f\"    - {f.name}\")\n",
    "    \n",
    "    proceed = input(f\"\\nProcess all {len(rst_files)} files? (y/n): \").strip().lower()\n",
    "    if proceed != 'y':\n",
    "        print(\"❌ Processing cancelled\")\n",
    "        return\n",
    "    \n",
    "    # Process all files - SAME AS ORIGINAL\n",
    "    total_chunks = 0\n",
    "    processed_files = 0\n",
    "    \n",
    "    for file_path in rst_files:\n",
    "        try:\n",
    "            chunks = process_rst_file(file_path)\n",
    "            \n",
    "            if chunks:\n",
    "                # Extract references for this file\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                    references = extract_references_from_source(content)\n",
    "                except:\n",
    "                    references = []\n",
    "                \n",
    "                # Save chunks - SAME AS ORIGINAL\n",
    "                saved_files = save_chunks_to_files(\n",
    "                    chunks, file_path, input_directory, output_dir, references\n",
    "                )\n",
    "                \n",
    "                total_chunks += len(chunks)\n",
    "                processed_files += 1\n",
    "                \n",
    "                print_chunk_summary(chunks, file_path.name)\n",
    "            else:\n",
    "                print(f\"⚠️ No chunks generated for {file_path.name}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Processing complete!\")\n",
    "    print(f\"✅ Files processed: {processed_files}/{len(rst_files)}\")\n",
    "    print(f\"📄 Total chunks created: {total_chunks}\")\n",
    "    print(f\"📁 Output directory: {output_dir}\")\n",
    "    print(\"🌳 All chunks generated with tree-sitter (no warning contamination!)\")\n",
    "\n",
    "\n",
    "def extract_references_from_source(rst_content: str) -> List[RSTReference]:\n",
    "    \"\"\"Extract references from source text as fallback\"\"\"\n",
    "    references = []\n",
    "    lines = rst_content.split('\\n')\n",
    "    \n",
    "    for line_num, line in enumerate(lines, 1):\n",
    "        line = line.strip()\n",
    "        if line.startswith('..'):\n",
    "            for ref_type in ['include', 'literalinclude', 'image', 'figure', 'csv-table']:\n",
    "                if f'.. {ref_type}::' in line:\n",
    "                    parts = line.split('::', 1)\n",
    "                    if len(parts) > 1:\n",
    "                        target = parts[1].strip()\n",
    "                        references.append(RSTReference(\n",
    "                            reference_type=ref_type,\n",
    "                            target=target,\n",
    "                            start_line=line_num,\n",
    "                            end_line=line_num\n",
    "                        ))\n",
    "                    break\n",
    "    \n",
    "    return references\n",
    "\n",
    "def generate_unique_id(length: int = 6) -> str:\n",
    "    \"\"\"Generate a random unique ID\"\"\"\n",
    "    alphabet = string.ascii_lowercase + string.digits\n",
    "    return ''.join(secrets.choice(alphabet) for _ in range(length))\n",
    "\n",
    "\n",
    "def create_chunk_markdown(chunk: RSTChunk, source_file_path: str, references: List[RSTReference]) -> str:\n",
    "    \"\"\"Create markdown content with YAML frontmatter\"\"\"\n",
    "    unique_id = generate_unique_id()\n",
    "    \n",
    "    # Filter references that might apply to this chunk\n",
    "    chunk_references = []\n",
    "    for ref in references:\n",
    "        if chunk.start_line <= ref.start_line <= chunk.end_line:\n",
    "            chunk_references.append(f\"{ref.reference_type}: {ref.target}\")\n",
    "    \n",
    "    frontmatter = f\"\"\"---\n",
    "file_path: \"{source_file_path}\"\n",
    "chunk_id: \"{unique_id}\"\n",
    "chunk_type: \"{chunk.node_type}\"\n",
    "chunk_name: \"{chunk.name}\"\n",
    "start_line: {chunk.start_line}\n",
    "end_line: {chunk.end_line}\n",
    "token_count: {chunk.token_count}\n",
    "depth: {chunk.depth}\n",
    "level: {chunk.level}\n",
    "language: \"rst\"\n",
    "references: {chunk_references}\n",
    "---\n",
    "\n",
    "# {chunk.name}\n",
    "\n",
    "```rst\n",
    "{chunk.content}\n",
    "```\n",
    "\"\"\"\n",
    "    return frontmatter\n",
    "\n",
    "\n",
    "def save_chunks_as_markdown(chunks: List[RSTChunk], output_dir: str = \"rst_chunks_tree_sitter\") -> None:\n",
    "    \"\"\"Save chunks as markdown files\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    file_unique_id = generate_unique_id()\n",
    "    saved_count = 0\n",
    "    \n",
    "    print(f\"\\n💾 Saving {len(chunks)} chunks to {output_path}/\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        chunk_filename = f\"chunk_{i:03d}_{file_unique_id}.md\"\n",
    "        markdown_content = create_chunk_markdown(chunk, \"notebook_content.rst\", [])\n",
    "        \n",
    "        chunk_file_path = output_path / chunk_filename\n",
    "        try:\n",
    "            with open(chunk_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "            \n",
    "            print(f\"  ✅ Saved: {chunk_filename}\")\n",
    "            saved_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error saving {chunk_filename}: {e}\")\n",
    "    \n",
    "    print(f\"✅ Successfully saved {saved_count} clean chunk files\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# NOTEBOOK INTERFACE (for direct content processing)\n",
    "# =============================================================================\n",
    "\n",
    "def save_chunks_as_markdown(chunks: List[RSTChunk], output_dir: str = \"rst_chunks_tree_sitter\") -> None:\n",
    "    \"\"\"Save chunks as markdown files (notebook version)\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    file_unique_id = generate_unique_id()\n",
    "    saved_count = 0\n",
    "    \n",
    "    print(f\"\\n💾 Saving {len(chunks)} chunks to {output_path}/\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        chunk_filename = f\"chunk_{i:03d}_{file_unique_id}.md\"\n",
    "        markdown_content = create_chunk_markdown(chunk, \"notebook_content.rst\", [])\n",
    "        \n",
    "        chunk_file_path = output_path / chunk_filename\n",
    "        try:\n",
    "            with open(chunk_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "            \n",
    "            print(f\"  ✅ Saved: {chunk_filename}\")\n",
    "            saved_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error saving {chunk_filename}: {e}\")\n",
    "    \n",
    "    print(f\"✅ Successfully saved {saved_count} clean chunk files (no warnings!)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
