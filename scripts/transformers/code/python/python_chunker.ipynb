{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aed5f0f",
   "metadata": {},
   "source": [
    "Key Features of the Python AST Chunker:\n",
    "1. Python-Specific AST Node Types\n",
    "\n",
    "function_definition - Regular function definitions\n",
    "async_function_definition - Async function definitions\n",
    "class_definition - Class definitions\n",
    "decorated_definition - Functions/classes with\n",
    "RetryAContinueEditKey Features of the Python AST Chunker (continued):\n",
    "1. Python-Specific AST Node Types\n",
    "\n",
    "function_definition - Regular function definitions\n",
    "async_function_definition - Async function definitions\n",
    "class_definition - Class definitions\n",
    "decorated_definition - Functions/classes with decorators (@decorator)\n",
    "import_statement & import_from_statement - Import statements\n",
    "assignment - Global variable assignments and constants\n",
    "if_statement, try_statement, with_statement - Control structures\n",
    "for_statement, while_statement - Loop structures\n",
    "\n",
    "2. Python-Specific Name Extraction\n",
    "The extract_node_name() function handles Python patterns:\n",
    "\n",
    "Function names: def function_name(\n",
    "Class names: class ClassName\n",
    "Async functions: async def function_name(\n",
    "Decorated functions: @decorator def function_name(\n",
    "Import statements: import module or from module import something\n",
    "Special handling for if __name__ == \"__main__\": blocks\n",
    "Variable assignments: CONSTANT = value\n",
    "\n",
    "3. Semantic Boundaries for Python\n",
    "The chunker identifies meaningful Python code blocks:\n",
    "\n",
    "Top-level functions and classes (including async and decorated)\n",
    "Import statements (grouped together)\n",
    "Global assignments (constants, configuration variables)\n",
    "Main execution blocks (if __name__ == \"__main__\":)\n",
    "Module-level control structures (substantial if/try/with blocks)\n",
    "\n",
    "4. Intelligent Grouping\n",
    "\n",
    "Import consolidation: Groups all imports into a single chunk if under token limit\n",
    "Small chunk merging: Combines related small chunks to reach optimal size\n",
    "Complete module detection: If entire file is under token limit, keeps it as one chunk\n",
    "Hierarchical sub-chunking: Breaks down oversized chunks using line-based splitting\n",
    "\n",
    "5. Enhanced Interactive Interface\n",
    "\n",
    "Directory scanning: Finds all .py files recursively\n",
    "File selection options: Process all, select specific files, or choose by directory\n",
    "Progress reporting: Shows what semantic units are found in each file\n",
    "Statistics: Provides token counts, chunk counts, and processing summaries\n",
    "Markdown export: Saves chunks with YAML frontmatter for documentation\n",
    "\n",
    "Questions for You:\n",
    "\n",
    "Additional Node Types: Are there other Python constructs you'd like to treat as semantic boundaries? (e.g., global variables, module docstrings, exception handlers)\n",
    "Decorator Handling: Should decorated functions be treated differently, or is the current approach of keeping them as single units appropriate?\n",
    "Class Method Chunking: Should we chunk individual methods within large classes, or keep entire classes together?\n",
    "Module Structure: Do you want special handling for common Python patterns like __all__ definitions, module-level constants, or configuration sections?\n",
    "Testing Integration: Should the chunker have special handling for test files (recognizing test classes, test functions, fixtures, etc.)?\n",
    "\n",
    "The chunker is ready to use and follows the same robust architecture as the TypeScript version, with Python-specific adaptations for the language's syntax and common patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "263a2f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using Python parser\n",
      "🚀 Python Semantic Chunking Test\n",
      "Max chunk tokens: 1000\n",
      "Max recursion depth: 3\n",
      "Supported extensions: .py\n",
      "📁 Found 20 Python files:\n",
      "  📂 py_src/fusion: 18 files\n",
      "    📄 __init__.py\n",
      "    📄 __main__.py\n",
      "    📄 attributes.py\n",
      "    ... and 15 more\n",
      "  📂 py_src/fusion/_legacy: 2 files\n",
      "    📄 __init__.py\n",
      "    📄 authentication.py\n",
      "\n",
      "🔍 Select files to process:\n",
      "1. Process all files\n",
      "2. Select specific files\n",
      "3. Process files in a specific directory\n",
      "✅ Processing all 20 files\n",
      "\n",
      "🔄 Processing 20 file(s)...\n",
      "\n",
      "=== Processing: fusion_types.py ===\n",
      "File size: 385 characters\n",
      "Found 2 semantic units\n",
      "  - import_statement: import_from_enum (4 tokens)\n",
      "    Preview: from enum import Enum...\n",
      "  - class_definition: Types (112 tokens)\n",
      "    Preview: class Types(Enum):     \"\"\"Fusion types.      Args:         Enum (class: `enum.Enum`): Enum inheritan...\n",
      "Created 1 semantic chunks\n",
      "Final result: 1 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for fusion_types.py ---\n",
      "1. complete_module_2_parts\n",
      "   Type: complete_module\n",
      "   Size: 117 tokens, 22 lines\n",
      "   Content preview:\n",
      "     from enum import Enum\n",
      "     \n",
      "     class Types(Enum):\n",
      "     ... (19 more lines)\n",
      "\n",
      "\n",
      "=== Processing: report_attributes.py ===\n",
      "File size: 7972 characters\n",
      "Found 6 semantic units\n",
      "  - import_statement: import_from_dataclasses (8 tokens)\n",
      "    Preview: from dataclasses import dataclass, field...\n",
      "  - import_statement: import_from_typing (12 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any, Union, cast...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_from_fusion.utils (23 tokens)\n",
      "    Preview: from fusion.utils import (     CamelCaseMeta,     camel_to_snake,     requests_raise_for_status, )...\n",
      "  - decorated_definition: decorated_ReportAttribute (441 tokens)\n",
      "    Preview: @dataclass class ReportAttribute(metaclass=CamelCaseMeta):     title: str     sourceIdentifier: str...\n",
      "  - decorated_definition: decorated_ReportAttributes (1209 tokens)\n",
      "    Preview: @dataclass class ReportAttributes:     attributes: list[ReportAttribute] = field(default_factory=lis...\n",
      "Created 3 semantic chunks\n",
      "  Sub-chunking decorated_ReportAttributes (1209 tokens)\n",
      "    Breaking down decorated_ReportAttributes (1209 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 4 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for report_attributes.py ---\n",
      "1. imports_4_statements\n",
      "   Type: imports_group\n",
      "   Size: 50 tokens, 8 lines\n",
      "   Content preview:\n",
      "     from dataclasses import dataclass, field\n",
      "     from typing import TYPE_CHECKING, Any, Union, cast\n",
      "     import pandas as pd\n",
      "     ... (5 more lines)\n",
      "\n",
      "2. decorated_ReportAttribute\n",
      "   Type: decorated_definition\n",
      "   Size: 441 tokens, 52 lines\n",
      "   Content preview:\n",
      "     @dataclass\n",
      "     class ReportAttribute(metaclass=CamelCaseMeta):\n",
      "     title: str\n",
      "     ... (49 more lines)\n",
      "\n",
      "  3. decorated_ReportAttributes_part_1\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1023 tokens, 140 lines\n",
      "     Content preview:\n",
      "       @dataclass\n",
      "       class ReportAttributes:\n",
      "       attributes: list[ReportAttribute] = field(default_factory=list)\n",
      "       ... (137 more lines)\n",
      "\n",
      "  4. decorated_ReportAttributes_part_2\n",
      "     Type: decorated_definition_part\n",
      "     Size: 186 tokens, 22 lines\n",
      "     Content preview:\n",
      "       ) -> requests.Response | None:\n",
      "       \"\"\"\n",
      "       Create the ReportAttributes to the core-lineage API.\n",
      "       ... (19 more lines)\n",
      "\n",
      "\n",
      "=== Processing: fusion_filesystem.py ===\n",
      "File size: 40652 characters\n",
      "Found 24 semantic units\n",
      "  - import_statement: import_import_asyncio (2 tokens)\n",
      "    Preview: import asyncio...\n",
      "  - import_statement: import_import_base64 (3 tokens)\n",
      "    Preview: import base64...\n",
      "  - import_statement: import_import_hashlib (2 tokens)\n",
      "    Preview: import hashlib...\n",
      "  - import_statement: import_import_io (2 tokens)\n",
      "    Preview: import io...\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_import_time (2 tokens)\n",
      "    Preview: import time...\n",
      "  - import_statement: import_from_collections.abc (9 tokens)\n",
      "    Preview: from collections.abc import AsyncGenerator, Generator...\n",
      "  - import_statement: import_from_copy (4 tokens)\n",
      "    Preview: from copy import deepcopy...\n",
      "  - import_statement: import_from_pathlib (4 tokens)\n",
      "    Preview: from pathlib import Path...\n",
      "  - import_statement: import_from_typing (8 tokens)\n",
      "    Preview: from typing import Any, Optional, Union...\n",
      "  - import_statement: import_from_urllib.parse (8 tokens)\n",
      "    Preview: from urllib.parse import quote, urljoin...\n",
      "  - import_statement: import_import_aiohttp (3 tokens)\n",
      "    Preview: import aiohttp...\n",
      "  - import_statement: import_import_fsspec (4 tokens)\n",
      "    Preview: import fsspec...\n",
      "  - import_statement: import_import_fsspec.asyn (6 tokens)\n",
      "    Preview: import fsspec.asyn...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_import_requests (2 tokens)\n",
      "    Preview: import requests...\n",
      "  - import_statement: import_from_fsspec.callbacks (9 tokens)\n",
      "    Preview: from fsspec.callbacks import _DEFAULT_CALLBACK...\n",
      "  - import_statement: import_from_fsspec.implementations.http (19 tokens)\n",
      "    Preview: from fsspec.implementations.http import HTTPFile, HTTPFileSystem, sync, sync_wrapper...\n",
      "  - import_statement: import_from_fsspec.utils (8 tokens)\n",
      "    Preview: from fsspec.utils import nullcontext...\n",
      "  - import_statement: import_from_fusion._fusion (7 tokens)\n",
      "    Preview: from fusion._fusion import FusionCredentials...\n",
      "  - import_statement: import_from_fusion.exceptions (7 tokens)\n",
      "    Preview: from fusion.exceptions import APIResponseError...\n",
      "  - import_statement: import_from_.utils (16 tokens)\n",
      "    Preview: from .utils import cpu_count, get_client, get_default_fs, get_session...\n",
      "  - class_definition: FusionHTTPFileSystem (8357 tokens)\n",
      "    Preview: class FusionHTTPFileSystem(HTTPFileSystem):  # type: ignore     \"\"\"Fusion HTTP filesystem.\"\"\"      d...\n",
      "  - class_definition: FusionFile (431 tokens)\n",
      "    Preview: class FusionFile(HTTPFile):  # type: ignore     \"\"\"Fusion File.\"\"\"      def __init__(self, *args: An...\n",
      "Created 3 semantic chunks\n",
      "  Sub-chunking FusionHTTPFileSystem (8357 tokens)\n",
      "    Breaking down FusionHTTPFileSystem (8357 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 11 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for fusion_filesystem.py ---\n",
      "1. imports_22_statements\n",
      "   Type: imports_group\n",
      "   Size: 152 tokens, 22 lines\n",
      "   Content preview:\n",
      "     import asyncio\n",
      "     import base64\n",
      "     import hashlib\n",
      "     ... (19 more lines)\n",
      "\n",
      "  2. FusionHTTPFileSystem_part_1\n",
      "     Type: class_definition_part\n",
      "     Size: 1019 tokens, 119 lines\n",
      "     Content preview:\n",
      "       class FusionHTTPFileSystem(HTTPFileSystem):  # type: ignore\n",
      "       \"\"\"Fusion HTTP filesystem.\"\"\"\n",
      "       \n",
      "       ... (116 more lines)\n",
      "\n",
      "  3. FusionHTTPFileSystem_part_2\n",
      "     Type: class_definition_part\n",
      "     Size: 1001 tokens, 122 lines\n",
      "     Content preview:\n",
      "       except BaseException:\n",
      "       logger.exception(VERBOSE_LVL, f\"{url} cannot be parsed to json\")\n",
      "       out = {}\n",
      "       ... (119 more lines)\n",
      "\n",
      "  4. FusionHTTPFileSystem_part_3\n",
      "     Type: class_definition_part\n",
      "     Size: 995 tokens, 135 lines\n",
      "     Content preview:\n",
      "       k[\"name\"] = k[\"name\"].split(f\"{self.client_kwargs['root_url']}catalogs/\")[-1]\n",
      "       \n",
      "       elif not keep_protocol:\n",
      "       ... (132 more lines)\n",
      "\n",
      "  5. FusionHTTPFileSystem_part_4\n",
      "     Type: class_definition_part\n",
      "     Size: 1011 tokens, 129 lines\n",
      "     Content preview:\n",
      "       except Exception as ex:  # noqa: BLE001, PERF203\n",
      "       if attempt < retries - 1:\n",
      "       wait_time = 2**attempt  # Exponential backoff\n",
      "       ... (126 more lines)\n",
      "\n",
      "  6. FusionHTTPFileSystem_part_5\n",
      "     Type: class_definition_part\n",
      "     Size: 1008 tokens, 112 lines\n",
      "     Content preview:\n",
      "       **kwargs: Any,\n",
      "       ) -> Any:\n",
      "       \"\"\"Download file(s) from remote to local.\n",
      "       ... (109 more lines)\n",
      "\n",
      "  7. FusionHTTPFileSystem_part_6\n",
      "     Type: class_definition_part\n",
      "     Size: 997 tokens, 109 lines\n",
      "     Content preview:\n",
      "       additional_headers: Optional[dict[str, str]] = None,\n",
      "       **kwargs: Any,\n",
      "       ) -> None:\n",
      "       ... (106 more lines)\n",
      "\n",
      "  8. FusionHTTPFileSystem_part_7\n",
      "     Type: class_definition_part\n",
      "     Size: 1006 tokens, 105 lines\n",
      "     Content preview:\n",
      "       headers[\"Digest\"] = \"SHA-256=\" + base64.b64encode(hash_sha256.digest()).decode()\n",
      "       else:\n",
      "       headers[\"Digest\"] = \"SHA-256=\" + base64.b64encode(hash_sha256_chunk.digest()).decode()\n",
      "       ... (102 more lines)\n",
      "\n",
      "  9. FusionHTTPFileSystem_part_8\n",
      "     Type: class_definition_part\n",
      "     Size: 1004 tokens, 128 lines\n",
      "     Content preview:\n",
      "       resps = list(put_data())\n",
      "       hash_sha256 = hash_sha256_lst[0]\n",
      "       headers[\"Digest\"] = \"SHA-256=\" + base64.b64encode(hash_sha256.digest()).decode()\n",
      "       ... (125 more lines)\n",
      "\n",
      "  10. FusionHTTPFileSystem_part_9\n",
      "     Type: class_definition_part\n",
      "     Size: 315 tokens, 46 lines\n",
      "     Content preview:\n",
      "       block_size: Optional[int] = None,\n",
      "       _autocommit: Optional[bool] = None,\n",
      "       cache_type: None = None,\n",
      "       ... (43 more lines)\n",
      "\n",
      "11. FusionFile\n",
      "   Type: class_definition\n",
      "   Size: 431 tokens, 48 lines\n",
      "   Content preview:\n",
      "     class FusionFile(HTTPFile):  # type: ignore\n",
      "     \"\"\"Fusion File.\"\"\"\n",
      "     \n",
      "     ... (45 more lines)\n",
      "\n",
      "\n",
      "=== Processing: attributes.py ===\n",
      "File size: 38138 characters\n",
      "Found 8 semantic units\n",
      "  - import_statement: import_from_dataclasses (10 tokens)\n",
      "    Preview: from dataclasses import dataclass, field, fields...\n",
      "  - import_statement: import_from_typing (10 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any, cast...\n",
      "  - import_statement: import_import_numpy (4 tokens)\n",
      "    Preview: import numpy as np...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_from_fusion.fusion_types (7 tokens)\n",
      "    Preview: from fusion.fusion_types import Types...\n",
      "  - import_statement: import_from_fusion.utils (42 tokens)\n",
      "    Preview: from fusion.utils import (     CamelCaseMeta,     camel_to_snake,     convert_date_format,     make_...\n",
      "  - decorated_definition: decorated_Attribute (4324 tokens)\n",
      "    Preview: @dataclass class Attribute(metaclass=CamelCaseMeta):     \"\"\"Fusion Attribute class for managing attr...\n",
      "  - decorated_definition: decorated_Attributes (3171 tokens)\n",
      "    Preview: @dataclass class Attributes:     \"\"\"Class representing a collection of Attribute instances for manag...\n",
      "Created 3 semantic chunks\n",
      "  Sub-chunking decorated_Attribute (4324 tokens)\n",
      "    Breaking down decorated_Attribute (4324 tokens) into smaller pieces...\n",
      "  Sub-chunking decorated_Attributes (3171 tokens)\n",
      "    Breaking down decorated_Attributes (3171 tokens) into smaller pieces...\n",
      "Sub-chunked 2 oversized chunks\n",
      "Final result: 10 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for attributes.py ---\n",
      "1. imports_6_statements\n",
      "   Type: imports_group\n",
      "   Size: 82 tokens, 14 lines\n",
      "   Content preview:\n",
      "     from dataclasses import dataclass, field, fields\n",
      "     from typing import TYPE_CHECKING, Any, cast\n",
      "     import numpy as np\n",
      "     ... (11 more lines)\n",
      "\n",
      "  2. decorated_Attribute_part_1\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1020 tokens, 78 lines\n",
      "     Content preview:\n",
      "       @dataclass\n",
      "       class Attribute(metaclass=CamelCaseMeta):\n",
      "       \"\"\"Fusion Attribute class for managing attributes metadata in a Fusion catalog.\n",
      "       ... (75 more lines)\n",
      "\n",
      "  3. decorated_Attribute_part_2\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1013 tokens, 116 lines\n",
      "     Content preview:\n",
      "       self.available_from = convert_date_format(self.available_from) if self.available_from else None\n",
      "       self.deprecated_from = convert_date_format(self.deprecated_from) if self.deprecated_from else None\n",
      "       self.data_type = Types[str(self.data_type).strip().rsplit(\".\", maxsplit=1)[-1].title()]\n",
      "       ... (113 more lines)\n",
      "\n",
      "  4. decorated_Attribute_part_3\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1009 tokens, 130 lines\n",
      "     Content preview:\n",
      "       multiplier=series.get(\"multiplier\", 1.0),\n",
      "       is_propagation_eligible=is_propagation_eligible,\n",
      "       is_metric=is_metric,\n",
      "       ... (127 more lines)\n",
      "\n",
      "  5. decorated_Attribute_part_4\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1002 tokens, 114 lines\n",
      "     Content preview:\n",
      "       If instantiated from a Fusion object, then the client is set automatically.\n",
      "       catalog (str, optional): A catalog identifier. Defaults to None.\n",
      "       return_resp_obj (bool, optional): If True then return the response object. Defaults to False.\n",
      "       ... (111 more lines)\n",
      "\n",
      "  6. decorated_Attribute_part_5\n",
      "     Type: decorated_definition_part\n",
      "     Size: 280 tokens, 32 lines\n",
      "     Content preview:\n",
      "       >>> my_attr2 = fusion.attribute(identifier=\"my_attribute2\", index=0, application_id=\"12345\")\n",
      "       >>> my_attr3 = fusion.attribute(identifier=\"my_attribute3\", index=0, application_id=\"12345\")\n",
      "       >>> attrs = [my_attr1, my_attr2]\n",
      "       ... (29 more lines)\n",
      "\n",
      "  7. decorated_Attributes_part_1\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1020 tokens, 164 lines\n",
      "     Content preview:\n",
      "       @dataclass\n",
      "       class Attributes:\n",
      "       \"\"\"Class representing a collection of Attribute instances for managing atrribute metadata in a Fusion catalog.\n",
      "       ... (161 more lines)\n",
      "\n",
      "  8. decorated_Attributes_part_2\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1006 tokens, 135 lines\n",
      "     Content preview:\n",
      "       @classmethod\n",
      "       def _from_dataframe(cls: type[Attributes], data: pd.DataFrame) -> Attributes:\n",
      "       \"\"\"Create an Attributes instance from a pandas DataFrame.\n",
      "       ... (132 more lines)\n",
      "\n",
      "  9. decorated_Attributes_part_3\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1009 tokens, 124 lines\n",
      "     Content preview:\n",
      "       response = client.session.get(url)\n",
      "       requests_raise_for_status(response)\n",
      "       list_attributes = response.json()[\"resources\"]\n",
      "       ... (121 more lines)\n",
      "\n",
      "  10. decorated_Attributes_part_4\n",
      "     Type: decorated_definition_part\n",
      "     Size: 136 tokens, 21 lines\n",
      "     Content preview:\n",
      "       otherwise None.\n",
      "       \n",
      "       Examples:\n",
      "       ... (18 more lines)\n",
      "\n",
      "\n",
      "=== Processing: __init__.py ===\n",
      "File size: 364 characters\n",
      "Found 4 semantic units\n",
      "  - import_statement: import_from_fusion._fusion (7 tokens)\n",
      "    Preview: from fusion._fusion import FusionCredentials...\n",
      "  - import_statement: import_from_fusion.fs_sync (7 tokens)\n",
      "    Preview: from fusion.fs_sync import fsync...\n",
      "  - import_statement: import_from_fusion.fusion (6 tokens)\n",
      "    Preview: from fusion.fusion import Fusion...\n",
      "  - import_statement: import_from_._fusion (5 tokens)\n",
      "    Preview: from ._fusion import *...\n",
      "Created 1 semantic chunks\n",
      "Final result: 1 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for __init__.py ---\n",
      "1. complete_module_4_parts\n",
      "   Type: complete_module\n",
      "   Size: 28 tokens, 7 lines\n",
      "   Content preview:\n",
      "     from fusion._fusion import FusionCredentials\n",
      "     \n",
      "     from fusion.fs_sync import fsync\n",
      "     ... (4 more lines)\n",
      "\n",
      "\n",
      "=== Processing: types.py ===\n",
      "File size: 163 characters\n",
      "Found 2 semantic units\n",
      "  - import_statement: import_from_queue (4 tokens)\n",
      "    Preview: from queue import Queue...\n",
      "  - import_statement: import_from_typing (6 tokens)\n",
      "    Preview: from typing import Any, Union...\n",
      "Created 1 semantic chunks\n",
      "Final result: 1 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for types.py ---\n",
      "1. complete_module_2_parts\n",
      "   Type: complete_module\n",
      "   Size: 11 tokens, 3 lines\n",
      "   Content preview:\n",
      "     from queue import Queue\n",
      "     \n",
      "     from typing import Any, Union\n",
      "\n",
      "\n",
      "=== Processing: fs_sync.py ===\n",
      "File size: 13303 characters\n",
      "Found 22 semantic units\n",
      "  - import_statement: import_import_base64 (3 tokens)\n",
      "    Preview: import base64...\n",
      "  - import_statement: import_import_hashlib (2 tokens)\n",
      "    Preview: import hashlib...\n",
      "  - import_statement: import_import_json (2 tokens)\n",
      "    Preview: import json...\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_import_sys (2 tokens)\n",
      "    Preview: import sys...\n",
      "  - import_statement: import_import_time (2 tokens)\n",
      "    Preview: import time...\n",
      "  - import_statement: import_import_warnings (2 tokens)\n",
      "    Preview: import warnings...\n",
      "  - import_statement: import_from_os.path (6 tokens)\n",
      "    Preview: from os.path import relpath...\n",
      "  - import_statement: import_from_pathlib (4 tokens)\n",
      "    Preview: from pathlib import Path...\n",
      "  - import_statement: import_from_typing (4 tokens)\n",
      "    Preview: from typing import Optional...\n",
      "  - import_statement: import_import_fsspec (4 tokens)\n",
      "    Preview: import fsspec...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_from_rich.progress (5 tokens)\n",
      "    Preview: from rich.progress import Progress...\n",
      "  - import_statement: import_from_.utils (34 tokens)\n",
      "    Preview: from .utils import (     cpu_count,     distribution_to_filename,     is_dataset_raw,     path_to_ur...\n",
      "  - function_definition: _url_to_path (69 tokens)\n",
      "    Preview: def _url_to_path(x: str) -> str:     file_name = distribution_to_filename(\"\", x.split(\"/\")[2], x.spl...\n",
      "  - function_definition: _download (217 tokens)\n",
      "    Preview: def _download(     fs_fusion: fsspec.filesystem,     fs_local: fsspec.filesystem,     df: pd.DataFra...\n",
      "  - function_definition: _upload (131 tokens)\n",
      "    Preview: def _upload(     fs_fusion: fsspec.filesystem,     fs_local: fsspec.filesystem,     df: pd.DataFrame...\n",
      "  - function_definition: _generate_sha256_token (163 tokens)\n",
      "    Preview: def _generate_sha256_token(path: str, fs: fsspec.filesystem, chunk_size: int = 5 * 2**20) -> str:...\n",
      "  - function_definition: _get_fusion_df (425 tokens)\n",
      "    Preview: def _get_fusion_df(     fs_fusion: fsspec.filesystem,     datasets_lst: list[str],     catalog: str,...\n",
      "  - function_definition: _get_local_state (555 tokens)\n",
      "    Preview: def _get_local_state(     fs_local: fsspec.filesystem,     fs_fusion: fsspec.filesystem,     dataset...\n",
      "  - function_definition: _synchronize (410 tokens)\n",
      "    Preview: def _synchronize(  # noqa: PLR0913     fs_fusion: fsspec.filesystem,     fs_local: fsspec.filesystem...\n",
      "  - function_definition: fsync (1050 tokens)\n",
      "    Preview: def fsync(  # noqa: PLR0912, PLR0913, PLR0915     fs_fusion: fsspec.filesystem,     fs_local: fsspec...\n",
      "Created 9 semantic chunks\n",
      "  Sub-chunking fsync (1050 tokens)\n",
      "    Breaking down fsync (1050 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 10 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for fs_sync.py ---\n",
      "1. imports_14_statements\n",
      "   Type: imports_group\n",
      "   Size: 89 tokens, 21 lines\n",
      "   Content preview:\n",
      "     import base64\n",
      "     import hashlib\n",
      "     import json\n",
      "     ... (18 more lines)\n",
      "\n",
      "2. _url_to_path\n",
      "   Type: function_definition\n",
      "   Size: 69 tokens, 3 lines\n",
      "   Content preview:\n",
      "     def _url_to_path(x: str) -> str:\n",
      "     file_name = distribution_to_filename(\"\", x.split(\"/\")[2], x.split(\"/\")[4], x.split(\"/\")[6], x.split(\"/\")[0])\n",
      "     return f\"{x.split('/')[0]}/{x.split('/')[2]}/{x.split('/')[4]}/{file_name}\"\n",
      "\n",
      "3. _download\n",
      "   Type: function_definition\n",
      "   Size: 217 tokens, 25 lines\n",
      "   Content preview:\n",
      "     def _download(\n",
      "     fs_fusion: fsspec.filesystem,\n",
      "     fs_local: fsspec.filesystem,\n",
      "     ... (22 more lines)\n",
      "\n",
      "4. _upload\n",
      "   Type: function_definition\n",
      "   Size: 131 tokens, 18 lines\n",
      "   Content preview:\n",
      "     def _upload(\n",
      "     fs_fusion: fsspec.filesystem,\n",
      "     fs_local: fsspec.filesystem,\n",
      "     ... (15 more lines)\n",
      "\n",
      "5. _generate_sha256_token\n",
      "   Type: function_definition\n",
      "   Size: 163 tokens, 16 lines\n",
      "   Content preview:\n",
      "     def _generate_sha256_token(path: str, fs: fsspec.filesystem, chunk_size: int = 5 * 2**20) -> str:\n",
      "     hash_sha256 = hashlib.sha256()\n",
      "     hash_sha256_chunk = hashlib.sha256()\n",
      "     ... (13 more lines)\n",
      "\n",
      "6. _get_fusion_df\n",
      "   Type: function_definition\n",
      "   Size: 425 tokens, 36 lines\n",
      "   Content preview:\n",
      "     def _get_fusion_df(\n",
      "     fs_fusion: fsspec.filesystem,\n",
      "     datasets_lst: list[str],\n",
      "     ... (33 more lines)\n",
      "\n",
      "7. _get_local_state\n",
      "   Type: function_definition\n",
      "   Size: 555 tokens, 46 lines\n",
      "   Content preview:\n",
      "     def _get_local_state(\n",
      "     fs_local: fsspec.filesystem,\n",
      "     fs_fusion: fsspec.filesystem,\n",
      "     ... (43 more lines)\n",
      "\n",
      "8. _synchronize\n",
      "   Type: function_definition\n",
      "   Size: 410 tokens, 48 lines\n",
      "   Content preview:\n",
      "     def _synchronize(  # noqa: PLR0913\n",
      "     fs_fusion: fsspec.filesystem,\n",
      "     fs_local: fsspec.filesystem,\n",
      "     ... (45 more lines)\n",
      "\n",
      "  9. fsync_part_1\n",
      "     Type: function_definition_part\n",
      "     Size: 1000 tokens, 119 lines\n",
      "     Content preview:\n",
      "       def fsync(  # noqa: PLR0912, PLR0913, PLR0915\n",
      "       fs_fusion: fsspec.filesystem,\n",
      "       fs_local: fsspec.filesystem,\n",
      "       ... (116 more lines)\n",
      "\n",
      "  10. fsync_part_2\n",
      "     Type: function_definition_part\n",
      "     Size: 50 tokens, 8 lines\n",
      "     Content preview:\n",
      "       except KeyboardInterrupt:  # noqa: PERF203\n",
      "       if input(\"Type exit to exit: \") != \"exit\":\n",
      "       continue\n",
      "       ... (5 more lines)\n",
      "\n",
      "\n",
      "=== Processing: dataset.py ===\n",
      "File size: 33669 characters\n",
      "Found 6 semantic units\n",
      "  - import_statement: import_import_json (4 tokens)\n",
      "    Preview: import json as js...\n",
      "  - import_statement: import_from_dataclasses (10 tokens)\n",
      "    Preview: from dataclasses import dataclass, field, fields...\n",
      "  - import_statement: import_from_typing (8 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_from_.utils (51 tokens)\n",
      "    Preview: from .utils import (     CamelCaseMeta,     _is_json,     camel_to_snake,     convert_date_format,...\n",
      "  - decorated_definition: decorated_Dataset (7058 tokens)\n",
      "    Preview: @dataclass class Dataset(metaclass=CamelCaseMeta):     \"\"\"Fusion Dataset class for managing dataset...\n",
      "Created 2 semantic chunks\n",
      "  Sub-chunking decorated_Dataset (7058 tokens)\n",
      "    Breaking down decorated_Dataset (7058 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 8 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for dataset.py ---\n",
      "1. imports_5_statements\n",
      "   Type: imports_group\n",
      "   Size: 81 tokens, 15 lines\n",
      "   Content preview:\n",
      "     import json as js\n",
      "     from dataclasses import dataclass, field, fields\n",
      "     from typing import TYPE_CHECKING, Any\n",
      "     ... (12 more lines)\n",
      "\n",
      "  2. decorated_Dataset_part_1\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1000 tokens, 66 lines\n",
      "     Content preview:\n",
      "       @dataclass\n",
      "       class Dataset(metaclass=CamelCaseMeta):\n",
      "       \"\"\"Fusion Dataset class for managing dataset metadata in a Fusion catalog.\n",
      "       ... (63 more lines)\n",
      "\n",
      "  3. decorated_Dataset_part_2\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1028 tokens, 105 lines\n",
      "     Content preview:\n",
      "       delivery_channel: str | list[str] = field(default_factory=lambda: [\"API\"])\n",
      "       language: str = \"English\"\n",
      "       status: str = \"Available\"\n",
      "       ... (102 more lines)\n",
      "\n",
      "  4. decorated_Dataset_part_3\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1004 tokens, 94 lines\n",
      "     Content preview:\n",
      "       \"\"\"Determine client.\"\"\"\n",
      "       \n",
      "       res = self._client if client is None else client\n",
      "       ... (91 more lines)\n",
      "\n",
      "  5. decorated_Dataset_part_4\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1000 tokens, 126 lines\n",
      "     Content preview:\n",
      "       def _from_dict(cls: type[Dataset], data: dict[str, Any]) -> Dataset:\n",
      "       \"\"\"Instantiate a Dataset object from a dictionary.\n",
      "       \n",
      "       ... (123 more lines)\n",
      "\n",
      "  6. decorated_Dataset_part_5\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1018 tokens, 134 lines\n",
      "     Content preview:\n",
      "       raise TypeError(f\"Could not resolve the object provided: {dataset_source}\")\n",
      "       \n",
      "       dataset.client = self._client\n",
      "       ... (131 more lines)\n",
      "\n",
      "  7. decorated_Dataset_part_6\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1005 tokens, 122 lines\n",
      "     Content preview:\n",
      "       >>> from fusion import Fusion\n",
      "       >>> fusion = Fusion()\n",
      "       >>> dataset_series = pd.Series({\n",
      "       ... (119 more lines)\n",
      "\n",
      "  8. decorated_Dataset_part_7\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1001 tokens, 119 lines\n",
      "     Content preview:\n",
      "       catalog_to (str): A catalog identifier to which to copy dataset.\n",
      "       catalog_from (str, optional): A catalog identifier from which to copy dataset. Defaults to \"common\".\n",
      "       client (Fusion, optional): A Fusion client object. Defaults to the instance's _client.\n",
      "       ... (116 more lines)\n",
      "\n",
      "\n",
      "=== Processing: embeddings.py ===\n",
      "File size: 32521 characters\n",
      "Found 26 semantic units\n",
      "  - import_statement: import_import_asyncio (2 tokens)\n",
      "    Preview: import asyncio...\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_import_ssl (2 tokens)\n",
      "    Preview: import ssl...\n",
      "  - import_statement: import_import_time (2 tokens)\n",
      "    Preview: import time...\n",
      "  - import_statement: import_import_warnings (2 tokens)\n",
      "    Preview: import warnings...\n",
      "  - import_statement: import_from_collections.abc (8 tokens)\n",
      "    Preview: from collections.abc import Collection, Mapping...\n",
      "  - import_statement: import_from_pathlib (4 tokens)\n",
      "    Preview: from pathlib import Path...\n",
      "  - import_statement: import_from_typing (8 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any...\n",
      "  - import_statement: import_import_requests (2 tokens)\n",
      "    Preview: import requests...\n",
      "  - import_statement: import_from_aiohttp (6 tokens)\n",
      "    Preview: from aiohttp import ClientTimeout...\n",
      "  - import_statement: import_from_opensearchpy._async._extra_imports (21 tokens)\n",
      "    Preview: from opensearchpy._async._extra_imports import aiohttp, aiohttp_exceptions, yarl...\n",
      "  - import_statement: import_from_opensearchpy._async.compat (12 tokens)\n",
      "    Preview: from opensearchpy._async.compat import get_running_loop...\n",
      "  - import_statement: import_from_opensearchpy._async.http_aiohttp (16 tokens)\n",
      "    Preview: from opensearchpy._async.http_aiohttp import AIOHttpConnection...\n",
      "  - import_statement: import_from_opensearchpy.compat (16 tokens)\n",
      "    Preview: from opensearchpy.compat import reraise_exceptions, string_types, urlencode...\n",
      "  - import_statement: import_from_opensearchpy.connection.base (9 tokens)\n",
      "    Preview: from opensearchpy.connection.base import Connection...\n",
      "  - import_statement: import_from_opensearchpy.exceptions (18 tokens)\n",
      "    Preview: from opensearchpy.exceptions import (     ConnectionError as OpenSearchConnectionError, )...\n",
      "  - import_statement: import_from_opensearchpy.exceptions (24 tokens)\n",
      "    Preview: from opensearchpy.exceptions import (     ConnectionTimeout,     ImproperlyConfigured,     SSLError,...\n",
      "  - import_statement: import_from_opensearchpy.metrics (11 tokens)\n",
      "    Preview: from opensearchpy.metrics import Metrics, MetricsNone...\n",
      "  - import_statement: import_from_fusion._fusion (7 tokens)\n",
      "    Preview: from fusion._fusion import FusionCredentials...\n",
      "  - import_statement: import_from_fusion.embeddings_utils (33 tokens)\n",
      "    Preview: from fusion.embeddings_utils import (     _modify_post_haystack,     _modify_post_response_langchain...\n",
      "  - import_statement: import_from_fusion.utils (9 tokens)\n",
      "    Preview: from fusion.utils import get_client, get_session...\n",
      "  - if_statement: if_block (25 tokens)\n",
      "    Preview: if TYPE_CHECKING:     from collections.abc import Collection, Mapping      from fusion.authenticatio...\n",
      "  - class_definition: FusionEmbeddingsConnection (2534 tokens)\n",
      "    Preview: class FusionEmbeddingsConnection(Connection):  # type: ignore     \"\"\"     Class responsible for main...\n",
      "  - class_definition: FusionAsyncHttpConnection (3228 tokens)\n",
      "    Preview: class FusionAsyncHttpConnection(AIOHttpConnection):  # type: ignore     session: FusionAiohttpSessio...\n",
      "  - function_definition: format_index_body (278 tokens)\n",
      "    Preview: def format_index_body(number_of_shards: int = 1, number_of_replicas: int = 1, dimension: int = 1536)...\n",
      "  - class_definition: PromptTemplateManager (483 tokens)\n",
      "    Preview: class PromptTemplateManager:     \"\"\"Class to manage prompt templates for different packages and task...\n",
      "Created 6 semantic chunks\n",
      "  Sub-chunking FusionEmbeddingsConnection (2534 tokens)\n",
      "    Breaking down FusionEmbeddingsConnection (2534 tokens) into smaller pieces...\n",
      "  Sub-chunking FusionAsyncHttpConnection (3228 tokens)\n",
      "    Breaking down FusionAsyncHttpConnection (3228 tokens) into smaller pieces...\n",
      "Sub-chunked 2 oversized chunks\n",
      "Final result: 11 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for embeddings.py ---\n",
      "1. imports_21_statements\n",
      "   Type: imports_group\n",
      "   Size: 231 tokens, 31 lines\n",
      "   Content preview:\n",
      "     import asyncio\n",
      "     import logging\n",
      "     import ssl\n",
      "     ... (28 more lines)\n",
      "\n",
      "2. if_block\n",
      "   Type: if_statement\n",
      "   Size: 25 tokens, 4 lines\n",
      "   Content preview:\n",
      "     if TYPE_CHECKING:\n",
      "     from collections.abc import Collection, Mapping\n",
      "     \n",
      "     ... (1 more lines)\n",
      "\n",
      "  3. FusionEmbeddingsConnection_part_1\n",
      "     Type: class_definition_part\n",
      "     Size: 1011 tokens, 82 lines\n",
      "     Content preview:\n",
      "       class FusionEmbeddingsConnection(Connection):  # type: ignore\n",
      "       \"\"\"\n",
      "       Class responsible for maintaining HTTP connection to the Fusion Embedding API using OpenSearch. This class is a\n",
      "       ... (79 more lines)\n",
      "\n",
      "  4. FusionEmbeddingsConnection_part_2\n",
      "     Type: class_definition_part\n",
      "     Size: 1014 tokens, 119 lines\n",
      "     Content preview:\n",
      "       for key in list(self.session.headers):\n",
      "       self.session.headers.pop(key)\n",
      "       \n",
      "       ... (116 more lines)\n",
      "\n",
      "  5. FusionEmbeddingsConnection_part_3\n",
      "     Type: class_definition_part\n",
      "     Size: 508 tokens, 78 lines\n",
      "     Content preview:\n",
      "       settings = self.session.merge_environment_settings(prepared_request.url, {}, None, None, None)\n",
      "       send_kwargs: Any = {\n",
      "       \"timeout\": timeout or self.timeout,\n",
      "       ... (75 more lines)\n",
      "\n",
      "  6. FusionAsyncHttpConnection_part_1\n",
      "     Type: class_definition_part\n",
      "     Size: 1008 tokens, 75 lines\n",
      "     Content preview:\n",
      "       class FusionAsyncHttpConnection(AIOHttpConnection):  # type: ignore\n",
      "       session: FusionAiohttpSession | None\n",
      "       \n",
      "       ... (72 more lines)\n",
      "\n",
      "  7. FusionAsyncHttpConnection_part_2\n",
      "     Type: class_definition_part\n",
      "     Size: 1023 tokens, 117 lines\n",
      "     Content preview:\n",
      "       credentials: FusionCredentials | str | None = kwargs.get(\"credentials\", \"config/client_credentials.json\")\n",
      "       if isinstance(credentials, FusionCredentials):\n",
      "       self.credentials = credentials\n",
      "       ... (114 more lines)\n",
      "\n",
      "  8. FusionAsyncHttpConnection_part_3\n",
      "     Type: class_definition_part\n",
      "     Size: 1024 tokens, 127 lines\n",
      "     Content preview:\n",
      "       def _remap_endpoints(url: str) -> str:\n",
      "       return url.replace(\"_bulk\", \"embeddings\").replace(\"_search\", \"search\")\n",
      "       \n",
      "       ... (124 more lines)\n",
      "\n",
      "  9. FusionAsyncHttpConnection_part_4\n",
      "     Type: class_definition_part\n",
      "     Size: 172 tokens, 24 lines\n",
      "     Content preview:\n",
      "       response=raw_data_modified,\n",
      "       )\n",
      "       self._raise_error(response.status, raw_data_modified)\n",
      "       ... (21 more lines)\n",
      "\n",
      "10. format_index_body\n",
      "   Type: function_definition\n",
      "   Size: 278 tokens, 30 lines\n",
      "   Content preview:\n",
      "     def format_index_body(number_of_shards: int = 1, number_of_replicas: int = 1, dimension: int = 1536) -> dict[str, Any]:\n",
      "     \"\"\"Format index body for index creation in Embeddings API.\n",
      "     \n",
      "     ... (27 more lines)\n",
      "\n",
      "11. PromptTemplateManager\n",
      "   Type: class_definition\n",
      "   Size: 483 tokens, 85 lines\n",
      "   Content preview:\n",
      "     class PromptTemplateManager:\n",
      "     \"\"\"Class to manage prompt templates for different packages and tasks.\"\"\"\n",
      "     \n",
      "     ... (82 more lines)\n",
      "\n",
      "\n",
      "=== Processing: utils.py ===\n",
      "File size: 30447 characters\n",
      "Found 61 semantic units\n",
      "  - import_statement: import_import_contextlib (3 tokens)\n",
      "    Preview: import contextlib...\n",
      "  - import_statement: import_import_json (4 tokens)\n",
      "    Preview: import json as js...\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_import_multiprocessing (4 tokens)\n",
      "    Preview: import multiprocessing as mp...\n",
      "  - import_statement: import_import_os (2 tokens)\n",
      "    Preview: import os...\n",
      "  - import_statement: import_import_re (2 tokens)\n",
      "    Preview: import re...\n",
      "  - import_statement: import_import_ssl (2 tokens)\n",
      "    Preview: import ssl...\n",
      "  - import_statement: import_import_zipfile (2 tokens)\n",
      "    Preview: import zipfile...\n",
      "  - import_statement: import_from_contextlib (6 tokens)\n",
      "    Preview: from contextlib import nullcontext...\n",
      "  - import_statement: import_from_datetime (6 tokens)\n",
      "    Preview: from datetime import date, datetime...\n",
      "  - import_statement: import_from_io (5 tokens)\n",
      "    Preview: from io import BytesIO...\n",
      "  - import_statement: import_from_pathlib (4 tokens)\n",
      "    Preview: from pathlib import Path...\n",
      "  - import_statement: import_from_typing (12 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any, Union, cast...\n",
      "  - import_statement: import_from_urllib.parse (9 tokens)\n",
      "    Preview: from urllib.parse import urlparse, urlunparse...\n",
      "  - import_statement: import_import_aiohttp (3 tokens)\n",
      "    Preview: import aiohttp...\n",
      "  - import_statement: import_import_certifi (3 tokens)\n",
      "    Preview: import certifi...\n",
      "  - import_statement: import_import_fsspec (4 tokens)\n",
      "    Preview: import fsspec...\n",
      "  - import_statement: import_import_joblib (3 tokens)\n",
      "    Preview: import joblib...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_import_pyarrow (5 tokens)\n",
      "    Preview: import pyarrow as pa...\n",
      "  - import_statement: import_import_pyarrow.parquet (7 tokens)\n",
      "    Preview: import pyarrow.parquet as pq...\n",
      "  - import_statement: import_import_requests (2 tokens)\n",
      "    Preview: import requests...\n",
      "  - import_statement: import_from_dateutil (5 tokens)\n",
      "    Preview: from dateutil import parser...\n",
      "  - import_statement: import_from_pyarrow (11 tokens)\n",
      "    Preview: from pyarrow import csv, json, unify_schemas...\n",
      "  - import_statement: import_from_pyarrow.parquet (9 tokens)\n",
      "    Preview: from pyarrow.parquet import filters_to_expression...\n",
      "  - import_statement: import_from_rich.progress (39 tokens)\n",
      "    Preview: from rich.progress import (     BarColumn,     MofNCompleteColumn,     Progress,     TaskProgressCol...\n",
      "  - import_statement: import_from_urllib3.util.retry (7 tokens)\n",
      "    Preview: from urllib3.util.retry import Retry...\n",
      "  - import_statement: import_from_.authentication (13 tokens)\n",
      "    Preview: from .authentication import FusionAiohttpSession, FusionOAuthAdapter...\n",
      "  - if_statement: if_block (31 tokens)\n",
      "    Preview: if TYPE_CHECKING:     from collections.abc import Generator      from fusion._fusion import FusionCr...\n",
      "  - function_definition: get_default_fs (144 tokens)\n",
      "    Preview: def get_default_fs() -> fsspec.filesystem:     \"\"\"Retrieve default filesystem.      Returns: filesys...\n",
      "  - decorated_definition: decorated_joblib_progress (302 tokens)\n",
      "    Preview: @contextlib.contextmanager def joblib_progress(description: str, total: int | None) -> Generator[Pro...\n",
      "  - function_definition: cpu_count (154 tokens)\n",
      "    Preview: def cpu_count(thread_pool_size: int | None = None, is_threading: bool = False) -> int:     \"\"\"Determ...\n",
      "  - function_definition: csv_to_table (181 tokens)\n",
      "    Preview: def csv_to_table(     path: str,     fs: fsspec.filesystem | None = None,     columns: list[str] | N...\n",
      "  - function_definition: json_to_table (178 tokens)\n",
      "    Preview: def json_to_table(     path: str,     fs: fsspec.filesystem | None = None,     columns: list[str] |...\n",
      "  - function_definition: parquet_to_table (251 tokens)\n",
      "    Preview: def parquet_to_table(     path: PathLikeT | list[PathLikeT],     fs: fsspec.filesystem | None = None...\n",
      "  - function_definition: read_csv (601 tokens)\n",
      "    Preview: def read_csv(  # noqa: PLR0912     path: str | zipfile.ZipFile,     columns: list[str] | None = None...\n",
      "  - function_definition: read_json (443 tokens)\n",
      "    Preview: def read_json(     path: str,     columns: list[str] | None = None,     filters: PyArrowFilterT | No...\n",
      "  - function_definition: read_parquet (235 tokens)\n",
      "    Preview: def read_parquet(     path: PathLikeT,     columns: list[str] | None = None,     filters: PyArrowFil...\n",
      "  - function_definition: _normalise_dt_param (250 tokens)\n",
      "    Preview: def _normalise_dt_param(dt: str | int | datetime | date) -> str:     \"\"\"Convert dates into a normali...\n",
      "  - function_definition: normalise_dt_param_str (146 tokens)\n",
      "    Preview: def normalise_dt_param_str(dt: str) -> tuple[str, ...]:     \"\"\"Convert a date parameter which may be...\n",
      "  - function_definition: distribution_to_filename (258 tokens)\n",
      "    Preview: def distribution_to_filename(     root_folder: str,     dataset: str,     datasetseries: str,     fi...\n",
      "  - function_definition: _filename_to_distribution (126 tokens)\n",
      "    Preview: def _filename_to_distribution(file_name: str) -> tuple[str, str, str, str]:     \"\"\"Breaks a filename...\n",
      "  - function_definition: distribution_to_url (295 tokens)\n",
      "    Preview: def distribution_to_url(     root_url: str,     dataset: str,     datasetseries: str,     file_forma...\n",
      "  - function_definition: _get_canonical_root_url (89 tokens)\n",
      "    Preview: def _get_canonical_root_url(any_url: str) -> str:     \"\"\"Get the full URL for the API endpoint....\n",
      "  - function_definition: get_client (234 tokens)\n",
      "    Preview: async def get_client(credentials: FusionCredentials, **kwargs: Any) -> FusionAiohttpSession:  # noqa...\n",
      "  - function_definition: get_session (237 tokens)\n",
      "    Preview: def get_session(     credentials: FusionCredentials, root_url: str, get_retries: int | Retry | None...\n",
      "  - function_definition: validate_file_names (196 tokens)\n",
      "    Preview: def validate_file_names(paths: list[str]) -> list[bool]:     \"\"\"Validate if the file name format adh...\n",
      "  - function_definition: is_dataset_raw (175 tokens)\n",
      "    Preview: def is_dataset_raw(paths: list[str], fs_fusion: fsspec.AbstractFileSystem) -> list[bool]:     \"\"\"Che...\n",
      "  - function_definition: path_to_url (148 tokens)\n",
      "    Preview: def path_to_url(x: str, is_raw: bool = False, is_download: bool = False) -> str:     \"\"\"Convert file...\n",
      "  - function_definition: upload_files (684 tokens)\n",
      "    Preview: def upload_files(  # noqa: PLR0913     fs_fusion: fsspec.AbstractFileSystem,     fs_local: fsspec.Ab...\n",
      "  - function_definition: camel_to_snake (60 tokens)\n",
      "    Preview: def camel_to_snake(name: str) -> str:     \"\"\"Convert camelCase to snake_case.\"\"\"     s1 = re.sub(re_...\n",
      "  - class_definition: CamelCaseMeta (253 tokens)\n",
      "    Preview: class CamelCaseMeta(type):     \"\"\"Metaclass to support both snake and camel case typing.\"\"\"      def...\n",
      "  - function_definition: snake_to_camel (49 tokens)\n",
      "    Preview: def snake_to_camel(name: str) -> str:     \"\"\"Convert snake_case to camelCase.\"\"\"     components = na...\n",
      "  - function_definition: tidy_string (71 tokens)\n",
      "    Preview: def tidy_string(x: str) -> str:     \"\"\"Tidy string.      Args:         x (str): String to tidy....\n",
      "  - function_definition: make_list (75 tokens)\n",
      "    Preview: def make_list(obj: Any) -> list[str]:     \"\"\"Make list.\"\"\"     if isinstance(obj, list):         lst...\n",
      "  - function_definition: make_bool (66 tokens)\n",
      "    Preview: def make_bool(obj: Any) -> bool:     \"\"\"Make boolean.\"\"\"     if isinstance(obj, str):         false_...\n",
      "  - function_definition: convert_date_format (57 tokens)\n",
      "    Preview: def convert_date_format(date_str: str) -> Any:     \"\"\"Convert date to YYYY-MM-DD format.\"\"\"     desi...\n",
      "  - function_definition: _is_json (48 tokens)\n",
      "    Preview: def _is_json(data: str) -> bool:     \"\"\"Check if the data is in JSON format.\"\"\"     try:         js....\n",
      "  - function_definition: requests_raise_for_status (54 tokens)\n",
      "    Preview: def requests_raise_for_status(response: requests.Response) -> None:     \"\"\"Send response text into r...\n",
      "  - function_definition: validate_file_formats (263 tokens)\n",
      "    Preview: def validate_file_formats(fs_fusion: fsspec.AbstractFileSystem, path: str) -> None:     \"\"\"     Vali...\n",
      "  - function_definition: file_name_to_url (216 tokens)\n",
      "    Preview: def file_name_to_url(     file_name: str, dataset: str, catalog: str, is_download: bool = False ) ->...\n",
      "Created 34 semantic chunks\n",
      "Final result: 34 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for utils.py ---\n",
      "1. imports_28_statements\n",
      "   Type: imports_group\n",
      "   Size: 204 tokens, 36 lines\n",
      "   Content preview:\n",
      "     import contextlib\n",
      "     import json as js\n",
      "     import logging\n",
      "     ... (33 more lines)\n",
      "\n",
      "2. if_block\n",
      "   Type: if_statement\n",
      "   Size: 31 tokens, 6 lines\n",
      "   Content preview:\n",
      "     if TYPE_CHECKING:\n",
      "     from collections.abc import Generator\n",
      "     \n",
      "     ... (3 more lines)\n",
      "\n",
      "3. get_default_fs\n",
      "   Type: function_definition\n",
      "   Size: 144 tokens, 18 lines\n",
      "   Content preview:\n",
      "     def get_default_fs() -> fsspec.filesystem:\n",
      "     \"\"\"Retrieve default filesystem.\n",
      "     \n",
      "     ... (15 more lines)\n",
      "\n",
      "4. decorated_joblib_progress\n",
      "   Type: decorated_definition\n",
      "   Size: 302 tokens, 37 lines\n",
      "   Content preview:\n",
      "     @contextlib.contextmanager\n",
      "     def joblib_progress(description: str, total: int | None) -> Generator[Progress, None, None]:\n",
      "     show_speed = not total\n",
      "     ... (34 more lines)\n",
      "\n",
      "5. cpu_count\n",
      "   Type: function_definition\n",
      "   Size: 154 tokens, 19 lines\n",
      "   Content preview:\n",
      "     def cpu_count(thread_pool_size: int | None = None, is_threading: bool = False) -> int:\n",
      "     \"\"\"Determine the number of cpus/threads for parallelization.\n",
      "     \n",
      "     ... (16 more lines)\n",
      "\n",
      "6. csv_to_table\n",
      "   Type: function_definition\n",
      "   Size: 181 tokens, 25 lines\n",
      "   Content preview:\n",
      "     def csv_to_table(\n",
      "     path: str,\n",
      "     fs: fsspec.filesystem | None = None,\n",
      "     ... (22 more lines)\n",
      "\n",
      "7. json_to_table\n",
      "   Type: function_definition\n",
      "   Size: 178 tokens, 25 lines\n",
      "   Content preview:\n",
      "     def json_to_table(\n",
      "     path: str,\n",
      "     fs: fsspec.filesystem | None = None,\n",
      "     ... (22 more lines)\n",
      "\n",
      "8. parquet_to_table\n",
      "   Type: function_definition\n",
      "   Size: 251 tokens, 46 lines\n",
      "   Content preview:\n",
      "     def parquet_to_table(\n",
      "     path: PathLikeT | list[PathLikeT],\n",
      "     fs: fsspec.filesystem | None = None,\n",
      "     ... (43 more lines)\n",
      "\n",
      "9. read_csv\n",
      "   Type: function_definition\n",
      "   Size: 601 tokens, 81 lines\n",
      "   Content preview:\n",
      "     def read_csv(  # noqa: PLR0912\n",
      "     path: str | zipfile.ZipFile,\n",
      "     columns: list[str] | None = None,\n",
      "     ... (78 more lines)\n",
      "\n",
      "10. read_json\n",
      "   Type: function_definition\n",
      "   Size: 443 tokens, 62 lines\n",
      "   Content preview:\n",
      "     def read_json(\n",
      "     path: str,\n",
      "     columns: list[str] | None = None,\n",
      "     ... (59 more lines)\n",
      "\n",
      "11. read_parquet\n",
      "   Type: function_definition\n",
      "   Size: 235 tokens, 30 lines\n",
      "   Content preview:\n",
      "     def read_parquet(\n",
      "     path: PathLikeT,\n",
      "     columns: list[str] | None = None,\n",
      "     ... (27 more lines)\n",
      "\n",
      "12. _normalise_dt_param\n",
      "   Type: function_definition\n",
      "   Size: 250 tokens, 36 lines\n",
      "   Content preview:\n",
      "     def _normalise_dt_param(dt: str | int | datetime | date) -> str:\n",
      "     \"\"\"Convert dates into a normalised string representation.\n",
      "     \n",
      "     ... (33 more lines)\n",
      "\n",
      "13. normalise_dt_param_str\n",
      "   Type: function_definition\n",
      "   Size: 146 tokens, 15 lines\n",
      "   Content preview:\n",
      "     def normalise_dt_param_str(dt: str) -> tuple[str, ...]:\n",
      "     \"\"\"Convert a date parameter which may be a single date or a date range into a tuple.\n",
      "     \n",
      "     ... (12 more lines)\n",
      "\n",
      "14. distribution_to_filename\n",
      "   Type: function_definition\n",
      "   Size: 258 tokens, 33 lines\n",
      "   Content preview:\n",
      "     def distribution_to_filename(\n",
      "     root_folder: str,\n",
      "     dataset: str,\n",
      "     ... (30 more lines)\n",
      "\n",
      "15. _filename_to_distribution\n",
      "   Type: function_definition\n",
      "   Size: 126 tokens, 12 lines\n",
      "   Content preview:\n",
      "     def _filename_to_distribution(file_name: str) -> tuple[str, str, str, str]:\n",
      "     \"\"\"Breaks a filename down into the components that represent a dataset distribution in the catalog.\n",
      "     \n",
      "     ... (9 more lines)\n",
      "\n",
      "16. distribution_to_url\n",
      "   Type: function_definition\n",
      "   Size: 295 tokens, 32 lines\n",
      "   Content preview:\n",
      "     def distribution_to_url(\n",
      "     root_url: str,\n",
      "     dataset: str,\n",
      "     ... (29 more lines)\n",
      "\n",
      "17. _get_canonical_root_url\n",
      "   Type: function_definition\n",
      "   Size: 89 tokens, 13 lines\n",
      "   Content preview:\n",
      "     def _get_canonical_root_url(any_url: str) -> str:\n",
      "     \"\"\"Get the full URL for the API endpoint.\n",
      "     \n",
      "     ... (10 more lines)\n",
      "\n",
      "18. get_client\n",
      "   Type: function_definition\n",
      "   Size: 234 tokens, 29 lines\n",
      "   Content preview:\n",
      "     async def get_client(credentials: FusionCredentials, **kwargs: Any) -> FusionAiohttpSession:  # noqa: PLR0915\n",
      "     \"\"\"Gets session for async.\n",
      "     \n",
      "     ... (26 more lines)\n",
      "\n",
      "19. get_session\n",
      "   Type: function_definition\n",
      "   Size: 237 tokens, 27 lines\n",
      "   Content preview:\n",
      "     def get_session(\n",
      "     credentials: FusionCredentials, root_url: str, get_retries: int | Retry | None = None\n",
      "     ) -> requests.Session:\n",
      "     ... (24 more lines)\n",
      "\n",
      "20. validate_file_names\n",
      "   Type: function_definition\n",
      "   Size: 196 tokens, 25 lines\n",
      "   Content preview:\n",
      "     def validate_file_names(paths: list[str]) -> list[bool]:\n",
      "     \"\"\"Validate if the file name format adheres to the standard.\n",
      "     \n",
      "     ... (22 more lines)\n",
      "\n",
      "21. is_dataset_raw\n",
      "   Type: function_definition\n",
      "   Size: 175 tokens, 20 lines\n",
      "   Content preview:\n",
      "     def is_dataset_raw(paths: list[str], fs_fusion: fsspec.AbstractFileSystem) -> list[bool]:\n",
      "     \"\"\"Check if the files correspond to a raw dataset.\n",
      "     \n",
      "     ... (17 more lines)\n",
      "\n",
      "22. path_to_url\n",
      "   Type: function_definition\n",
      "   Size: 148 tokens, 14 lines\n",
      "   Content preview:\n",
      "     def path_to_url(x: str, is_raw: bool = False, is_download: bool = False) -> str:\n",
      "     \"\"\"Convert file name to fusion url.\n",
      "     \n",
      "     ... (11 more lines)\n",
      "\n",
      "23. upload_files\n",
      "   Type: function_definition\n",
      "   Size: 684 tokens, 82 lines\n",
      "   Content preview:\n",
      "     def upload_files(  # noqa: PLR0913\n",
      "     fs_fusion: fsspec.AbstractFileSystem,\n",
      "     fs_local: fsspec.AbstractFileSystem,\n",
      "     ... (79 more lines)\n",
      "\n",
      "24. camel_to_snake\n",
      "   Type: function_definition\n",
      "   Size: 60 tokens, 4 lines\n",
      "   Content preview:\n",
      "     def camel_to_snake(name: str) -> str:\n",
      "     \"\"\"Convert camelCase to snake_case.\"\"\"\n",
      "     s1 = re.sub(re_str_1, r\"\\1_\\2\", name)\n",
      "     ... (1 more lines)\n",
      "\n",
      "25. CamelCaseMeta\n",
      "   Type: class_definition\n",
      "   Size: 253 tokens, 24 lines\n",
      "   Content preview:\n",
      "     class CamelCaseMeta(type):\n",
      "     \"\"\"Metaclass to support both snake and camel case typing.\"\"\"\n",
      "     \n",
      "     ... (21 more lines)\n",
      "\n",
      "26. snake_to_camel\n",
      "   Type: function_definition\n",
      "   Size: 49 tokens, 4 lines\n",
      "   Content preview:\n",
      "     def snake_to_camel(name: str) -> str:\n",
      "     \"\"\"Convert snake_case to camelCase.\"\"\"\n",
      "     components = name.lower().split(\"_\")\n",
      "     ... (1 more lines)\n",
      "\n",
      "27. tidy_string\n",
      "   Type: function_definition\n",
      "   Size: 71 tokens, 14 lines\n",
      "   Content preview:\n",
      "     def tidy_string(x: str) -> str:\n",
      "     \"\"\"Tidy string.\n",
      "     \n",
      "     ... (11 more lines)\n",
      "\n",
      "28. make_list\n",
      "   Type: function_definition\n",
      "   Size: 75 tokens, 12 lines\n",
      "   Content preview:\n",
      "     def make_list(obj: Any) -> list[str]:\n",
      "     \"\"\"Make list.\"\"\"\n",
      "     if isinstance(obj, list):\n",
      "     ... (9 more lines)\n",
      "\n",
      "29. make_bool\n",
      "   Type: function_definition\n",
      "   Size: 66 tokens, 10 lines\n",
      "   Content preview:\n",
      "     def make_bool(obj: Any) -> bool:\n",
      "     \"\"\"Make boolean.\"\"\"\n",
      "     if isinstance(obj, str):\n",
      "     ... (7 more lines)\n",
      "\n",
      "30. convert_date_format\n",
      "   Type: function_definition\n",
      "   Size: 57 tokens, 6 lines\n",
      "   Content preview:\n",
      "     def convert_date_format(date_str: str) -> Any:\n",
      "     \"\"\"Convert date to YYYY-MM-DD format.\"\"\"\n",
      "     desired_format = \"%Y-%m-%d\"\n",
      "     ... (3 more lines)\n",
      "\n",
      "31. _is_json\n",
      "   Type: function_definition\n",
      "   Size: 48 tokens, 7 lines\n",
      "   Content preview:\n",
      "     def _is_json(data: str) -> bool:\n",
      "     \"\"\"Check if the data is in JSON format.\"\"\"\n",
      "     try:\n",
      "     ... (4 more lines)\n",
      "\n",
      "32. requests_raise_for_status\n",
      "   Type: function_definition\n",
      "   Size: 54 tokens, 8 lines\n",
      "   Content preview:\n",
      "     def requests_raise_for_status(response: requests.Response) -> None:\n",
      "     \"\"\"Send response text into raise for status.\"\"\"\n",
      "     real_reason = \"\"\n",
      "     ... (5 more lines)\n",
      "\n",
      "33. validate_file_formats\n",
      "   Type: function_definition\n",
      "   Size: 263 tokens, 28 lines\n",
      "   Content preview:\n",
      "     def validate_file_formats(fs_fusion: fsspec.AbstractFileSystem, path: str) -> None:\n",
      "     \"\"\"\n",
      "     Validate file formats in the given folder and subfolders.\n",
      "     ... (25 more lines)\n",
      "\n",
      "34. file_name_to_url\n",
      "   Type: function_definition\n",
      "   Size: 216 tokens, 26 lines\n",
      "   Content preview:\n",
      "     def file_name_to_url(\n",
      "     file_name: str, dataset: str, catalog: str, is_download: bool = False\n",
      "     ) -> str:\n",
      "     ... (23 more lines)\n",
      "\n",
      "\n",
      "=== Processing: fusion.py ===\n",
      "File size: 121500 characters\n",
      "Found 29 semantic units\n",
      "  - import_statement: import_import_copy (2 tokens)\n",
      "    Preview: import copy...\n",
      "  - import_statement: import_import_json (4 tokens)\n",
      "    Preview: import json as js...\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_import_re (2 tokens)\n",
      "    Preview: import re...\n",
      "  - import_statement: import_import_sys (2 tokens)\n",
      "    Preview: import sys...\n",
      "  - import_statement: import_import_warnings (2 tokens)\n",
      "    Preview: import warnings...\n",
      "  - import_statement: import_from_http (5 tokens)\n",
      "    Preview: from http import HTTPStatus...\n",
      "  - import_statement: import_from_io (5 tokens)\n",
      "    Preview: from io import BytesIO...\n",
      "  - import_statement: import_from_pathlib (4 tokens)\n",
      "    Preview: from pathlib import Path...\n",
      "  - import_statement: import_from_typing (8 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any...\n",
      "  - import_statement: import_from_zipfile (5 tokens)\n",
      "    Preview: from zipfile import ZipFile...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_import_pyarrow (5 tokens)\n",
      "    Preview: import pyarrow as pa...\n",
      "  - import_statement: import_from_rich.progress (5 tokens)\n",
      "    Preview: from rich.progress import Progress...\n",
      "  - import_statement: import_from_tabulate (6 tokens)\n",
      "    Preview: from tabulate import tabulate...\n",
      "  - import_statement: import_from_fusion._fusion (7 tokens)\n",
      "    Preview: from fusion._fusion import FusionCredentials...\n",
      "  - import_statement: import_from_fusion.attributes (7 tokens)\n",
      "    Preview: from fusion.attributes import Attribute, Attributes...\n",
      "  - import_statement: import_from_fusion.dataflow (12 tokens)\n",
      "    Preview: from fusion.dataflow import InputDataFlow, OutputDataFlow...\n",
      "  - import_statement: import_from_fusion.dataset (5 tokens)\n",
      "    Preview: from fusion.dataset import Dataset...\n",
      "  - import_statement: import_from_fusion.fusion_types (7 tokens)\n",
      "    Preview: from fusion.fusion_types import Types...\n",
      "  - import_statement: import_from_fusion.product (5 tokens)\n",
      "    Preview: from fusion.product import Product...\n",
      "  - import_statement: import_from_fusion.report (8 tokens)\n",
      "    Preview: from fusion.report import Report, ReportsWrapper...\n",
      "  - import_statement: import_from_fusion.report_attributes (10 tokens)\n",
      "    Preview: from fusion.report_attributes import ReportAttribute, ReportAttributes...\n",
      "  - import_statement: import_from_.embeddings_utils (17 tokens)\n",
      "    Preview: from .embeddings_utils import _format_full_index_response, _format_summary_index_response...\n",
      "  - import_statement: import_from_.exceptions (14 tokens)\n",
      "    Preview: from .exceptions import APIResponseError, CredentialError, FileFormatError...\n",
      "  - import_statement: import_from_.fusion_filesystem (9 tokens)\n",
      "    Preview: from .fusion_filesystem import FusionHTTPFileSystem...\n",
      "  - import_statement: import_from_.utils (109 tokens)\n",
      "    Preview: from .utils import (     RECOGNIZED_FORMATS,     cpu_count,     csv_to_table,     distribution_to_fi...\n",
      "  - if_statement: if_block (47 tokens)\n",
      "    Preview: if TYPE_CHECKING:     from collections.abc import AsyncGenerator      import fsspec     import reque...\n",
      "  - class_definition: Fusion (24336 tokens)\n",
      "    Preview: class Fusion:     \"\"\"Core Fusion class for API access.\"\"\"      @staticmethod     def _call_for_dataf...\n",
      "Created 3 semantic chunks\n",
      "  Sub-chunking Fusion (24336 tokens)\n",
      "    Breaking down Fusion (24336 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 27 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for fusion.py ---\n",
      "1. imports_27_statements\n",
      "   Type: imports_group\n",
      "   Size: 297 tokens, 48 lines\n",
      "   Content preview:\n",
      "     import copy\n",
      "     import json as js\n",
      "     import logging\n",
      "     ... (45 more lines)\n",
      "\n",
      "2. if_block\n",
      "   Type: if_statement\n",
      "   Size: 47 tokens, 8 lines\n",
      "   Content preview:\n",
      "     if TYPE_CHECKING:\n",
      "     from collections.abc import AsyncGenerator\n",
      "     \n",
      "     ... (5 more lines)\n",
      "\n",
      "  3. Fusion_part_1\n",
      "     Type: class_definition_part\n",
      "     Size: 1015 tokens, 118 lines\n",
      "     Content preview:\n",
      "       class Fusion:\n",
      "       \"\"\"Core Fusion class for API access.\"\"\"\n",
      "       \n",
      "       ... (115 more lines)\n",
      "\n",
      "  4. Fusion_part_2\n",
      "     Type: class_definition_part\n",
      "     Size: 1011 tokens, 142 lines\n",
      "     Content preview:\n",
      "       ]\n",
      "       + [p for p in dir(Fusion) if isinstance(getattr(Fusion, p), property)],\n",
      "       [\n",
      "       ... (139 more lines)\n",
      "\n",
      "  5. Fusion_part_3\n",
      "     Type: class_definition_part\n",
      "     Size: 1011 tokens, 118 lines\n",
      "     Content preview:\n",
      "       products whose identifier matches any of the strings. Defaults to None.\n",
      "       id_contains (bool): Filter datasets only where the string(s) are contained in the identifier,\n",
      "       ignoring description.\n",
      "       ... (115 more lines)\n",
      "\n",
      "  6. Fusion_part_4\n",
      "     Type: class_definition_part\n",
      "     Size: 1006 tokens, 127 lines\n",
      "     Content preview:\n",
      "       ds_df = Fusion._call_for_dataframe(url, self.session)\n",
      "       \n",
      "       if contains:\n",
      "       ... (124 more lines)\n",
      "\n",
      "  7. Fusion_part_5\n",
      "     Type: class_definition_part\n",
      "     Size: 1001 tokens, 144 lines\n",
      "     Content preview:\n",
      "       dataset (str): A dataset identifier\n",
      "       catalog (str, optional): A catalog identifier. Defaults to 'common'.\n",
      "       output (bool, optional): If True then print the dataframe. Defaults to False.\n",
      "       ... (141 more lines)\n",
      "\n",
      "  8. Fusion_part_6\n",
      "     Type: class_definition_part\n",
      "     Size: 997 tokens, 110 lines\n",
      "     Content preview:\n",
      "       url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/datasetseries/{series}/distributions\"\n",
      "       distros_df = Fusion._call_for_dataframe(url, self.session)\n",
      "       \n",
      "       ... (107 more lines)\n",
      "\n",
      "  9. Fusion_part_7\n",
      "     Type: class_definition_part\n",
      "     Size: 1004 tokens, 112 lines\n",
      "     Content preview:\n",
      "       If set to None, the function will download if only one format is available, else it will raise an error.\n",
      "       catalog (str, optional): A catalog identifier. Defaults to 'common'.\n",
      "       n_par (int, optional): Specify how many distributions to download in parallel.\n",
      "       ... (109 more lines)\n",
      "\n",
      "  10. Fusion_part_8\n",
      "     Type: class_definition_part\n",
      "     Size: 1002 tokens, 91 lines\n",
      "     Content preview:\n",
      "       else:\n",
      "       res = [self.get_fusion_filesystem().download(**spec) for spec in download_spec]\n",
      "       \n",
      "       ... (88 more lines)\n",
      "\n",
      "  11. Fusion_part_9\n",
      "     Type: class_definition_part\n",
      "     Size: 1012 tokens, 145 lines\n",
      "     Content preview:\n",
      "       class:`pandas.DataFrame`: a dataframe containing the requested data.\n",
      "       If multiple dataset instances are retrieved then these are concatenated first.\n",
      "       \"\"\"\n",
      "       ... (142 more lines)\n",
      "\n",
      "  12. Fusion_part_10\n",
      "     Type: class_definition_part\n",
      "     Size: 1001 tokens, 104 lines\n",
      "     Content preview:\n",
      "       show_progress: bool = True,\n",
      "       columns: list[str] | None = None,\n",
      "       filters: PyArrowFilterT | None = None,\n",
      "       ... (101 more lines)\n",
      "\n",
      "  13. Fusion_part_11\n",
      "     Type: class_definition_part\n",
      "     Size: 997 tokens, 89 lines\n",
      "     Content preview:\n",
      "       def upload(  # noqa: PLR0912, PLR0913, PLR0915\n",
      "       self,\n",
      "       path: str,\n",
      "       ... (86 more lines)\n",
      "\n",
      "  14. Fusion_part_12\n",
      "     Type: class_definition_part\n",
      "     Size: 1009 tokens, 96 lines\n",
      "     Content preview:\n",
      "       is_raw_lst = is_dataset_raw(file_path_lst, fs_fusion)\n",
      "       local_url_eqiv = [path_to_url(i, r) for i, r in zip(file_path_lst, is_raw_lst)]\n",
      "       if preserve_original_name:\n",
      "       ... (93 more lines)\n",
      "\n",
      "  15. Fusion_part_13\n",
      "     Type: class_definition_part\n",
      "     Size: 1018 tokens, 140 lines\n",
      "     Content preview:\n",
      "       data_map_df.columns = [\"path\", \"url\", \"file_name\"]  # type: ignore[assignment]\n",
      "       \n",
      "       res = upload_files(\n",
      "       ... (137 more lines)\n",
      "\n",
      "  16. Fusion_part_14\n",
      "     Type: class_definition_part\n",
      "     Size: 1004 tokens, 133 lines\n",
      "     Content preview:\n",
      "       event = js.loads(msg.data)\n",
      "       # Preserve the original metaData column\n",
      "       original_meta_data = event.get(\"metaData\", {})\n",
      "       ... (130 more lines)\n",
      "\n",
      "  17. Fusion_part_15\n",
      "     Type: class_definition_part\n",
      "     Size: 990 tokens, 103 lines\n",
      "     Content preview:\n",
      "       HTTPError: If the request is unsuccessful.\n",
      "       \n",
      "       Examples:\n",
      "       ... (100 more lines)\n",
      "\n",
      "  18. Fusion_part_16\n",
      "     Type: class_definition_part\n",
      "     Size: 1003 tokens, 103 lines\n",
      "     Content preview:\n",
      "       category (str | list[str] | None, optional): Category. Defaults to None.\n",
      "       short_abstract (str, optional): Short description. Defaults to \"\".\n",
      "       description (str, optional): Description. If not provided, defaults to identifier.\n",
      "       ... (100 more lines)\n",
      "\n",
      "  19. Fusion_part_17\n",
      "     Type: class_definition_part\n",
      "     Size: 1002 tokens, 94 lines\n",
      "     Content preview:\n",
      "       Defaults to None.\n",
      "       description (str, optional): Dataset description. If not provided, defaults to identifier.\n",
      "       frequency (str, optional): The frequency of the dataset. Defaults to \"Once\".\n",
      "       ... (91 more lines)\n",
      "\n",
      "  20. Fusion_part_18\n",
      "     Type: class_definition_part\n",
      "     Size: 1002 tokens, 113 lines\n",
      "     Content preview:\n",
      "       index: int,\n",
      "       data_type: str | Types = \"String\",\n",
      "       title: str = \"\",\n",
      "       ... (110 more lines)\n",
      "\n",
      "  21. Fusion_part_19\n",
      "     Type: class_definition_part\n",
      "     Size: 1008 tokens, 140 lines\n",
      "     Content preview:\n",
      "       title: str,\n",
      "       sourceIdentifier: str | None = None,\n",
      "       description: str | None = None,\n",
      "       ... (137 more lines)\n",
      "\n",
      "  22. Fusion_part_20\n",
      "     Type: class_definition_part\n",
      "     Size: 994 tokens, 103 lines\n",
      "     Content preview:\n",
      "       return resp if return_resp_obj else None\n",
      "       \n",
      "       def list_registered_attributes(\n",
      "       ... (100 more lines)\n",
      "\n",
      "  23. Fusion_part_21\n",
      "     Type: class_definition_part\n",
      "     Size: 1000 tokens, 97 lines\n",
      "     Content preview:\n",
      "       publisher (str, optional): Name of vendor that publishes the data. Defaults to \"J.P. Morgan\".\n",
      "       product (str | list[str] | None, optional): Product to associate dataset with. Defaults to None.\n",
      "       sub_category (str | list[str] | None, optional): Sub-category. Defaults to None.\n",
      "       ... (94 more lines)\n",
      "\n",
      "  24. Fusion_part_22\n",
      "     Type: class_definition_part\n",
      "     Size: 985 tokens, 70 lines\n",
      "     Content preview:\n",
      "       frequency: str = \"Once\",\n",
      "       is_internal_only_dataset: bool = False,\n",
      "       is_third_party_data: bool = True,\n",
      "       ... (67 more lines)\n",
      "\n",
      "  25. Fusion_part_23\n",
      "     Type: class_definition_part\n",
      "     Size: 1002 tokens, 125 lines\n",
      "     Content preview:\n",
      "       is_pii (bool | None, optional): is_pii. Defaults to None.\n",
      "       is_client (bool | None, optional): is_client. Defaults to None.\n",
      "       is_public (bool | None, optional): is_public. Defaults to None.\n",
      "       ... (122 more lines)\n",
      "\n",
      "  26. Fusion_part_24\n",
      "     Type: class_definition_part\n",
      "     Size: 999 tokens, 120 lines\n",
      "     Content preview:\n",
      "       def get_async_fusion_vector_store_client(self, knowledge_base: str, catalog: str | None = None) -> AsyncOpenSearch:\n",
      "       \"\"\"Returns Fusion Embeddings Search client.\n",
      "       \n",
      "       ... (117 more lines)\n",
      "\n",
      "  27. Fusion_part_25\n",
      "     Type: class_definition_part\n",
      "     Size: 260 tokens, 43 lines\n",
      "     Content preview:\n",
      "       risk_area=risk_area,\n",
      "       risk_stripe=risk_stripe,\n",
      "       sap_code=sap_code,\n",
      "       ... (40 more lines)\n",
      "\n",
      "\n",
      "=== Processing: authentication.py ===\n",
      "File size: 6037 characters\n",
      "Found 13 semantic units\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_from_typing (8 tokens)\n",
      "    Preview: from typing import Any, Optional, Union...\n",
      "  - import_statement: import_from_urllib.parse (5 tokens)\n",
      "    Preview: from urllib.parse import urlparse...\n",
      "  - import_statement: import_import_aiohttp (3 tokens)\n",
      "    Preview: import aiohttp...\n",
      "  - import_statement: import_import_requests (2 tokens)\n",
      "    Preview: import requests...\n",
      "  - import_statement: import_from_requests.adapters (6 tokens)\n",
      "    Preview: from requests.adapters import HTTPAdapter...\n",
      "  - import_statement: import_from_urllib3.util.retry (7 tokens)\n",
      "    Preview: from urllib3.util.retry import Retry...\n",
      "  - import_statement: import_from_fusion._fusion (7 tokens)\n",
      "    Preview: from fusion._fusion import FusionCredentials...\n",
      "  - import_statement: import_from_fusion.exceptions (7 tokens)\n",
      "    Preview: from fusion.exceptions import APIResponseError...\n",
      "  - function_definition: _res_plural (106 tokens)\n",
      "    Preview: def _res_plural(ref_int: int, pluraliser: str = \"s\") -> str:     \"\"\"Private function to return the p...\n",
      "  - function_definition: _is_url (90 tokens)\n",
      "    Preview: def _is_url(url: str) -> bool:     \"\"\"Test whether the content of a string is a valid URL.      Args...\n",
      "  - class_definition: FusionOAuthAdapter (690 tokens)\n",
      "    Preview: class FusionOAuthAdapter(HTTPAdapter):     \"\"\"An OAuth adapter to manage authentication and session...\n",
      "  - class_definition: FusionAiohttpSession (287 tokens)\n",
      "    Preview: class FusionAiohttpSession(aiohttp.ClientSession):     \"\"\"Bespoke aiohttp session.\"\"\"      def __ini...\n",
      "Created 5 semantic chunks\n",
      "Final result: 5 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for authentication.py ---\n",
      "1. imports_9_statements\n",
      "   Type: imports_group\n",
      "   Size: 55 tokens, 9 lines\n",
      "   Content preview:\n",
      "     import logging\n",
      "     from typing import Any, Optional, Union\n",
      "     from urllib.parse import urlparse\n",
      "     ... (6 more lines)\n",
      "\n",
      "2. _res_plural\n",
      "   Type: function_definition\n",
      "   Size: 106 tokens, 11 lines\n",
      "   Content preview:\n",
      "     def _res_plural(ref_int: int, pluraliser: str = \"s\") -> str:\n",
      "     \"\"\"Private function to return the plural form when the number is more than one.\n",
      "     \n",
      "     ... (8 more lines)\n",
      "\n",
      "3. _is_url\n",
      "   Type: function_definition\n",
      "   Size: 90 tokens, 14 lines\n",
      "   Content preview:\n",
      "     def _is_url(url: str) -> bool:\n",
      "     \"\"\"Test whether the content of a string is a valid URL.\n",
      "     \n",
      "     ... (11 more lines)\n",
      "\n",
      "4. FusionOAuthAdapter\n",
      "   Type: class_definition\n",
      "   Size: 690 tokens, 87 lines\n",
      "   Content preview:\n",
      "     class FusionOAuthAdapter(HTTPAdapter):\n",
      "     \"\"\"An OAuth adapter to manage authentication and session tokens.\"\"\"\n",
      "     \n",
      "     ... (84 more lines)\n",
      "\n",
      "5. FusionAiohttpSession\n",
      "   Type: class_definition\n",
      "   Size: 287 tokens, 28 lines\n",
      "   Content preview:\n",
      "     class FusionAiohttpSession(aiohttp.ClientSession):\n",
      "     \"\"\"Bespoke aiohttp session.\"\"\"\n",
      "     \n",
      "     ... (25 more lines)\n",
      "\n",
      "\n",
      "=== Processing: embeddings_utils.py ===\n",
      "File size: 5987 characters\n",
      "Found 9 semantic units\n",
      "  - import_statement: import_import_json (2 tokens)\n",
      "    Preview: import json...\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_from_typing (6 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - function_definition: _format_full_index_response (195 tokens)\n",
      "    Preview: def _format_full_index_response(response: requests.Response) -> pd.DataFrame:     \"\"\"Format get inde...\n",
      "  - function_definition: _format_summary_index_response (217 tokens)\n",
      "    Preview: def _format_summary_index_response(response: requests.Response) -> pd.DataFrame:     \"\"\"Format summa...\n",
      "  - function_definition: _retrieve_index_name_from_bulk_body (113 tokens)\n",
      "    Preview: def _retrieve_index_name_from_bulk_body(body: bytes | None) -> str:     body_str = body.decode(\"utf-...\n",
      "  - function_definition: _modify_post_response_langchain (289 tokens)\n",
      "    Preview: def _modify_post_response_langchain(raw_data: str | bytes | bytearray) -> str | bytes | bytearray:...\n",
      "  - function_definition: _modify_post_haystack (436 tokens)\n",
      "    Preview: def _modify_post_haystack(knowledge_base: str | list[str] | None, body: bytes | None, method: str) -...\n",
      "Created 4 semantic chunks\n",
      "Final result: 4 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for embeddings_utils.py ---\n",
      "1. imports_4_statements\n",
      "   Type: imports_group\n",
      "   Size: 17 tokens, 4 lines\n",
      "   Content preview:\n",
      "     import json\n",
      "     import logging\n",
      "     from typing import TYPE_CHECKING\n",
      "     ... (1 more lines)\n",
      "\n",
      "2. python_group_3_definitions\n",
      "   Type: grouped_content\n",
      "   Size: 526 tokens, 64 lines\n",
      "   Content preview:\n",
      "     def _format_full_index_response(response: requests.Response) -> pd.DataFrame:\n",
      "     \"\"\"Format get index response.\n",
      "     \n",
      "     ... (61 more lines)\n",
      "\n",
      "3. python_group_1_definitions\n",
      "   Type: grouped_content\n",
      "   Size: 289 tokens, 33 lines\n",
      "   Content preview:\n",
      "     def _modify_post_response_langchain(raw_data: str | bytes | bytearray) -> str | bytes | bytearray:\n",
      "     \"\"\"Modify the response from langchain POST request to match the expected format.\n",
      "     \n",
      "     ... (30 more lines)\n",
      "\n",
      "4. _modify_post_haystack\n",
      "   Type: function_definition\n",
      "   Size: 436 tokens, 40 lines\n",
      "   Content preview:\n",
      "     def _modify_post_haystack(knowledge_base: str | list[str] | None, body: bytes | None, method: str) -> bytes | None:\n",
      "     \"\"\"Method to modify haystack POST body to match the embeddings API, which expects the embedding field to be\n",
      "     named \"vector\".\n",
      "     ... (37 more lines)\n",
      "\n",
      "\n",
      "=== Processing: exceptions.py ===\n",
      "File size: 2366 characters\n",
      "Found 7 semantic units\n",
      "  - import_statement: import_from_typing (4 tokens)\n",
      "    Preview: from typing import Optional...\n",
      "  - class_definition: APIResponseError (164 tokens)\n",
      "    Preview: class APIResponseError(Exception):     \"\"\"APIResponseError exception wrapper to handle API response...\n",
      "  - class_definition: APIRequestError (33 tokens)\n",
      "    Preview: class APIRequestError(Exception):     \"\"\"APIRequestError exception wrapper to handle API request ero...\n",
      "  - class_definition: APIConnectError (31 tokens)\n",
      "    Preview: class APIConnectError(Exception):     \"\"\"APIConnectError exception wrapper to handle API connection...\n",
      "  - class_definition: UnrecognizedFormatError (32 tokens)\n",
      "    Preview: class UnrecognizedFormatError(Exception):     \"\"\"UnrecognizedFormatError exception wrapper to handle...\n",
      "  - class_definition: CredentialError (150 tokens)\n",
      "    Preview: class CredentialError(Exception):     \"\"\"CredentialError exception wrapper to handle errors in crede...\n",
      "  - class_definition: FileFormatError (44 tokens)\n",
      "    Preview: class FileFormatError(Exception):     \"\"\"FileFormatRequiredError exception wrapper to handle errors...\n",
      "Created 1 semantic chunks\n",
      "Final result: 1 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for exceptions.py ---\n",
      "1. complete_module_7_parts\n",
      "   Type: complete_module\n",
      "   Size: 459 tokens, 67 lines\n",
      "   Content preview:\n",
      "     from typing import Optional\n",
      "     \n",
      "     class APIResponseError(Exception):\n",
      "     ... (64 more lines)\n",
      "\n",
      "\n",
      "=== Processing: product.py ===\n",
      "File size: 24857 characters\n",
      "Found 6 semantic units\n",
      "  - import_statement: import_import_json (4 tokens)\n",
      "    Preview: import json as js...\n",
      "  - import_statement: import_from_dataclasses (10 tokens)\n",
      "    Preview: from dataclasses import dataclass, field, fields...\n",
      "  - import_statement: import_from_typing (8 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_from_fusion.utils (51 tokens)\n",
      "    Preview: from fusion.utils import (     CamelCaseMeta,     _is_json,     camel_to_snake,     convert_date_for...\n",
      "  - decorated_definition: decorated_Product (5046 tokens)\n",
      "    Preview: @dataclass class Product(metaclass=CamelCaseMeta):     \"\"\"Fusion Product class for managing product...\n",
      "Created 2 semantic chunks\n",
      "  Sub-chunking decorated_Product (5046 tokens)\n",
      "    Breaking down decorated_Product (5046 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 7 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for product.py ---\n",
      "1. imports_5_statements\n",
      "   Type: imports_group\n",
      "   Size: 81 tokens, 15 lines\n",
      "   Content preview:\n",
      "     import json as js\n",
      "     from dataclasses import dataclass, field, fields\n",
      "     from typing import TYPE_CHECKING, Any\n",
      "     ... (12 more lines)\n",
      "\n",
      "  2. decorated_Product_part_1\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1014 tokens, 85 lines\n",
      "     Content preview:\n",
      "       @dataclass\n",
      "       class Product(metaclass=CamelCaseMeta):\n",
      "       \"\"\"Fusion Product class for managing product metadata in a Fusion catalog.\n",
      "       ... (82 more lines)\n",
      "\n",
      "  3. decorated_Product_part_2\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1000 tokens, 126 lines\n",
      "     Content preview:\n",
      "       else make_bool(self.is_restricted)\n",
      "       )\n",
      "       self.maintainer = (\n",
      "       ... (123 more lines)\n",
      "\n",
      "  4. decorated_Product_part_3\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1009 tokens, 119 lines\n",
      "     Content preview:\n",
      "       Product._from_series(data[data[\"identifier\"] == identifier].reset_index(drop=True).iloc[0])\n",
      "       if identifier\n",
      "       else Product._from_series(data.reset_index(drop=True).iloc[0])\n",
      "       ... (116 more lines)\n",
      "\n",
      "  5. decorated_Product_part_4\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1002 tokens, 136 lines\n",
      "     Content preview:\n",
      "       else:\n",
      "       raise TypeError(f\"Could not resolve the object provided: {product_source}\")\n",
      "       product.client = self._client\n",
      "       ... (133 more lines)\n",
      "\n",
      "  6. decorated_Product_part_5\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1011 tokens, 121 lines\n",
      "     Content preview:\n",
      "       delivery_channel = self.delivery_channel if self.delivery_channel else [\"API\"]\n",
      "       \n",
      "       self.release_date = release_date\n",
      "       ... (118 more lines)\n",
      "\n",
      "  7. decorated_Product_part_6\n",
      "     Type: decorated_definition_part\n",
      "     Size: 9 tokens, 1 lines\n",
      "     Content preview:\n",
      "       return resp if return_resp_obj else None\n",
      "\n",
      "\n",
      "=== Processing: dataflow.py ===\n",
      "File size: 4007 characters\n",
      "Found 7 semantic units\n",
      "  - import_statement: import_from_dataclasses (8 tokens)\n",
      "    Preview: from dataclasses import dataclass, field...\n",
      "  - import_statement: import_from_typing (6 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING...\n",
      "  - import_statement: import_from_fusion.dataset (5 tokens)\n",
      "    Preview: from fusion.dataset import Dataset...\n",
      "  - import_statement: import_from_fusion.utils (8 tokens)\n",
      "    Preview: from fusion.utils import requests_raise_for_status...\n",
      "  - decorated_definition: decorated_DataFlow (525 tokens)\n",
      "    Preview: @dataclass class DataFlow(Dataset):     \"\"\"Dataflow class for maintaining data flow metadata.      A...\n",
      "  - decorated_definition: decorated_InputDataFlow (151 tokens)\n",
      "    Preview: @dataclass class InputDataFlow(DataFlow):     \"\"\"InputDataFlow class for maintaining input data flow...\n",
      "  - decorated_definition: decorated_OutputDataFlow (151 tokens)\n",
      "    Preview: @dataclass class OutputDataFlow(DataFlow):     \"\"\"OutputDataFlow class for maintaining output data f...\n",
      "Created 1 semantic chunks\n",
      "Final result: 1 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for dataflow.py ---\n",
      "1. complete_module_7_parts\n",
      "   Type: complete_module\n",
      "   Size: 859 tokens, 101 lines\n",
      "   Content preview:\n",
      "     from dataclasses import dataclass, field\n",
      "     \n",
      "     from typing import TYPE_CHECKING\n",
      "     ... (98 more lines)\n",
      "\n",
      "\n",
      "=== Processing: __main__.py ===\n",
      "File size: 1597 characters\n",
      "Found 4 semantic units\n",
      "  - import_statement: import_import_argparse (2 tokens)\n",
      "    Preview: import argparse...\n",
      "  - import_statement: import_import_inspect (2 tokens)\n",
      "    Preview: import inspect...\n",
      "  - import_statement: import_from_fusion (4 tokens)\n",
      "    Preview: from fusion import Fusion...\n",
      "  - if_statement: main_block (353 tokens)\n",
      "    Preview: if __name__ == \"__main__\":     parser = argparse.ArgumentParser(description=\"Fusion command line env...\n",
      "Created 1 semantic chunks\n",
      "Final result: 1 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for __main__.py ---\n",
      "1. complete_module_4_parts\n",
      "   Type: complete_module\n",
      "   Size: 364 tokens, 49 lines\n",
      "   Content preview:\n",
      "     import argparse\n",
      "     \n",
      "     import inspect\n",
      "     ... (46 more lines)\n",
      "\n",
      "\n",
      "=== Processing: report.py ===\n",
      "File size: 18069 characters\n",
      "Found 11 semantic units\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_from_dataclasses (10 tokens)\n",
      "    Preview: from dataclasses import dataclass, field, fields...\n",
      "  - import_statement: import_from_pathlib (4 tokens)\n",
      "    Preview: from pathlib import Path...\n",
      "  - import_statement: import_from_typing (11 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any, TypedDict...\n",
      "  - import_statement: import_import_numpy (4 tokens)\n",
      "    Preview: import numpy as np...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_from_.utils (37 tokens)\n",
      "    Preview: from .utils import (     CamelCaseMeta,     camel_to_snake,     make_bool,     requests_raise_for_st...\n",
      "  - if_statement: if_block (22 tokens)\n",
      "    Preview: if TYPE_CHECKING:     from collections.abc import Iterator      import requests      from fusion imp...\n",
      "  - decorated_definition: decorated_Report (2942 tokens)\n",
      "    Preview: @dataclass class Report(metaclass=CamelCaseMeta):     \"\"\"     Fusion Report class for managing repor...\n",
      "  - class_definition: ass (503 tokens)\n",
      "    Preview: ass Reports:     def __init__(self, reports: list[Report] | None = None) -> None:         self.repor...\n",
      "  - class_definition: s (141 tokens)\n",
      "    Preview: s ReportsWrapper(Reports) :         def __init__(self, client: Fusion) -> None:             super()....\n",
      "Created 5 semantic chunks\n",
      "  Sub-chunking decorated_Report (2942 tokens)\n",
      "    Breaking down decorated_Report (2942 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 7 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for report.py ---\n",
      "1. imports_7_statements\n",
      "   Type: imports_group\n",
      "   Size: 78 tokens, 14 lines\n",
      "   Content preview:\n",
      "     import logging\n",
      "     from dataclasses import dataclass, field, fields\n",
      "     from pathlib import Path\n",
      "     ... (11 more lines)\n",
      "\n",
      "2. if_block\n",
      "   Type: if_statement\n",
      "   Size: 22 tokens, 6 lines\n",
      "   Content preview:\n",
      "     if TYPE_CHECKING:\n",
      "     from collections.abc import Iterator\n",
      "     \n",
      "     ... (3 more lines)\n",
      "\n",
      "  3. decorated_Report_part_1\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1017 tokens, 116 lines\n",
      "     Content preview:\n",
      "       @dataclass\n",
      "       class Report(metaclass=CamelCaseMeta):\n",
      "       \"\"\"\n",
      "       ... (113 more lines)\n",
      "\n",
      "  4. decorated_Report_part_2\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1011 tokens, 121 lines\n",
      "     Content preview:\n",
      "       converted_data[\"isBCBS239Program\"] = make_bool(converted_data[\"isBCBS239Program\"])\n",
      "       \n",
      "       valid_fields = {f.name for f in fields(cls)}\n",
      "       ... (118 more lines)\n",
      "\n",
      "  5. decorated_Report_part_3\n",
      "     Type: decorated_definition_part\n",
      "     Size: 914 tokens, 127 lines\n",
      "     Content preview:\n",
      "       report_obj.client = client  # Attach the client if provided\n",
      "       \n",
      "       try:\n",
      "       ... (124 more lines)\n",
      "\n",
      "6. ass\n",
      "   Type: class_definition\n",
      "   Size: 503 tokens, 66 lines\n",
      "   Content preview:\n",
      "     ass Reports:\n",
      "     def __init__(self, reports: list[Report] | None = None) -> None:\n",
      "     self.reports = reports or []\n",
      "     ... (63 more lines)\n",
      "\n",
      "7. s\n",
      "   Type: class_definition\n",
      "   Size: 141 tokens, 15 lines\n",
      "   Content preview:\n",
      "     s ReportsWrapper(Reports) :\n",
      "     def __init__(self, client: Fusion) -> None:\n",
      "     super().__init__([])\n",
      "     ... (12 more lines)\n",
      "\n",
      "\n",
      "=== Processing: __init__.py ===\n",
      "File size: 27 characters\n",
      "Found 0 semantic units\n",
      "Created 0 semantic chunks\n",
      "Final result: 0 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for __init__.py ---\n",
      "\n",
      "=== Processing: authentication.py ===\n",
      "File size: 9393 characters\n",
      "Found 15 semantic units\n",
      "  - import_statement: import_import_json (2 tokens)\n",
      "    Preview: import json...\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_import_os (2 tokens)\n",
      "    Preview: import os...\n",
      "  - import_statement: import_from_datetime (6 tokens)\n",
      "    Preview: from datetime import datetime, timezone...\n",
      "  - import_statement: import_from_pathlib (4 tokens)\n",
      "    Preview: from pathlib import Path...\n",
      "  - import_statement: import_from_typing (6 tokens)\n",
      "    Preview: from typing import Any, Optional...\n",
      "  - import_statement: import_import_fsspec (4 tokens)\n",
      "    Preview: import fsspec...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_from_fusion.exceptions (6 tokens)\n",
      "    Preview: from fusion.exceptions import CredentialError...\n",
      "  - import_statement: import_from_fusion.utils (7 tokens)\n",
      "    Preview: from fusion.utils import get_default_fs...\n",
      "  - function_definition: try_get_env_var (107 tokens)\n",
      "    Preview: def try_get_env_var(var_name: str, default: Optional[str] = None) -> Optional[str]:     \"\"\"Get the v...\n",
      "  - function_definition: try_get_client_id (67 tokens)\n",
      "    Preview: def try_get_client_id(client_id: Optional[str]) -> Optional[str]:     \"\"\"Get the client ID from the...\n",
      "  - function_definition: try_get_client_secret (67 tokens)\n",
      "    Preview: def try_get_client_secret(client_secret: Optional[str]) -> Optional[str]:     \"\"\"Get the client secr...\n",
      "  - function_definition: try_get_fusion_e2e (81 tokens)\n",
      "    Preview: def try_get_fusion_e2e(fusion_e2e: Optional[str]) -> Optional[str]:     \"\"\"Get the Fusion E2E token...\n",
      "  - class_definition: FusionCredentials (1529 tokens)\n",
      "    Preview: class FusionCredentials:     \"\"\"Utility functions to manage credentials.\"\"\"      def __init__(  # no...\n",
      "Created 6 semantic chunks\n",
      "  Sub-chunking FusionCredentials (1529 tokens)\n",
      "    Breaking down FusionCredentials (1529 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 7 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for authentication.py ---\n",
      "1. imports_10_statements\n",
      "   Type: imports_group\n",
      "   Size: 52 tokens, 10 lines\n",
      "   Content preview:\n",
      "     import json\n",
      "     import logging\n",
      "     import os\n",
      "     ... (7 more lines)\n",
      "\n",
      "2. try_get_env_var\n",
      "   Type: function_definition\n",
      "   Size: 107 tokens, 11 lines\n",
      "   Content preview:\n",
      "     def try_get_env_var(var_name: str, default: Optional[str] = None) -> Optional[str]:\n",
      "     \"\"\"Get the value of an environment variable or return a default value.\n",
      "     \n",
      "     ... (8 more lines)\n",
      "\n",
      "3. try_get_client_id\n",
      "   Type: function_definition\n",
      "   Size: 67 tokens, 9 lines\n",
      "   Content preview:\n",
      "     def try_get_client_id(client_id: Optional[str]) -> Optional[str]:\n",
      "     \"\"\"Get the client ID from the environment variable or return None.\n",
      "     \n",
      "     ... (6 more lines)\n",
      "\n",
      "4. try_get_client_secret\n",
      "   Type: function_definition\n",
      "   Size: 67 tokens, 9 lines\n",
      "   Content preview:\n",
      "     def try_get_client_secret(client_secret: Optional[str]) -> Optional[str]:\n",
      "     \"\"\"Get the client secret from the environment variable or return None.\n",
      "     \n",
      "     ... (6 more lines)\n",
      "\n",
      "5. try_get_fusion_e2e\n",
      "   Type: function_definition\n",
      "   Size: 81 tokens, 9 lines\n",
      "   Content preview:\n",
      "     def try_get_fusion_e2e(fusion_e2e: Optional[str]) -> Optional[str]:\n",
      "     \"\"\"Get the Fusion E2E token from the environment variable or return None.\n",
      "     \n",
      "     ... (6 more lines)\n",
      "\n",
      "  6. FusionCredentials_part_1\n",
      "     Type: class_definition_part\n",
      "     Size: 1008 tokens, 100 lines\n",
      "     Content preview:\n",
      "       class FusionCredentials:\n",
      "       \"\"\"Utility functions to manage credentials.\"\"\"\n",
      "       \n",
      "       ... (97 more lines)\n",
      "\n",
      "  7. FusionCredentials_part_2\n",
      "     Type: class_definition_part\n",
      "     Size: 521 tokens, 64 lines\n",
      "     Content preview:\n",
      "       grant_type = credentials.get(\"grant_type\", \"client_credentials\")\n",
      "       try:\n",
      "       if grant_type == \"client_credentials\":\n",
      "       ... (61 more lines)\n",
      "\n",
      "\n",
      "📊 Final Summary:\n",
      "   Files processed: 20\n",
      "   Total chunks: 151\n",
      "   Total tokens: 81,739\n",
      "   Average tokens per chunk: 541.3\n",
      "✅ Saved 151 chunk files to python_chunks\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import secrets\n",
    "import string\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import tiktoken\n",
    "\n",
    "# Tree-sitter setup for Python\n",
    "try:\n",
    "    from tree_sitter_language_pack import get_language, get_parser\n",
    "    python_language = get_language('python')\n",
    "    python_parser = get_parser('python')\n",
    "    print(\"✅ Using Python parser\")\n",
    "except ImportError:\n",
    "    print(\"Please install: pip install tree-sitter-languages\")\n",
    "    exit(1)\n",
    "\n",
    "# Configuration\n",
    "MAX_CHUNK_TOKENS = 1000\n",
    "MAX_RECURSION_DEPTH = 3\n",
    "\n",
    "# Supported file extensions and their parsers\n",
    "SUPPORTED_EXTENSIONS = ['.py']\n",
    "PARSER_MAP = {\n",
    "    '.py': python_parser\n",
    "}\n",
    "\n",
    "# Initialize token encoder\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in text\"\"\"\n",
    "    return len(encoder.encode(text))\n",
    "\n",
    "def get_syntax_highlighting_language(file_extension: str) -> str:\n",
    "    \"\"\"Get appropriate syntax highlighting language for markdown output\"\"\"\n",
    "    language_map = {\n",
    "        '.py': 'python'\n",
    "    }\n",
    "    return language_map.get(file_extension, 'text')\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self, start_byte: int, end_byte: int, content: str, node_type: str, name: str, depth: int = 0):\n",
    "        self.start_byte = start_byte\n",
    "        self.end_byte = end_byte\n",
    "        self.content = content\n",
    "        self.node_type = node_type\n",
    "        self.name = name\n",
    "        self.depth = depth\n",
    "        self.token_count = count_tokens(content)\n",
    "        self.sub_chunks = []\n",
    "\n",
    "def extract_node_name(node, source_code: str) -> str:\n",
    "    \"\"\"Extract meaningful name from AST node\"\"\"\n",
    "    node_text = source_code[node.start_byte:node.end_byte]\n",
    "    lines = node_text.split('\\n')\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    # Try different patterns based on node type\n",
    "    if node.type == 'function_definition':\n",
    "        # Look for function name: def function_name(\n",
    "        match = re.search(r'def\\s+([a-zA-Z_][a-zA-Z0-9_]*)', node_text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    \n",
    "    elif node.type == 'class_definition':\n",
    "        # Look for class name: class ClassName\n",
    "        match = re.search(r'class\\s+([a-zA-Z_][a-zA-Z0-9_]*)', node_text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    \n",
    "    elif node.type == 'assignment':\n",
    "        # Look for variable assignment: variable_name = \n",
    "        match = re.search(r'^([a-zA-Z_][a-zA-Z0-9_]*)\\s*=', node_text.strip())\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    \n",
    "    elif node.type == 'import_statement' or node.type == 'import_from_statement':\n",
    "        # Handle import statements\n",
    "        if 'import' in node_text:\n",
    "            # Extract what's being imported\n",
    "            if 'from' in node_text:\n",
    "                # from module import something\n",
    "                match = re.search(r'from\\s+([a-zA-Z0-9_.]+)', node_text)\n",
    "                if match:\n",
    "                    return f\"from_{match.group(1)}\"\n",
    "            else:\n",
    "                # import module\n",
    "                match = re.search(r'import\\s+([a-zA-Z0-9_.]+)', node_text)\n",
    "                if match:\n",
    "                    return f\"import_{match.group(1)}\"\n",
    "    \n",
    "    elif node.type == 'decorated_definition':\n",
    "        # Handle decorated functions/classes (with @decorator)\n",
    "        # Look for the actual function/class name after decorators\n",
    "        lines_list = node_text.split('\\n')\n",
    "        for line in lines_list:\n",
    "            if line.strip().startswith('def '):\n",
    "                match = re.search(r'def\\s+([a-zA-Z_][a-zA-Z0-9_]*)', line)\n",
    "                if match:\n",
    "                    return f\"decorated_{match.group(1)}\"\n",
    "            elif line.strip().startswith('class '):\n",
    "                match = re.search(r'class\\s+([a-zA-Z_][a-zA-Z0-9_]*)', line)\n",
    "                if match:\n",
    "                    return f\"decorated_{match.group(1)}\"\n",
    "    \n",
    "    elif node.type == 'async_function_definition':\n",
    "        # Look for async function name: async def function_name(\n",
    "        match = re.search(r'async\\s+def\\s+([a-zA-Z_][a-zA-Z0-9_]*)', node_text)\n",
    "        if match:\n",
    "            return f\"async_{match.group(1)}\"\n",
    "    \n",
    "    elif node.type == 'if_statement':\n",
    "        # Special handling for if __name__ == \"__main__\"\n",
    "        if '__name__' in node_text and '__main__' in node_text:\n",
    "            return \"main_block\"\n",
    "        else:\n",
    "            return \"if_block\"\n",
    "    \n",
    "    elif node.type == 'try_statement':\n",
    "        return \"try_block\"\n",
    "    \n",
    "    elif node.type == 'with_statement':\n",
    "        return \"with_block\"\n",
    "    \n",
    "    elif node.type == 'for_statement':\n",
    "        return \"for_loop\"\n",
    "    \n",
    "    elif node.type == 'while_statement':\n",
    "        return \"while_loop\"\n",
    "    \n",
    "    # Fallback - try to extract first identifier\n",
    "    first_line = lines[0][:50].strip()\n",
    "    simple_match = re.search(r'\\b([a-zA-Z_][a-zA-Z0-9_]*)', first_line)\n",
    "    if simple_match:\n",
    "        return simple_match.group(1)\n",
    "    \n",
    "    return f\"{node.type}_{node.start_byte}\"\n",
    "\n",
    "def find_semantic_chunks(tree, source_code: str, file_extension: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Find semantic chunks - complete, meaningful code blocks\"\"\"\n",
    "    semantic_nodes = []\n",
    "    \n",
    "    def traverse(node, parent_types=None):\n",
    "        if parent_types is None:\n",
    "            parent_types = []\n",
    "        \n",
    "        current_parent_types = parent_types + [node.type]\n",
    "        node_text = source_code[node.start_byte:node.end_byte]\n",
    "        \n",
    "        # Python semantic boundaries\n",
    "        if file_extension == '.py':\n",
    "            # Top-level semantic boundaries\n",
    "            if node.type in ['import_statement', 'import_from_statement'] and len(parent_types) <= 1:\n",
    "                name = extract_node_name(node, source_code)\n",
    "                semantic_nodes.append({\n",
    "                    'node': node, 'name': f\"import_{name}\", 'start_byte': node.start_byte,\n",
    "                    'end_byte': node.end_byte, 'type': 'import_statement', 'content': node_text\n",
    "                })\n",
    "                return\n",
    "            \n",
    "            # Function definitions (including async)\n",
    "            elif node.type in ['function_definition', 'async_function_definition'] and len(parent_types) <= 2:\n",
    "                name = extract_node_name(node, source_code)\n",
    "                semantic_nodes.append({\n",
    "                    'node': node, 'name': name, 'start_byte': node.start_byte,\n",
    "                    'end_byte': node.end_byte, 'type': node.type, 'content': node_text\n",
    "                })\n",
    "                return\n",
    "            \n",
    "            # Class definitions\n",
    "            elif node.type == 'class_definition' and len(parent_types) <= 1:\n",
    "                name = extract_node_name(node, source_code)\n",
    "                semantic_nodes.append({\n",
    "                    'node': node, 'name': name, 'start_byte': node.start_byte,\n",
    "                    'end_byte': node.end_byte, 'type': node.type, 'content': node_text\n",
    "                })\n",
    "                return\n",
    "            \n",
    "            # Decorated definitions (functions/classes with decorators)\n",
    "            elif node.type == 'decorated_definition' and len(parent_types) <= 1:\n",
    "                name = extract_node_name(node, source_code)\n",
    "                semantic_nodes.append({\n",
    "                    'node': node, 'name': name, 'start_byte': node.start_byte,\n",
    "                    'end_byte': node.end_byte, 'type': node.type, 'content': node_text\n",
    "                })\n",
    "                return\n",
    "            \n",
    "            # Global assignments and constants\n",
    "            elif (node.type == 'assignment' and len(parent_types) <= 1 and \n",
    "                  len(node_text.strip()) > 50):  # Only substantial assignments\n",
    "                name = extract_node_name(node, source_code)\n",
    "                semantic_nodes.append({\n",
    "                    'node': node, 'name': name, 'start_byte': node.start_byte,\n",
    "                    'end_byte': node.end_byte, 'type': node.type, 'content': node_text\n",
    "                })\n",
    "                return\n",
    "            \n",
    "            # Control structures at module level\n",
    "            elif (node.type in ['if_statement', 'try_statement', 'with_statement', 'for_statement', 'while_statement'] \n",
    "                  and len(parent_types) <= 1 and len(node_text.strip()) > 100):\n",
    "                name = extract_node_name(node, source_code)\n",
    "                semantic_nodes.append({\n",
    "                    'node': node, 'name': name, 'start_byte': node.start_byte,\n",
    "                    'end_byte': node.end_byte, 'type': node.type, 'content': node_text\n",
    "                })\n",
    "                return\n",
    "        \n",
    "        # Continue traversing children\n",
    "        for child in node.children:\n",
    "            traverse(child, current_parent_types)\n",
    "    \n",
    "    traverse(tree.root_node)\n",
    "    \n",
    "    # Filter out overlapping nodes (keep the largest/most specific)\n",
    "    filtered_nodes = []\n",
    "    for node in semantic_nodes:\n",
    "        is_contained = False\n",
    "        for other in semantic_nodes:\n",
    "            if (other != node and \n",
    "                other['start_byte'] <= node['start_byte'] and \n",
    "                other['end_byte'] >= node['end_byte'] and\n",
    "                other['end_byte'] - other['start_byte'] > node['end_byte'] - node['start_byte']):\n",
    "                is_contained = True\n",
    "                break\n",
    "        \n",
    "        if not is_contained:\n",
    "            filtered_nodes.append(node)\n",
    "    \n",
    "    return filtered_nodes\n",
    "\n",
    "def create_semantic_chunks(semantic_nodes: List[Dict[str, Any]]) -> List[Chunk]:\n",
    "    \"\"\"Create chunks from semantic nodes\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    for node_info in semantic_nodes:\n",
    "        chunk = Chunk(\n",
    "            start_byte=node_info['start_byte'],\n",
    "            end_byte=node_info['end_byte'],\n",
    "            content=node_info['content'],\n",
    "            node_type=node_info['type'],\n",
    "            name=node_info['name'],\n",
    "            depth=0\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def group_small_chunks(chunks: List[Chunk], target_tokens: int = 600, file_extension: str = '') -> List[Chunk]:\n",
    "    \"\"\"Group small chunks together to reach reasonable size for Python files\"\"\"\n",
    "    if not chunks:\n",
    "        return chunks\n",
    "    \n",
    "    # Separate imports from other chunks\n",
    "    import_chunks = [c for c in chunks if c.node_type in ['import_statement', 'import_from_statement']]\n",
    "    other_chunks = [c for c in chunks if c.node_type not in ['import_statement', 'import_from_statement']]\n",
    "    \n",
    "    # Check if everything together is under limit\n",
    "    total_tokens = sum(c.token_count for c in chunks)\n",
    "    if total_tokens <= MAX_CHUNK_TOKENS:\n",
    "        # Combine everything into one chunk\n",
    "        combined_content = '\\n\\n'.join(c.content for c in chunks)\n",
    "        combined_chunk = Chunk(\n",
    "            start_byte=chunks[0].start_byte,\n",
    "            end_byte=chunks[-1].end_byte,\n",
    "            content=combined_content,\n",
    "            node_type='complete_module',\n",
    "            name=f\"complete_module_{len(chunks)}_parts\",\n",
    "            depth=0\n",
    "        )\n",
    "        return [combined_chunk]\n",
    "    \n",
    "    # If too large, handle imports separately\n",
    "    if import_chunks:\n",
    "        total_import_tokens = sum(c.token_count for c in import_chunks)\n",
    "        if total_import_tokens <= MAX_CHUNK_TOKENS:\n",
    "            # Combine all imports into one chunk\n",
    "            combined_imports = '\\n'.join(c.content for c in import_chunks)\n",
    "            imports_chunk = Chunk(\n",
    "                start_byte=import_chunks[0].start_byte,\n",
    "                end_byte=import_chunks[-1].end_byte,\n",
    "                content=combined_imports,\n",
    "                node_type='imports_group',\n",
    "                name=f\"imports_{len(import_chunks)}_statements\",\n",
    "                depth=0\n",
    "            )\n",
    "            import_chunks = [imports_chunk]\n",
    "    \n",
    "    # Group other chunks\n",
    "    grouped_others = group_chunks_by_size(other_chunks, target_tokens, file_extension)\n",
    "    \n",
    "    # Combine imports + other chunks\n",
    "    return import_chunks + grouped_others\n",
    "\n",
    "def group_chunks_by_size(chunks: List[Chunk], target_tokens: int = 600, file_extension: str = '') -> List[Chunk]:\n",
    "    \"\"\"Group chunks by size logic\"\"\"\n",
    "    if not chunks:\n",
    "        return chunks\n",
    "    \n",
    "    # Skip grouping if we already have reasonably sized chunks\n",
    "    if len(chunks) == 1 or any(c.token_count > target_tokens for c in chunks):\n",
    "        total_tokens = sum(c.token_count for c in chunks)\n",
    "        if total_tokens <= MAX_CHUNK_TOKENS:\n",
    "            # All chunks together are still under limit - combine them\n",
    "            if len(chunks) > 1:\n",
    "                combined_content = '\\n\\n'.join(c.content for c in chunks)\n",
    "                group_name = f\"python_module_{len(chunks)}_definitions\"\n",
    "                \n",
    "                combined_chunk = Chunk(\n",
    "                    start_byte=chunks[0].start_byte,\n",
    "                    end_byte=chunks[-1].end_byte,\n",
    "                    content=combined_content,\n",
    "                    node_type='grouped_content',\n",
    "                    name=group_name,\n",
    "                    depth=0\n",
    "                )\n",
    "                return [combined_chunk]\n",
    "        return chunks\n",
    "    \n",
    "    # Group small chunks\n",
    "    grouped_chunks = []\n",
    "    current_group = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if current_tokens + chunk.token_count > target_tokens and current_group:\n",
    "            # Finalize current group\n",
    "            group_content = '\\n\\n'.join(c.content for c in current_group)\n",
    "            group_name = f\"python_group_{len(current_group)}_definitions\"\n",
    "            \n",
    "            grouped_chunk = Chunk(\n",
    "                start_byte=current_group[0].start_byte,\n",
    "                end_byte=current_group[-1].end_byte,\n",
    "                content=group_content,\n",
    "                node_type='grouped_content',\n",
    "                name=group_name,\n",
    "                depth=0\n",
    "            )\n",
    "            grouped_chunks.append(grouped_chunk)\n",
    "            \n",
    "            # Start new group\n",
    "            current_group = [chunk]\n",
    "            current_tokens = chunk.token_count\n",
    "        else:\n",
    "            current_group.append(chunk)\n",
    "            current_tokens += chunk.token_count\n",
    "    \n",
    "    # Add final group\n",
    "    if current_group:\n",
    "        if len(current_group) == 1:\n",
    "            grouped_chunks.append(current_group[0])\n",
    "        else:\n",
    "            group_content = '\\n\\n'.join(c.content for c in current_group)\n",
    "            group_name = f\"python_group_{len(current_group)}_definitions\"\n",
    "            \n",
    "            grouped_chunk = Chunk(\n",
    "                start_byte=current_group[0].start_byte,\n",
    "                end_byte=current_group[-1].end_byte,\n",
    "                content=group_content,\n",
    "                node_type='grouped_content',\n",
    "                name=group_name,\n",
    "                depth=0\n",
    "            )\n",
    "            grouped_chunks.append(grouped_chunk)\n",
    "    \n",
    "    return grouped_chunks\n",
    "\n",
    "def sub_chunk_by_statements(chunk: Chunk, tree, source_code: str, depth: int = 0) -> List[Chunk]:\n",
    "    \"\"\"Sub-chunk by breaking down into logical statements/blocks\"\"\"\n",
    "    if depth >= MAX_RECURSION_DEPTH or chunk.token_count <= MAX_CHUNK_TOKENS:\n",
    "        return [chunk]\n",
    "    \n",
    "    print(f\"    Breaking down {chunk.name} ({chunk.token_count} tokens) into smaller pieces...\")\n",
    "    \n",
    "    # Simple line-based splitting for now\n",
    "    lines = chunk.content.split('\\n')\n",
    "    sub_chunks = []\n",
    "    current_lines = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        line_tokens = count_tokens(line)\n",
    "        \n",
    "        if current_size + line_tokens > MAX_CHUNK_TOKENS and current_lines:\n",
    "            # Create sub-chunk\n",
    "            sub_content = '\\n'.join(current_lines)\n",
    "            if sub_content.strip():\n",
    "                sub_chunk = Chunk(\n",
    "                    start_byte=chunk.start_byte,  # Approximate\n",
    "                    end_byte=chunk.start_byte + len(sub_content),\n",
    "                    content=sub_content,\n",
    "                    node_type=f\"{chunk.node_type}_part\",\n",
    "                    name=f\"{chunk.name}_part_{len(sub_chunks)+1}\",\n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                sub_chunks.append(sub_chunk)\n",
    "            \n",
    "            current_lines = [line]\n",
    "            current_size = line_tokens\n",
    "        else:\n",
    "            current_lines.append(line)\n",
    "            current_size += line_tokens\n",
    "    \n",
    "    # Add remaining lines\n",
    "    if current_lines:\n",
    "        sub_content = '\\n'.join(current_lines)\n",
    "        if sub_content.strip():\n",
    "            sub_chunk = Chunk(\n",
    "                start_byte=chunk.start_byte,\n",
    "                end_byte=chunk.end_byte,\n",
    "                content=sub_content,\n",
    "                node_type=f\"{chunk.node_type}_part\",\n",
    "                name=f\"{chunk.name}_part_{len(sub_chunks)+1}\",\n",
    "                depth=depth + 1\n",
    "            )\n",
    "            sub_chunks.append(sub_chunk)\n",
    "    \n",
    "    chunk.sub_chunks = sub_chunks\n",
    "    return sub_chunks if len(sub_chunks) > 1 else [chunk]\n",
    "\n",
    "def process_python_file(file_path: Path) -> List[Chunk]:\n",
    "    \"\"\"Process a single .py file and return semantic chunks\"\"\"\n",
    "    print(f\"\\n=== Processing: {file_path.name} ===\")\n",
    "    \n",
    "    # Read file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        source_code = f.read()\n",
    "    \n",
    "    print(f\"File size: {len(source_code)} characters\")\n",
    "    \n",
    "    # Parse with Tree-sitter Python parser\n",
    "    tree = python_parser.parse(source_code.encode('utf-8'))\n",
    "    \n",
    "    if tree.root_node.has_error:\n",
    "        print(\"⚠️ Parse errors detected\")\n",
    "    \n",
    "    # Find semantic chunks\n",
    "    semantic_nodes = find_semantic_chunks(tree, source_code, file_path.suffix)\n",
    "    print(f\"Found {len(semantic_nodes)} semantic units\")\n",
    "    \n",
    "    # Show what we found\n",
    "    for node in semantic_nodes:\n",
    "        preview = node['content'][:100].replace('\\n', ' ').strip()\n",
    "        print(f\"  - {node['type']}: {node['name']} ({count_tokens(node['content'])} tokens)\")\n",
    "        print(f\"    Preview: {preview}...\")\n",
    "    \n",
    "    # Create chunks\n",
    "    base_chunks = create_semantic_chunks(semantic_nodes)\n",
    "    \n",
    "    # Group small chunks\n",
    "    base_chunks = group_small_chunks(base_chunks, target_tokens=600, file_extension=file_path.suffix)\n",
    "    \n",
    "    print(f\"Created {len(base_chunks)} semantic chunks\")\n",
    "    \n",
    "    # Apply sub-chunking for oversized chunks\n",
    "    final_chunks = []\n",
    "    oversized_count = 0\n",
    "    \n",
    "    for chunk in base_chunks:\n",
    "        if chunk.token_count > MAX_CHUNK_TOKENS:\n",
    "            print(f\"  Sub-chunking {chunk.name} ({chunk.token_count} tokens)\")\n",
    "            sub_chunks = sub_chunk_by_statements(chunk, tree, source_code)\n",
    "            final_chunks.extend(sub_chunks)\n",
    "            oversized_count += 1\n",
    "        else:\n",
    "            final_chunks.append(chunk)\n",
    "    \n",
    "    if oversized_count > 0:\n",
    "        print(f\"Sub-chunked {oversized_count} oversized chunks\")\n",
    "    print(f\"Final result: {len(final_chunks)} total chunks\")\n",
    "    \n",
    "    return final_chunks\n",
    "\n",
    "def generate_unique_id(length: int = 6) -> str:\n",
    "    \"\"\"Generate a random unique ID\"\"\"\n",
    "    alphabet = string.ascii_lowercase + string.digits\n",
    "    return ''.join(secrets.choice(alphabet) for _ in range(length))\n",
    "\n",
    "def create_chunk_filename(original_filename: str, chunk_number: int, unique_id: str) -> str:\n",
    "    \"\"\"Create chunk filename: script.py_chunk_001_a1s2d3.md\"\"\"\n",
    "    return f\"{original_filename}_chunk_{chunk_number:03d}_{unique_id}.md\"\n",
    "\n",
    "def get_markdown_language(file_extension: str) -> str:\n",
    "    \"\"\"Get markdown language for code blocks\"\"\"\n",
    "    lang_map = {\n",
    "        '.py': 'python'\n",
    "    }\n",
    "    return lang_map.get(file_extension, 'text')\n",
    "\n",
    "def create_chunk_markdown(chunk: Chunk, source_file_path: str, file_extension: str) -> str:\n",
    "    \"\"\"Create markdown content with YAML frontmatter\"\"\"\n",
    "    language = get_markdown_language(file_extension)\n",
    "    unique_id = generate_unique_id()\n",
    "    \n",
    "    frontmatter = f\"\"\"---\n",
    "file_path: \"{source_file_path}\"\n",
    "chunk_id: \"{unique_id}\"\n",
    "chunk_type: \"{chunk.node_type}\"\n",
    "chunk_name: \"{chunk.name}\"\n",
    "start_byte: {chunk.start_byte}\n",
    "end_byte: {chunk.end_byte}\n",
    "token_count: {chunk.token_count}\n",
    "depth: {chunk.depth}\n",
    "language: \"{language}\"\n",
    "---\n",
    "\n",
    "# {chunk.name}\n",
    "\n",
    "**Type:** {chunk.node_type}  \n",
    "**Tokens:** {chunk.token_count}  \n",
    "**Depth:** {chunk.depth}\n",
    "\n",
    "```{language}\n",
    "{chunk.content}\n",
    "```\n",
    "\"\"\"\n",
    "    return frontmatter\n",
    "\n",
    "def print_chunk_summary(chunks: List[Chunk], file_name: str):\n",
    "    \"\"\"Print detailed summary of chunks\"\"\"\n",
    "    print(f\"\\n--- Semantic Chunk Summary for {file_name} ---\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        indent = \"  \" * chunk.depth\n",
    "        content_lines = len(chunk.content.split('\\n'))\n",
    "        \n",
    "        print(f\"{indent}{i}. {chunk.name}\")\n",
    "        print(f\"{indent}   Type: {chunk.node_type}\")\n",
    "        print(f\"{indent}   Size: {chunk.token_count} tokens, {content_lines} lines\")\n",
    "        print(f\"{indent}   Content preview:\")\n",
    "        \n",
    "        # Show first few lines of actual content\n",
    "        content_lines_list = chunk.content.split('\\n')\n",
    "        for j, line in enumerate(content_lines_list[:3]):\n",
    "            print(f\"{indent}     {line.strip()}\")\n",
    "        if len(content_lines_list) > 3:\n",
    "            print(f\"{indent}     ... ({len(content_lines_list) - 3} more lines)\")\n",
    "        print()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to test semantic chunking\"\"\"\n",
    "    print(\"🚀 Python Semantic Chunking Test\")\n",
    "    print(f\"Max chunk tokens: {MAX_CHUNK_TOKENS}\")\n",
    "    print(f\"Max recursion depth: {MAX_RECURSION_DEPTH}\")\n",
    "    print(f\"Supported extensions: {', '.join(SUPPORTED_EXTENSIONS)}\")\n",
    "    \n",
    "    # Get directory from user or use current directory\n",
    "    directory = input(\"\\nEnter directory path (or press Enter for current directory): \").strip()\n",
    "    if not directory:\n",
    "        directory = \".\"\n",
    "    \n",
    "    target_dir = Path(directory)\n",
    "    if not target_dir.exists():\n",
    "        print(f\"❌ Directory not found: {directory}\")\n",
    "        return\n",
    "    \n",
    "    # Find all Python files\n",
    "    all_files = list(target_dir.rglob(\"*.py\"))\n",
    "    \n",
    "    if not all_files:\n",
    "        print(f\"❌ No Python files found in {directory}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📁 Found {len(all_files)} Python files:\")\n",
    "    \n",
    "    # Group by directory for summary\n",
    "    by_dir = {}\n",
    "    for f in all_files:\n",
    "        rel_path = f.relative_to(target_dir)\n",
    "        dir_path = str(rel_path.parent) if rel_path.parent != Path('.') else '.'\n",
    "        by_dir[dir_path] = by_dir.get(dir_path, []) + [f.name]\n",
    "    \n",
    "    for dir_path, files in sorted(by_dir.items()):\n",
    "        print(f\"  📂 {dir_path}: {len(files)} files\")\n",
    "        for file_name in sorted(files)[:3]:  # Show first 3 files\n",
    "            print(f\"    📄 {file_name}\")\n",
    "        if len(files) > 3:\n",
    "            print(f\"    ... and {len(files) - 3} more\")\n",
    "    \n",
    "    # Interactive file selection\n",
    "    print(f\"\\n🔍 Select files to process:\")\n",
    "    print(\"1. Process all files\")\n",
    "    print(\"2. Select specific files\")\n",
    "    print(\"3. Process files in a specific directory\")\n",
    "    \n",
    "    choice = input(\"\\nEnter your choice (1-3): \").strip()\n",
    "    \n",
    "    selected_files = []\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        selected_files = all_files\n",
    "        print(f\"✅ Processing all {len(selected_files)} files\")\n",
    "    \n",
    "    elif choice == \"2\":\n",
    "        print(\"\\nAvailable files:\")\n",
    "        for i, f in enumerate(all_files, 1):\n",
    "            rel_path = f.relative_to(target_dir)\n",
    "            print(f\"  {i:2d}. {rel_path}\")\n",
    "        \n",
    "        selection = input(\"\\nEnter file numbers (comma-separated, e.g., 1,3,5): \").strip()\n",
    "        try:\n",
    "            indices = [int(x.strip()) - 1 for x in selection.split(',')]\n",
    "            selected_files = [all_files[i] for i in indices if 0 <= i < len(all_files)]\n",
    "            print(f\"✅ Selected {len(selected_files)} files\")\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"❌ Invalid selection, processing first file only\")\n",
    "            selected_files = [all_files[0]] if all_files else []\n",
    "    \n",
    "    elif choice == \"3\":\n",
    "        print(\"\\nAvailable directories:\")\n",
    "        unique_dirs = sorted(set(by_dir.keys()))\n",
    "        for i, dir_path in enumerate(unique_dirs, 1):\n",
    "            print(f\"  {i}. {dir_path} ({len(by_dir[dir_path])} files)\")\n",
    "        \n",
    "        try:\n",
    "            dir_choice = int(input(\"\\nEnter directory number: \").strip()) - 1\n",
    "            if 0 <= dir_choice < len(unique_dirs):\n",
    "                selected_dir = unique_dirs[dir_choice]\n",
    "                selected_files = [f for f in all_files \n",
    "                                if str(f.relative_to(target_dir).parent) == selected_dir or \n",
    "                                   (selected_dir == '.' and f.parent == target_dir)]\n",
    "                print(f\"✅ Selected {len(selected_files)} files from {selected_dir}\")\n",
    "            else:\n",
    "                print(\"❌ Invalid directory, processing first file only\")\n",
    "                selected_files = [all_files[0]] if all_files else []\n",
    "        except ValueError:\n",
    "            print(\"❌ Invalid input, processing first file only\")\n",
    "            selected_files = [all_files[0]] if all_files else []\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ Invalid choice, processing first file only\")\n",
    "        selected_files = [all_files[0]] if all_files else []\n",
    "    \n",
    "    if not selected_files:\n",
    "        print(\"❌ No files selected for processing\")\n",
    "        return\n",
    "    \n",
    "    # Process selected files\n",
    "    print(f\"\\n🔄 Processing {len(selected_files)} file(s)...\")\n",
    "    all_chunks = {}\n",
    "    \n",
    "    for file_path in selected_files:\n",
    "        try:\n",
    "            chunks = process_python_file(file_path)\n",
    "            all_chunks[file_path] = chunks\n",
    "            print_chunk_summary(chunks, file_path.name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Summary\n",
    "    total_chunks = sum(len(chunks) for chunks in all_chunks.values())\n",
    "    total_tokens = sum(chunk.token_count for chunks in all_chunks.values() for chunk in chunks)\n",
    "    \n",
    "    print(f\"\\n📊 Final Summary:\")\n",
    "    print(f\"   Files processed: {len(all_chunks)}\")\n",
    "    print(f\"   Total chunks: {total_chunks}\")\n",
    "    print(f\"   Total tokens: {total_tokens:,}\")\n",
    "    print(f\"   Average tokens per chunk: {total_tokens/total_chunks:.1f}\" if total_chunks > 0 else \"   No chunks created\")\n",
    "    \n",
    "    # Ask about saving chunks\n",
    "    save_chunks = input(f\"\\n💾 Save chunks to markdown files? (y/n): \").strip().lower()\n",
    "    if save_chunks == 'y':\n",
    "        output_dir = Path(\"python_chunks\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        saved_count = 0\n",
    "        for file_path, chunks in all_chunks.items():\n",
    "            for i, chunk in enumerate(chunks, 1):\n",
    "                chunk_filename = create_chunk_filename(file_path.name, i, generate_unique_id())\n",
    "                chunk_content = create_chunk_markdown(chunk, str(file_path), file_path.suffix)\n",
    "                \n",
    "                output_path = output_dir / chunk_filename\n",
    "                with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(chunk_content)\n",
    "                saved_count += 1\n",
    "        \n",
    "        print(f\"✅ Saved {saved_count} chunk files to {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c358e88",
   "metadata": {},
   "source": [
    "Key Changes Made:\n",
    "1. Parallel Output Directory Structure\n",
    "\n",
    "Creates output directory parallel to source directory: source_dir → source_dir_chunks\n",
    "Uses target_dir.parent / f\"{target_dir.name}_chunks\" to ensure parallel placement\n",
    "Preserves the entire directory structure within the output directory\n",
    "\n",
    "2. Automatic Processing\n",
    "\n",
    "Removes all interactive file selection options\n",
    "Automatically processes all .py files found in the directory tree\n",
    "No user prompts for file selection or save confirmation\n",
    "\n",
    "3. Directory Structure Preservation\n",
    "\n",
    "Calculates relative paths from source directory: rel_path = file_path.relative_to(target_dir)\n",
    "Creates corresponding subdirectories in output: chunk_dir = output_dir / rel_path.parent\n",
    "Uses mkdir(parents=True, exist_ok=True) to ensure all parent directories are created\n",
    "\n",
    "4. Consistent Naming Convention\n",
    "\n",
    "Follows the same naming pattern as TypeScript chunker: filename.py_chunk_001_a1s2d3.md\n",
    "Uses relative paths in YAML frontmatter for portability\n",
    "Maintains the same markdown structure with YAML frontmatter\n",
    "\n",
    "5. Streamlined Output\n",
    "\n",
    "Automatic markdown output by default\n",
    "Clear progress reporting during processing\n",
    "Final summary with directory structure confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7df069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using Python parser\n",
      "🚀 Python Semantic Chunking\n",
      "Max chunk tokens: 1000\n",
      "Max recursion depth: 3\n",
      "Supported extensions: .py\n",
      "📁 Output directory: /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/code_sources/python/fusion_latest_chunks\n",
      "📁 Found 20 Python files:\n",
      "  📂 py_src/fusion: 18 files\n",
      "    📄 __init__.py\n",
      "    📄 __main__.py\n",
      "    📄 attributes.py\n",
      "    ... and 15 more\n",
      "  📂 py_src/fusion/_legacy: 2 files\n",
      "    📄 __init__.py\n",
      "    📄 authentication.py\n",
      "\n",
      "🔄 Processing all 20 file(s)...\n",
      "\n",
      "=== Processing: fusion_types.py ===\n",
      "File size: 385 characters\n",
      "Found 2 semantic units\n",
      "  - import_statement: import_from_enum (4 tokens)\n",
      "    Preview: from enum import Enum...\n",
      "  - class_definition: Types (112 tokens)\n",
      "    Preview: class Types(Enum):     \"\"\"Fusion types.      Args:         Enum (class: `enum.Enum`): Enum inheritan...\n",
      "Created 1 semantic chunks\n",
      "Final result: 1 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for fusion_types.py ---\n",
      "1. complete_module_2_parts\n",
      "   Type: complete_module\n",
      "   Size: 117 tokens, 22 lines\n",
      "   Content preview:\n",
      "     from enum import Enum\n",
      "     \n",
      "     class Types(Enum):\n",
      "     ... (19 more lines)\n",
      "\n",
      "\n",
      "=== Processing: report_attributes.py ===\n",
      "File size: 7972 characters\n",
      "Found 6 semantic units\n",
      "  - import_statement: import_from_dataclasses (8 tokens)\n",
      "    Preview: from dataclasses import dataclass, field...\n",
      "  - import_statement: import_from_typing (12 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any, Union, cast...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_from_fusion.utils (23 tokens)\n",
      "    Preview: from fusion.utils import (     CamelCaseMeta,     camel_to_snake,     requests_raise_for_status, )...\n",
      "  - decorated_definition: decorated_ReportAttribute (441 tokens)\n",
      "    Preview: @dataclass class ReportAttribute(metaclass=CamelCaseMeta):     title: str     sourceIdentifier: str...\n",
      "  - decorated_definition: decorated_ReportAttributes (1209 tokens)\n",
      "    Preview: @dataclass class ReportAttributes:     attributes: list[ReportAttribute] = field(default_factory=lis...\n",
      "Created 3 semantic chunks\n",
      "  Sub-chunking decorated_ReportAttributes (1209 tokens)\n",
      "    Breaking down decorated_ReportAttributes (1209 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 4 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for report_attributes.py ---\n",
      "1. imports_4_statements\n",
      "   Type: imports_group\n",
      "   Size: 50 tokens, 8 lines\n",
      "   Content preview:\n",
      "     from dataclasses import dataclass, field\n",
      "     from typing import TYPE_CHECKING, Any, Union, cast\n",
      "     import pandas as pd\n",
      "     ... (5 more lines)\n",
      "\n",
      "2. decorated_ReportAttribute\n",
      "   Type: decorated_definition\n",
      "   Size: 441 tokens, 52 lines\n",
      "   Content preview:\n",
      "     @dataclass\n",
      "     class ReportAttribute(metaclass=CamelCaseMeta):\n",
      "     title: str\n",
      "     ... (49 more lines)\n",
      "\n",
      "  3. decorated_ReportAttributes_part_1\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1023 tokens, 140 lines\n",
      "     Content preview:\n",
      "       @dataclass\n",
      "       class ReportAttributes:\n",
      "       attributes: list[ReportAttribute] = field(default_factory=list)\n",
      "       ... (137 more lines)\n",
      "\n",
      "  4. decorated_ReportAttributes_part_2\n",
      "     Type: decorated_definition_part\n",
      "     Size: 186 tokens, 22 lines\n",
      "     Content preview:\n",
      "       ) -> requests.Response | None:\n",
      "       \"\"\"\n",
      "       Create the ReportAttributes to the core-lineage API.\n",
      "       ... (19 more lines)\n",
      "\n",
      "\n",
      "=== Processing: fusion_filesystem.py ===\n",
      "File size: 40652 characters\n",
      "Found 24 semantic units\n",
      "  - import_statement: import_import_asyncio (2 tokens)\n",
      "    Preview: import asyncio...\n",
      "  - import_statement: import_import_base64 (3 tokens)\n",
      "    Preview: import base64...\n",
      "  - import_statement: import_import_hashlib (2 tokens)\n",
      "    Preview: import hashlib...\n",
      "  - import_statement: import_import_io (2 tokens)\n",
      "    Preview: import io...\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_import_time (2 tokens)\n",
      "    Preview: import time...\n",
      "  - import_statement: import_from_collections.abc (9 tokens)\n",
      "    Preview: from collections.abc import AsyncGenerator, Generator...\n",
      "  - import_statement: import_from_copy (4 tokens)\n",
      "    Preview: from copy import deepcopy...\n",
      "  - import_statement: import_from_pathlib (4 tokens)\n",
      "    Preview: from pathlib import Path...\n",
      "  - import_statement: import_from_typing (8 tokens)\n",
      "    Preview: from typing import Any, Optional, Union...\n",
      "  - import_statement: import_from_urllib.parse (8 tokens)\n",
      "    Preview: from urllib.parse import quote, urljoin...\n",
      "  - import_statement: import_import_aiohttp (3 tokens)\n",
      "    Preview: import aiohttp...\n",
      "  - import_statement: import_import_fsspec (4 tokens)\n",
      "    Preview: import fsspec...\n",
      "  - import_statement: import_import_fsspec.asyn (6 tokens)\n",
      "    Preview: import fsspec.asyn...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_import_requests (2 tokens)\n",
      "    Preview: import requests...\n",
      "  - import_statement: import_from_fsspec.callbacks (9 tokens)\n",
      "    Preview: from fsspec.callbacks import _DEFAULT_CALLBACK...\n",
      "  - import_statement: import_from_fsspec.implementations.http (19 tokens)\n",
      "    Preview: from fsspec.implementations.http import HTTPFile, HTTPFileSystem, sync, sync_wrapper...\n",
      "  - import_statement: import_from_fsspec.utils (8 tokens)\n",
      "    Preview: from fsspec.utils import nullcontext...\n",
      "  - import_statement: import_from_fusion._fusion (7 tokens)\n",
      "    Preview: from fusion._fusion import FusionCredentials...\n",
      "  - import_statement: import_from_fusion.exceptions (7 tokens)\n",
      "    Preview: from fusion.exceptions import APIResponseError...\n",
      "  - import_statement: import_from_.utils (16 tokens)\n",
      "    Preview: from .utils import cpu_count, get_client, get_default_fs, get_session...\n",
      "  - class_definition: FusionHTTPFileSystem (8357 tokens)\n",
      "    Preview: class FusionHTTPFileSystem(HTTPFileSystem):  # type: ignore     \"\"\"Fusion HTTP filesystem.\"\"\"      d...\n",
      "  - class_definition: FusionFile (431 tokens)\n",
      "    Preview: class FusionFile(HTTPFile):  # type: ignore     \"\"\"Fusion File.\"\"\"      def __init__(self, *args: An...\n",
      "Created 3 semantic chunks\n",
      "  Sub-chunking FusionHTTPFileSystem (8357 tokens)\n",
      "    Breaking down FusionHTTPFileSystem (8357 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 11 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for fusion_filesystem.py ---\n",
      "1. imports_22_statements\n",
      "   Type: imports_group\n",
      "   Size: 152 tokens, 22 lines\n",
      "   Content preview:\n",
      "     import asyncio\n",
      "     import base64\n",
      "     import hashlib\n",
      "     ... (19 more lines)\n",
      "\n",
      "  2. FusionHTTPFileSystem_part_1\n",
      "     Type: class_definition_part\n",
      "     Size: 1019 tokens, 119 lines\n",
      "     Content preview:\n",
      "       class FusionHTTPFileSystem(HTTPFileSystem):  # type: ignore\n",
      "       \"\"\"Fusion HTTP filesystem.\"\"\"\n",
      "       \n",
      "       ... (116 more lines)\n",
      "\n",
      "  3. FusionHTTPFileSystem_part_2\n",
      "     Type: class_definition_part\n",
      "     Size: 1001 tokens, 122 lines\n",
      "     Content preview:\n",
      "       except BaseException:\n",
      "       logger.exception(VERBOSE_LVL, f\"{url} cannot be parsed to json\")\n",
      "       out = {}\n",
      "       ... (119 more lines)\n",
      "\n",
      "  4. FusionHTTPFileSystem_part_3\n",
      "     Type: class_definition_part\n",
      "     Size: 995 tokens, 135 lines\n",
      "     Content preview:\n",
      "       k[\"name\"] = k[\"name\"].split(f\"{self.client_kwargs['root_url']}catalogs/\")[-1]\n",
      "       \n",
      "       elif not keep_protocol:\n",
      "       ... (132 more lines)\n",
      "\n",
      "  5. FusionHTTPFileSystem_part_4\n",
      "     Type: class_definition_part\n",
      "     Size: 1011 tokens, 129 lines\n",
      "     Content preview:\n",
      "       except Exception as ex:  # noqa: BLE001, PERF203\n",
      "       if attempt < retries - 1:\n",
      "       wait_time = 2**attempt  # Exponential backoff\n",
      "       ... (126 more lines)\n",
      "\n",
      "  6. FusionHTTPFileSystem_part_5\n",
      "     Type: class_definition_part\n",
      "     Size: 1008 tokens, 112 lines\n",
      "     Content preview:\n",
      "       **kwargs: Any,\n",
      "       ) -> Any:\n",
      "       \"\"\"Download file(s) from remote to local.\n",
      "       ... (109 more lines)\n",
      "\n",
      "  7. FusionHTTPFileSystem_part_6\n",
      "     Type: class_definition_part\n",
      "     Size: 997 tokens, 109 lines\n",
      "     Content preview:\n",
      "       additional_headers: Optional[dict[str, str]] = None,\n",
      "       **kwargs: Any,\n",
      "       ) -> None:\n",
      "       ... (106 more lines)\n",
      "\n",
      "  8. FusionHTTPFileSystem_part_7\n",
      "     Type: class_definition_part\n",
      "     Size: 1006 tokens, 105 lines\n",
      "     Content preview:\n",
      "       headers[\"Digest\"] = \"SHA-256=\" + base64.b64encode(hash_sha256.digest()).decode()\n",
      "       else:\n",
      "       headers[\"Digest\"] = \"SHA-256=\" + base64.b64encode(hash_sha256_chunk.digest()).decode()\n",
      "       ... (102 more lines)\n",
      "\n",
      "  9. FusionHTTPFileSystem_part_8\n",
      "     Type: class_definition_part\n",
      "     Size: 1004 tokens, 128 lines\n",
      "     Content preview:\n",
      "       resps = list(put_data())\n",
      "       hash_sha256 = hash_sha256_lst[0]\n",
      "       headers[\"Digest\"] = \"SHA-256=\" + base64.b64encode(hash_sha256.digest()).decode()\n",
      "       ... (125 more lines)\n",
      "\n",
      "  10. FusionHTTPFileSystem_part_9\n",
      "     Type: class_definition_part\n",
      "     Size: 315 tokens, 46 lines\n",
      "     Content preview:\n",
      "       block_size: Optional[int] = None,\n",
      "       _autocommit: Optional[bool] = None,\n",
      "       cache_type: None = None,\n",
      "       ... (43 more lines)\n",
      "\n",
      "11. FusionFile\n",
      "   Type: class_definition\n",
      "   Size: 431 tokens, 48 lines\n",
      "   Content preview:\n",
      "     class FusionFile(HTTPFile):  # type: ignore\n",
      "     \"\"\"Fusion File.\"\"\"\n",
      "     \n",
      "     ... (45 more lines)\n",
      "\n",
      "\n",
      "=== Processing: attributes.py ===\n",
      "File size: 38138 characters\n",
      "Found 8 semantic units\n",
      "  - import_statement: import_from_dataclasses (10 tokens)\n",
      "    Preview: from dataclasses import dataclass, field, fields...\n",
      "  - import_statement: import_from_typing (10 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any, cast...\n",
      "  - import_statement: import_import_numpy (4 tokens)\n",
      "    Preview: import numpy as np...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_from_fusion.fusion_types (7 tokens)\n",
      "    Preview: from fusion.fusion_types import Types...\n",
      "  - import_statement: import_from_fusion.utils (42 tokens)\n",
      "    Preview: from fusion.utils import (     CamelCaseMeta,     camel_to_snake,     convert_date_format,     make_...\n",
      "  - decorated_definition: decorated_Attribute (4324 tokens)\n",
      "    Preview: @dataclass class Attribute(metaclass=CamelCaseMeta):     \"\"\"Fusion Attribute class for managing attr...\n",
      "  - decorated_definition: decorated_Attributes (3171 tokens)\n",
      "    Preview: @dataclass class Attributes:     \"\"\"Class representing a collection of Attribute instances for manag...\n",
      "Created 3 semantic chunks\n",
      "  Sub-chunking decorated_Attribute (4324 tokens)\n",
      "    Breaking down decorated_Attribute (4324 tokens) into smaller pieces...\n",
      "  Sub-chunking decorated_Attributes (3171 tokens)\n",
      "    Breaking down decorated_Attributes (3171 tokens) into smaller pieces...\n",
      "Sub-chunked 2 oversized chunks\n",
      "Final result: 10 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for attributes.py ---\n",
      "1. imports_6_statements\n",
      "   Type: imports_group\n",
      "   Size: 82 tokens, 14 lines\n",
      "   Content preview:\n",
      "     from dataclasses import dataclass, field, fields\n",
      "     from typing import TYPE_CHECKING, Any, cast\n",
      "     import numpy as np\n",
      "     ... (11 more lines)\n",
      "\n",
      "  2. decorated_Attribute_part_1\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1020 tokens, 78 lines\n",
      "     Content preview:\n",
      "       @dataclass\n",
      "       class Attribute(metaclass=CamelCaseMeta):\n",
      "       \"\"\"Fusion Attribute class for managing attributes metadata in a Fusion catalog.\n",
      "       ... (75 more lines)\n",
      "\n",
      "  3. decorated_Attribute_part_2\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1013 tokens, 116 lines\n",
      "     Content preview:\n",
      "       self.available_from = convert_date_format(self.available_from) if self.available_from else None\n",
      "       self.deprecated_from = convert_date_format(self.deprecated_from) if self.deprecated_from else None\n",
      "       self.data_type = Types[str(self.data_type).strip().rsplit(\".\", maxsplit=1)[-1].title()]\n",
      "       ... (113 more lines)\n",
      "\n",
      "  4. decorated_Attribute_part_3\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1009 tokens, 130 lines\n",
      "     Content preview:\n",
      "       multiplier=series.get(\"multiplier\", 1.0),\n",
      "       is_propagation_eligible=is_propagation_eligible,\n",
      "       is_metric=is_metric,\n",
      "       ... (127 more lines)\n",
      "\n",
      "  5. decorated_Attribute_part_4\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1002 tokens, 114 lines\n",
      "     Content preview:\n",
      "       If instantiated from a Fusion object, then the client is set automatically.\n",
      "       catalog (str, optional): A catalog identifier. Defaults to None.\n",
      "       return_resp_obj (bool, optional): If True then return the response object. Defaults to False.\n",
      "       ... (111 more lines)\n",
      "\n",
      "  6. decorated_Attribute_part_5\n",
      "     Type: decorated_definition_part\n",
      "     Size: 280 tokens, 32 lines\n",
      "     Content preview:\n",
      "       >>> my_attr2 = fusion.attribute(identifier=\"my_attribute2\", index=0, application_id=\"12345\")\n",
      "       >>> my_attr3 = fusion.attribute(identifier=\"my_attribute3\", index=0, application_id=\"12345\")\n",
      "       >>> attrs = [my_attr1, my_attr2]\n",
      "       ... (29 more lines)\n",
      "\n",
      "  7. decorated_Attributes_part_1\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1020 tokens, 164 lines\n",
      "     Content preview:\n",
      "       @dataclass\n",
      "       class Attributes:\n",
      "       \"\"\"Class representing a collection of Attribute instances for managing atrribute metadata in a Fusion catalog.\n",
      "       ... (161 more lines)\n",
      "\n",
      "  8. decorated_Attributes_part_2\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1006 tokens, 135 lines\n",
      "     Content preview:\n",
      "       @classmethod\n",
      "       def _from_dataframe(cls: type[Attributes], data: pd.DataFrame) -> Attributes:\n",
      "       \"\"\"Create an Attributes instance from a pandas DataFrame.\n",
      "       ... (132 more lines)\n",
      "\n",
      "  9. decorated_Attributes_part_3\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1009 tokens, 124 lines\n",
      "     Content preview:\n",
      "       response = client.session.get(url)\n",
      "       requests_raise_for_status(response)\n",
      "       list_attributes = response.json()[\"resources\"]\n",
      "       ... (121 more lines)\n",
      "\n",
      "  10. decorated_Attributes_part_4\n",
      "     Type: decorated_definition_part\n",
      "     Size: 136 tokens, 21 lines\n",
      "     Content preview:\n",
      "       otherwise None.\n",
      "       \n",
      "       Examples:\n",
      "       ... (18 more lines)\n",
      "\n",
      "\n",
      "=== Processing: __init__.py ===\n",
      "File size: 364 characters\n",
      "Found 4 semantic units\n",
      "  - import_statement: import_from_fusion._fusion (7 tokens)\n",
      "    Preview: from fusion._fusion import FusionCredentials...\n",
      "  - import_statement: import_from_fusion.fs_sync (7 tokens)\n",
      "    Preview: from fusion.fs_sync import fsync...\n",
      "  - import_statement: import_from_fusion.fusion (6 tokens)\n",
      "    Preview: from fusion.fusion import Fusion...\n",
      "  - import_statement: import_from_._fusion (5 tokens)\n",
      "    Preview: from ._fusion import *...\n",
      "Created 1 semantic chunks\n",
      "Final result: 1 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for __init__.py ---\n",
      "1. complete_module_4_parts\n",
      "   Type: complete_module\n",
      "   Size: 28 tokens, 7 lines\n",
      "   Content preview:\n",
      "     from fusion._fusion import FusionCredentials\n",
      "     \n",
      "     from fusion.fs_sync import fsync\n",
      "     ... (4 more lines)\n",
      "\n",
      "\n",
      "=== Processing: types.py ===\n",
      "File size: 163 characters\n",
      "Found 2 semantic units\n",
      "  - import_statement: import_from_queue (4 tokens)\n",
      "    Preview: from queue import Queue...\n",
      "  - import_statement: import_from_typing (6 tokens)\n",
      "    Preview: from typing import Any, Union...\n",
      "Created 1 semantic chunks\n",
      "Final result: 1 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for types.py ---\n",
      "1. complete_module_2_parts\n",
      "   Type: complete_module\n",
      "   Size: 11 tokens, 3 lines\n",
      "   Content preview:\n",
      "     from queue import Queue\n",
      "     \n",
      "     from typing import Any, Union\n",
      "\n",
      "\n",
      "=== Processing: fs_sync.py ===\n",
      "File size: 13303 characters\n",
      "Found 22 semantic units\n",
      "  - import_statement: import_import_base64 (3 tokens)\n",
      "    Preview: import base64...\n",
      "  - import_statement: import_import_hashlib (2 tokens)\n",
      "    Preview: import hashlib...\n",
      "  - import_statement: import_import_json (2 tokens)\n",
      "    Preview: import json...\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_import_sys (2 tokens)\n",
      "    Preview: import sys...\n",
      "  - import_statement: import_import_time (2 tokens)\n",
      "    Preview: import time...\n",
      "  - import_statement: import_import_warnings (2 tokens)\n",
      "    Preview: import warnings...\n",
      "  - import_statement: import_from_os.path (6 tokens)\n",
      "    Preview: from os.path import relpath...\n",
      "  - import_statement: import_from_pathlib (4 tokens)\n",
      "    Preview: from pathlib import Path...\n",
      "  - import_statement: import_from_typing (4 tokens)\n",
      "    Preview: from typing import Optional...\n",
      "  - import_statement: import_import_fsspec (4 tokens)\n",
      "    Preview: import fsspec...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_from_rich.progress (5 tokens)\n",
      "    Preview: from rich.progress import Progress...\n",
      "  - import_statement: import_from_.utils (34 tokens)\n",
      "    Preview: from .utils import (     cpu_count,     distribution_to_filename,     is_dataset_raw,     path_to_ur...\n",
      "  - function_definition: _url_to_path (69 tokens)\n",
      "    Preview: def _url_to_path(x: str) -> str:     file_name = distribution_to_filename(\"\", x.split(\"/\")[2], x.spl...\n",
      "  - function_definition: _download (217 tokens)\n",
      "    Preview: def _download(     fs_fusion: fsspec.filesystem,     fs_local: fsspec.filesystem,     df: pd.DataFra...\n",
      "  - function_definition: _upload (131 tokens)\n",
      "    Preview: def _upload(     fs_fusion: fsspec.filesystem,     fs_local: fsspec.filesystem,     df: pd.DataFrame...\n",
      "  - function_definition: _generate_sha256_token (163 tokens)\n",
      "    Preview: def _generate_sha256_token(path: str, fs: fsspec.filesystem, chunk_size: int = 5 * 2**20) -> str:...\n",
      "  - function_definition: _get_fusion_df (425 tokens)\n",
      "    Preview: def _get_fusion_df(     fs_fusion: fsspec.filesystem,     datasets_lst: list[str],     catalog: str,...\n",
      "  - function_definition: _get_local_state (555 tokens)\n",
      "    Preview: def _get_local_state(     fs_local: fsspec.filesystem,     fs_fusion: fsspec.filesystem,     dataset...\n",
      "  - function_definition: _synchronize (410 tokens)\n",
      "    Preview: def _synchronize(  # noqa: PLR0913     fs_fusion: fsspec.filesystem,     fs_local: fsspec.filesystem...\n",
      "  - function_definition: fsync (1050 tokens)\n",
      "    Preview: def fsync(  # noqa: PLR0912, PLR0913, PLR0915     fs_fusion: fsspec.filesystem,     fs_local: fsspec...\n",
      "Created 9 semantic chunks\n",
      "  Sub-chunking fsync (1050 tokens)\n",
      "    Breaking down fsync (1050 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 10 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for fs_sync.py ---\n",
      "1. imports_14_statements\n",
      "   Type: imports_group\n",
      "   Size: 89 tokens, 21 lines\n",
      "   Content preview:\n",
      "     import base64\n",
      "     import hashlib\n",
      "     import json\n",
      "     ... (18 more lines)\n",
      "\n",
      "2. _url_to_path\n",
      "   Type: function_definition\n",
      "   Size: 69 tokens, 3 lines\n",
      "   Content preview:\n",
      "     def _url_to_path(x: str) -> str:\n",
      "     file_name = distribution_to_filename(\"\", x.split(\"/\")[2], x.split(\"/\")[4], x.split(\"/\")[6], x.split(\"/\")[0])\n",
      "     return f\"{x.split('/')[0]}/{x.split('/')[2]}/{x.split('/')[4]}/{file_name}\"\n",
      "\n",
      "3. _download\n",
      "   Type: function_definition\n",
      "   Size: 217 tokens, 25 lines\n",
      "   Content preview:\n",
      "     def _download(\n",
      "     fs_fusion: fsspec.filesystem,\n",
      "     fs_local: fsspec.filesystem,\n",
      "     ... (22 more lines)\n",
      "\n",
      "4. _upload\n",
      "   Type: function_definition\n",
      "   Size: 131 tokens, 18 lines\n",
      "   Content preview:\n",
      "     def _upload(\n",
      "     fs_fusion: fsspec.filesystem,\n",
      "     fs_local: fsspec.filesystem,\n",
      "     ... (15 more lines)\n",
      "\n",
      "5. _generate_sha256_token\n",
      "   Type: function_definition\n",
      "   Size: 163 tokens, 16 lines\n",
      "   Content preview:\n",
      "     def _generate_sha256_token(path: str, fs: fsspec.filesystem, chunk_size: int = 5 * 2**20) -> str:\n",
      "     hash_sha256 = hashlib.sha256()\n",
      "     hash_sha256_chunk = hashlib.sha256()\n",
      "     ... (13 more lines)\n",
      "\n",
      "6. _get_fusion_df\n",
      "   Type: function_definition\n",
      "   Size: 425 tokens, 36 lines\n",
      "   Content preview:\n",
      "     def _get_fusion_df(\n",
      "     fs_fusion: fsspec.filesystem,\n",
      "     datasets_lst: list[str],\n",
      "     ... (33 more lines)\n",
      "\n",
      "7. _get_local_state\n",
      "   Type: function_definition\n",
      "   Size: 555 tokens, 46 lines\n",
      "   Content preview:\n",
      "     def _get_local_state(\n",
      "     fs_local: fsspec.filesystem,\n",
      "     fs_fusion: fsspec.filesystem,\n",
      "     ... (43 more lines)\n",
      "\n",
      "8. _synchronize\n",
      "   Type: function_definition\n",
      "   Size: 410 tokens, 48 lines\n",
      "   Content preview:\n",
      "     def _synchronize(  # noqa: PLR0913\n",
      "     fs_fusion: fsspec.filesystem,\n",
      "     fs_local: fsspec.filesystem,\n",
      "     ... (45 more lines)\n",
      "\n",
      "  9. fsync_part_1\n",
      "     Type: function_definition_part\n",
      "     Size: 1000 tokens, 119 lines\n",
      "     Content preview:\n",
      "       def fsync(  # noqa: PLR0912, PLR0913, PLR0915\n",
      "       fs_fusion: fsspec.filesystem,\n",
      "       fs_local: fsspec.filesystem,\n",
      "       ... (116 more lines)\n",
      "\n",
      "  10. fsync_part_2\n",
      "     Type: function_definition_part\n",
      "     Size: 50 tokens, 8 lines\n",
      "     Content preview:\n",
      "       except KeyboardInterrupt:  # noqa: PERF203\n",
      "       if input(\"Type exit to exit: \") != \"exit\":\n",
      "       continue\n",
      "       ... (5 more lines)\n",
      "\n",
      "\n",
      "=== Processing: dataset.py ===\n",
      "File size: 33669 characters\n",
      "Found 6 semantic units\n",
      "  - import_statement: import_import_json (4 tokens)\n",
      "    Preview: import json as js...\n",
      "  - import_statement: import_from_dataclasses (10 tokens)\n",
      "    Preview: from dataclasses import dataclass, field, fields...\n",
      "  - import_statement: import_from_typing (8 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_from_.utils (51 tokens)\n",
      "    Preview: from .utils import (     CamelCaseMeta,     _is_json,     camel_to_snake,     convert_date_format,...\n",
      "  - decorated_definition: decorated_Dataset (7058 tokens)\n",
      "    Preview: @dataclass class Dataset(metaclass=CamelCaseMeta):     \"\"\"Fusion Dataset class for managing dataset...\n",
      "Created 2 semantic chunks\n",
      "  Sub-chunking decorated_Dataset (7058 tokens)\n",
      "    Breaking down decorated_Dataset (7058 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 8 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for dataset.py ---\n",
      "1. imports_5_statements\n",
      "   Type: imports_group\n",
      "   Size: 81 tokens, 15 lines\n",
      "   Content preview:\n",
      "     import json as js\n",
      "     from dataclasses import dataclass, field, fields\n",
      "     from typing import TYPE_CHECKING, Any\n",
      "     ... (12 more lines)\n",
      "\n",
      "  2. decorated_Dataset_part_1\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1000 tokens, 66 lines\n",
      "     Content preview:\n",
      "       @dataclass\n",
      "       class Dataset(metaclass=CamelCaseMeta):\n",
      "       \"\"\"Fusion Dataset class for managing dataset metadata in a Fusion catalog.\n",
      "       ... (63 more lines)\n",
      "\n",
      "  3. decorated_Dataset_part_2\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1028 tokens, 105 lines\n",
      "     Content preview:\n",
      "       delivery_channel: str | list[str] = field(default_factory=lambda: [\"API\"])\n",
      "       language: str = \"English\"\n",
      "       status: str = \"Available\"\n",
      "       ... (102 more lines)\n",
      "\n",
      "  4. decorated_Dataset_part_3\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1004 tokens, 94 lines\n",
      "     Content preview:\n",
      "       \"\"\"Determine client.\"\"\"\n",
      "       \n",
      "       res = self._client if client is None else client\n",
      "       ... (91 more lines)\n",
      "\n",
      "  5. decorated_Dataset_part_4\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1000 tokens, 126 lines\n",
      "     Content preview:\n",
      "       def _from_dict(cls: type[Dataset], data: dict[str, Any]) -> Dataset:\n",
      "       \"\"\"Instantiate a Dataset object from a dictionary.\n",
      "       \n",
      "       ... (123 more lines)\n",
      "\n",
      "  6. decorated_Dataset_part_5\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1018 tokens, 134 lines\n",
      "     Content preview:\n",
      "       raise TypeError(f\"Could not resolve the object provided: {dataset_source}\")\n",
      "       \n",
      "       dataset.client = self._client\n",
      "       ... (131 more lines)\n",
      "\n",
      "  7. decorated_Dataset_part_6\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1005 tokens, 122 lines\n",
      "     Content preview:\n",
      "       >>> from fusion import Fusion\n",
      "       >>> fusion = Fusion()\n",
      "       >>> dataset_series = pd.Series({\n",
      "       ... (119 more lines)\n",
      "\n",
      "  8. decorated_Dataset_part_7\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1001 tokens, 119 lines\n",
      "     Content preview:\n",
      "       catalog_to (str): A catalog identifier to which to copy dataset.\n",
      "       catalog_from (str, optional): A catalog identifier from which to copy dataset. Defaults to \"common\".\n",
      "       client (Fusion, optional): A Fusion client object. Defaults to the instance's _client.\n",
      "       ... (116 more lines)\n",
      "\n",
      "\n",
      "=== Processing: embeddings.py ===\n",
      "File size: 32521 characters\n",
      "Found 26 semantic units\n",
      "  - import_statement: import_import_asyncio (2 tokens)\n",
      "    Preview: import asyncio...\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_import_ssl (2 tokens)\n",
      "    Preview: import ssl...\n",
      "  - import_statement: import_import_time (2 tokens)\n",
      "    Preview: import time...\n",
      "  - import_statement: import_import_warnings (2 tokens)\n",
      "    Preview: import warnings...\n",
      "  - import_statement: import_from_collections.abc (8 tokens)\n",
      "    Preview: from collections.abc import Collection, Mapping...\n",
      "  - import_statement: import_from_pathlib (4 tokens)\n",
      "    Preview: from pathlib import Path...\n",
      "  - import_statement: import_from_typing (8 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any...\n",
      "  - import_statement: import_import_requests (2 tokens)\n",
      "    Preview: import requests...\n",
      "  - import_statement: import_from_aiohttp (6 tokens)\n",
      "    Preview: from aiohttp import ClientTimeout...\n",
      "  - import_statement: import_from_opensearchpy._async._extra_imports (21 tokens)\n",
      "    Preview: from opensearchpy._async._extra_imports import aiohttp, aiohttp_exceptions, yarl...\n",
      "  - import_statement: import_from_opensearchpy._async.compat (12 tokens)\n",
      "    Preview: from opensearchpy._async.compat import get_running_loop...\n",
      "  - import_statement: import_from_opensearchpy._async.http_aiohttp (16 tokens)\n",
      "    Preview: from opensearchpy._async.http_aiohttp import AIOHttpConnection...\n",
      "  - import_statement: import_from_opensearchpy.compat (16 tokens)\n",
      "    Preview: from opensearchpy.compat import reraise_exceptions, string_types, urlencode...\n",
      "  - import_statement: import_from_opensearchpy.connection.base (9 tokens)\n",
      "    Preview: from opensearchpy.connection.base import Connection...\n",
      "  - import_statement: import_from_opensearchpy.exceptions (18 tokens)\n",
      "    Preview: from opensearchpy.exceptions import (     ConnectionError as OpenSearchConnectionError, )...\n",
      "  - import_statement: import_from_opensearchpy.exceptions (24 tokens)\n",
      "    Preview: from opensearchpy.exceptions import (     ConnectionTimeout,     ImproperlyConfigured,     SSLError,...\n",
      "  - import_statement: import_from_opensearchpy.metrics (11 tokens)\n",
      "    Preview: from opensearchpy.metrics import Metrics, MetricsNone...\n",
      "  - import_statement: import_from_fusion._fusion (7 tokens)\n",
      "    Preview: from fusion._fusion import FusionCredentials...\n",
      "  - import_statement: import_from_fusion.embeddings_utils (33 tokens)\n",
      "    Preview: from fusion.embeddings_utils import (     _modify_post_haystack,     _modify_post_response_langchain...\n",
      "  - import_statement: import_from_fusion.utils (9 tokens)\n",
      "    Preview: from fusion.utils import get_client, get_session...\n",
      "  - if_statement: if_block (25 tokens)\n",
      "    Preview: if TYPE_CHECKING:     from collections.abc import Collection, Mapping      from fusion.authenticatio...\n",
      "  - class_definition: FusionEmbeddingsConnection (2534 tokens)\n",
      "    Preview: class FusionEmbeddingsConnection(Connection):  # type: ignore     \"\"\"     Class responsible for main...\n",
      "  - class_definition: FusionAsyncHttpConnection (3228 tokens)\n",
      "    Preview: class FusionAsyncHttpConnection(AIOHttpConnection):  # type: ignore     session: FusionAiohttpSessio...\n",
      "  - function_definition: format_index_body (278 tokens)\n",
      "    Preview: def format_index_body(number_of_shards: int = 1, number_of_replicas: int = 1, dimension: int = 1536)...\n",
      "  - class_definition: PromptTemplateManager (483 tokens)\n",
      "    Preview: class PromptTemplateManager:     \"\"\"Class to manage prompt templates for different packages and task...\n",
      "Created 6 semantic chunks\n",
      "  Sub-chunking FusionEmbeddingsConnection (2534 tokens)\n",
      "    Breaking down FusionEmbeddingsConnection (2534 tokens) into smaller pieces...\n",
      "  Sub-chunking FusionAsyncHttpConnection (3228 tokens)\n",
      "    Breaking down FusionAsyncHttpConnection (3228 tokens) into smaller pieces...\n",
      "Sub-chunked 2 oversized chunks\n",
      "Final result: 11 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for embeddings.py ---\n",
      "1. imports_21_statements\n",
      "   Type: imports_group\n",
      "   Size: 231 tokens, 31 lines\n",
      "   Content preview:\n",
      "     import asyncio\n",
      "     import logging\n",
      "     import ssl\n",
      "     ... (28 more lines)\n",
      "\n",
      "2. if_block\n",
      "   Type: if_statement\n",
      "   Size: 25 tokens, 4 lines\n",
      "   Content preview:\n",
      "     if TYPE_CHECKING:\n",
      "     from collections.abc import Collection, Mapping\n",
      "     \n",
      "     ... (1 more lines)\n",
      "\n",
      "  3. FusionEmbeddingsConnection_part_1\n",
      "     Type: class_definition_part\n",
      "     Size: 1011 tokens, 82 lines\n",
      "     Content preview:\n",
      "       class FusionEmbeddingsConnection(Connection):  # type: ignore\n",
      "       \"\"\"\n",
      "       Class responsible for maintaining HTTP connection to the Fusion Embedding API using OpenSearch. This class is a\n",
      "       ... (79 more lines)\n",
      "\n",
      "  4. FusionEmbeddingsConnection_part_2\n",
      "     Type: class_definition_part\n",
      "     Size: 1014 tokens, 119 lines\n",
      "     Content preview:\n",
      "       for key in list(self.session.headers):\n",
      "       self.session.headers.pop(key)\n",
      "       \n",
      "       ... (116 more lines)\n",
      "\n",
      "  5. FusionEmbeddingsConnection_part_3\n",
      "     Type: class_definition_part\n",
      "     Size: 508 tokens, 78 lines\n",
      "     Content preview:\n",
      "       settings = self.session.merge_environment_settings(prepared_request.url, {}, None, None, None)\n",
      "       send_kwargs: Any = {\n",
      "       \"timeout\": timeout or self.timeout,\n",
      "       ... (75 more lines)\n",
      "\n",
      "  6. FusionAsyncHttpConnection_part_1\n",
      "     Type: class_definition_part\n",
      "     Size: 1008 tokens, 75 lines\n",
      "     Content preview:\n",
      "       class FusionAsyncHttpConnection(AIOHttpConnection):  # type: ignore\n",
      "       session: FusionAiohttpSession | None\n",
      "       \n",
      "       ... (72 more lines)\n",
      "\n",
      "  7. FusionAsyncHttpConnection_part_2\n",
      "     Type: class_definition_part\n",
      "     Size: 1023 tokens, 117 lines\n",
      "     Content preview:\n",
      "       credentials: FusionCredentials | str | None = kwargs.get(\"credentials\", \"config/client_credentials.json\")\n",
      "       if isinstance(credentials, FusionCredentials):\n",
      "       self.credentials = credentials\n",
      "       ... (114 more lines)\n",
      "\n",
      "  8. FusionAsyncHttpConnection_part_3\n",
      "     Type: class_definition_part\n",
      "     Size: 1024 tokens, 127 lines\n",
      "     Content preview:\n",
      "       def _remap_endpoints(url: str) -> str:\n",
      "       return url.replace(\"_bulk\", \"embeddings\").replace(\"_search\", \"search\")\n",
      "       \n",
      "       ... (124 more lines)\n",
      "\n",
      "  9. FusionAsyncHttpConnection_part_4\n",
      "     Type: class_definition_part\n",
      "     Size: 172 tokens, 24 lines\n",
      "     Content preview:\n",
      "       response=raw_data_modified,\n",
      "       )\n",
      "       self._raise_error(response.status, raw_data_modified)\n",
      "       ... (21 more lines)\n",
      "\n",
      "10. format_index_body\n",
      "   Type: function_definition\n",
      "   Size: 278 tokens, 30 lines\n",
      "   Content preview:\n",
      "     def format_index_body(number_of_shards: int = 1, number_of_replicas: int = 1, dimension: int = 1536) -> dict[str, Any]:\n",
      "     \"\"\"Format index body for index creation in Embeddings API.\n",
      "     \n",
      "     ... (27 more lines)\n",
      "\n",
      "11. PromptTemplateManager\n",
      "   Type: class_definition\n",
      "   Size: 483 tokens, 85 lines\n",
      "   Content preview:\n",
      "     class PromptTemplateManager:\n",
      "     \"\"\"Class to manage prompt templates for different packages and tasks.\"\"\"\n",
      "     \n",
      "     ... (82 more lines)\n",
      "\n",
      "\n",
      "=== Processing: utils.py ===\n",
      "File size: 30447 characters\n",
      "Found 61 semantic units\n",
      "  - import_statement: import_import_contextlib (3 tokens)\n",
      "    Preview: import contextlib...\n",
      "  - import_statement: import_import_json (4 tokens)\n",
      "    Preview: import json as js...\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_import_multiprocessing (4 tokens)\n",
      "    Preview: import multiprocessing as mp...\n",
      "  - import_statement: import_import_os (2 tokens)\n",
      "    Preview: import os...\n",
      "  - import_statement: import_import_re (2 tokens)\n",
      "    Preview: import re...\n",
      "  - import_statement: import_import_ssl (2 tokens)\n",
      "    Preview: import ssl...\n",
      "  - import_statement: import_import_zipfile (2 tokens)\n",
      "    Preview: import zipfile...\n",
      "  - import_statement: import_from_contextlib (6 tokens)\n",
      "    Preview: from contextlib import nullcontext...\n",
      "  - import_statement: import_from_datetime (6 tokens)\n",
      "    Preview: from datetime import date, datetime...\n",
      "  - import_statement: import_from_io (5 tokens)\n",
      "    Preview: from io import BytesIO...\n",
      "  - import_statement: import_from_pathlib (4 tokens)\n",
      "    Preview: from pathlib import Path...\n",
      "  - import_statement: import_from_typing (12 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any, Union, cast...\n",
      "  - import_statement: import_from_urllib.parse (9 tokens)\n",
      "    Preview: from urllib.parse import urlparse, urlunparse...\n",
      "  - import_statement: import_import_aiohttp (3 tokens)\n",
      "    Preview: import aiohttp...\n",
      "  - import_statement: import_import_certifi (3 tokens)\n",
      "    Preview: import certifi...\n",
      "  - import_statement: import_import_fsspec (4 tokens)\n",
      "    Preview: import fsspec...\n",
      "  - import_statement: import_import_joblib (3 tokens)\n",
      "    Preview: import joblib...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_import_pyarrow (5 tokens)\n",
      "    Preview: import pyarrow as pa...\n",
      "  - import_statement: import_import_pyarrow.parquet (7 tokens)\n",
      "    Preview: import pyarrow.parquet as pq...\n",
      "  - import_statement: import_import_requests (2 tokens)\n",
      "    Preview: import requests...\n",
      "  - import_statement: import_from_dateutil (5 tokens)\n",
      "    Preview: from dateutil import parser...\n",
      "  - import_statement: import_from_pyarrow (11 tokens)\n",
      "    Preview: from pyarrow import csv, json, unify_schemas...\n",
      "  - import_statement: import_from_pyarrow.parquet (9 tokens)\n",
      "    Preview: from pyarrow.parquet import filters_to_expression...\n",
      "  - import_statement: import_from_rich.progress (39 tokens)\n",
      "    Preview: from rich.progress import (     BarColumn,     MofNCompleteColumn,     Progress,     TaskProgressCol...\n",
      "  - import_statement: import_from_urllib3.util.retry (7 tokens)\n",
      "    Preview: from urllib3.util.retry import Retry...\n",
      "  - import_statement: import_from_.authentication (13 tokens)\n",
      "    Preview: from .authentication import FusionAiohttpSession, FusionOAuthAdapter...\n",
      "  - if_statement: if_block (31 tokens)\n",
      "    Preview: if TYPE_CHECKING:     from collections.abc import Generator      from fusion._fusion import FusionCr...\n",
      "  - function_definition: get_default_fs (144 tokens)\n",
      "    Preview: def get_default_fs() -> fsspec.filesystem:     \"\"\"Retrieve default filesystem.      Returns: filesys...\n",
      "  - decorated_definition: decorated_joblib_progress (302 tokens)\n",
      "    Preview: @contextlib.contextmanager def joblib_progress(description: str, total: int | None) -> Generator[Pro...\n",
      "  - function_definition: cpu_count (154 tokens)\n",
      "    Preview: def cpu_count(thread_pool_size: int | None = None, is_threading: bool = False) -> int:     \"\"\"Determ...\n",
      "  - function_definition: csv_to_table (181 tokens)\n",
      "    Preview: def csv_to_table(     path: str,     fs: fsspec.filesystem | None = None,     columns: list[str] | N...\n",
      "  - function_definition: json_to_table (178 tokens)\n",
      "    Preview: def json_to_table(     path: str,     fs: fsspec.filesystem | None = None,     columns: list[str] |...\n",
      "  - function_definition: parquet_to_table (251 tokens)\n",
      "    Preview: def parquet_to_table(     path: PathLikeT | list[PathLikeT],     fs: fsspec.filesystem | None = None...\n",
      "  - function_definition: read_csv (601 tokens)\n",
      "    Preview: def read_csv(  # noqa: PLR0912     path: str | zipfile.ZipFile,     columns: list[str] | None = None...\n",
      "  - function_definition: read_json (443 tokens)\n",
      "    Preview: def read_json(     path: str,     columns: list[str] | None = None,     filters: PyArrowFilterT | No...\n",
      "  - function_definition: read_parquet (235 tokens)\n",
      "    Preview: def read_parquet(     path: PathLikeT,     columns: list[str] | None = None,     filters: PyArrowFil...\n",
      "  - function_definition: _normalise_dt_param (250 tokens)\n",
      "    Preview: def _normalise_dt_param(dt: str | int | datetime | date) -> str:     \"\"\"Convert dates into a normali...\n",
      "  - function_definition: normalise_dt_param_str (146 tokens)\n",
      "    Preview: def normalise_dt_param_str(dt: str) -> tuple[str, ...]:     \"\"\"Convert a date parameter which may be...\n",
      "  - function_definition: distribution_to_filename (258 tokens)\n",
      "    Preview: def distribution_to_filename(     root_folder: str,     dataset: str,     datasetseries: str,     fi...\n",
      "  - function_definition: _filename_to_distribution (126 tokens)\n",
      "    Preview: def _filename_to_distribution(file_name: str) -> tuple[str, str, str, str]:     \"\"\"Breaks a filename...\n",
      "  - function_definition: distribution_to_url (295 tokens)\n",
      "    Preview: def distribution_to_url(     root_url: str,     dataset: str,     datasetseries: str,     file_forma...\n",
      "  - function_definition: _get_canonical_root_url (89 tokens)\n",
      "    Preview: def _get_canonical_root_url(any_url: str) -> str:     \"\"\"Get the full URL for the API endpoint....\n",
      "  - function_definition: get_client (234 tokens)\n",
      "    Preview: async def get_client(credentials: FusionCredentials, **kwargs: Any) -> FusionAiohttpSession:  # noqa...\n",
      "  - function_definition: get_session (237 tokens)\n",
      "    Preview: def get_session(     credentials: FusionCredentials, root_url: str, get_retries: int | Retry | None...\n",
      "  - function_definition: validate_file_names (196 tokens)\n",
      "    Preview: def validate_file_names(paths: list[str]) -> list[bool]:     \"\"\"Validate if the file name format adh...\n",
      "  - function_definition: is_dataset_raw (175 tokens)\n",
      "    Preview: def is_dataset_raw(paths: list[str], fs_fusion: fsspec.AbstractFileSystem) -> list[bool]:     \"\"\"Che...\n",
      "  - function_definition: path_to_url (148 tokens)\n",
      "    Preview: def path_to_url(x: str, is_raw: bool = False, is_download: bool = False) -> str:     \"\"\"Convert file...\n",
      "  - function_definition: upload_files (684 tokens)\n",
      "    Preview: def upload_files(  # noqa: PLR0913     fs_fusion: fsspec.AbstractFileSystem,     fs_local: fsspec.Ab...\n",
      "  - function_definition: camel_to_snake (60 tokens)\n",
      "    Preview: def camel_to_snake(name: str) -> str:     \"\"\"Convert camelCase to snake_case.\"\"\"     s1 = re.sub(re_...\n",
      "  - class_definition: CamelCaseMeta (253 tokens)\n",
      "    Preview: class CamelCaseMeta(type):     \"\"\"Metaclass to support both snake and camel case typing.\"\"\"      def...\n",
      "  - function_definition: snake_to_camel (49 tokens)\n",
      "    Preview: def snake_to_camel(name: str) -> str:     \"\"\"Convert snake_case to camelCase.\"\"\"     components = na...\n",
      "  - function_definition: tidy_string (71 tokens)\n",
      "    Preview: def tidy_string(x: str) -> str:     \"\"\"Tidy string.      Args:         x (str): String to tidy....\n",
      "  - function_definition: make_list (75 tokens)\n",
      "    Preview: def make_list(obj: Any) -> list[str]:     \"\"\"Make list.\"\"\"     if isinstance(obj, list):         lst...\n",
      "  - function_definition: make_bool (66 tokens)\n",
      "    Preview: def make_bool(obj: Any) -> bool:     \"\"\"Make boolean.\"\"\"     if isinstance(obj, str):         false_...\n",
      "  - function_definition: convert_date_format (57 tokens)\n",
      "    Preview: def convert_date_format(date_str: str) -> Any:     \"\"\"Convert date to YYYY-MM-DD format.\"\"\"     desi...\n",
      "  - function_definition: _is_json (48 tokens)\n",
      "    Preview: def _is_json(data: str) -> bool:     \"\"\"Check if the data is in JSON format.\"\"\"     try:         js....\n",
      "  - function_definition: requests_raise_for_status (54 tokens)\n",
      "    Preview: def requests_raise_for_status(response: requests.Response) -> None:     \"\"\"Send response text into r...\n",
      "  - function_definition: validate_file_formats (263 tokens)\n",
      "    Preview: def validate_file_formats(fs_fusion: fsspec.AbstractFileSystem, path: str) -> None:     \"\"\"     Vali...\n",
      "  - function_definition: file_name_to_url (216 tokens)\n",
      "    Preview: def file_name_to_url(     file_name: str, dataset: str, catalog: str, is_download: bool = False ) ->...\n",
      "Created 34 semantic chunks\n",
      "Final result: 34 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for utils.py ---\n",
      "1. imports_28_statements\n",
      "   Type: imports_group\n",
      "   Size: 204 tokens, 36 lines\n",
      "   Content preview:\n",
      "     import contextlib\n",
      "     import json as js\n",
      "     import logging\n",
      "     ... (33 more lines)\n",
      "\n",
      "2. if_block\n",
      "   Type: if_statement\n",
      "   Size: 31 tokens, 6 lines\n",
      "   Content preview:\n",
      "     if TYPE_CHECKING:\n",
      "     from collections.abc import Generator\n",
      "     \n",
      "     ... (3 more lines)\n",
      "\n",
      "3. get_default_fs\n",
      "   Type: function_definition\n",
      "   Size: 144 tokens, 18 lines\n",
      "   Content preview:\n",
      "     def get_default_fs() -> fsspec.filesystem:\n",
      "     \"\"\"Retrieve default filesystem.\n",
      "     \n",
      "     ... (15 more lines)\n",
      "\n",
      "4. decorated_joblib_progress\n",
      "   Type: decorated_definition\n",
      "   Size: 302 tokens, 37 lines\n",
      "   Content preview:\n",
      "     @contextlib.contextmanager\n",
      "     def joblib_progress(description: str, total: int | None) -> Generator[Progress, None, None]:\n",
      "     show_speed = not total\n",
      "     ... (34 more lines)\n",
      "\n",
      "5. cpu_count\n",
      "   Type: function_definition\n",
      "   Size: 154 tokens, 19 lines\n",
      "   Content preview:\n",
      "     def cpu_count(thread_pool_size: int | None = None, is_threading: bool = False) -> int:\n",
      "     \"\"\"Determine the number of cpus/threads for parallelization.\n",
      "     \n",
      "     ... (16 more lines)\n",
      "\n",
      "6. csv_to_table\n",
      "   Type: function_definition\n",
      "   Size: 181 tokens, 25 lines\n",
      "   Content preview:\n",
      "     def csv_to_table(\n",
      "     path: str,\n",
      "     fs: fsspec.filesystem | None = None,\n",
      "     ... (22 more lines)\n",
      "\n",
      "7. json_to_table\n",
      "   Type: function_definition\n",
      "   Size: 178 tokens, 25 lines\n",
      "   Content preview:\n",
      "     def json_to_table(\n",
      "     path: str,\n",
      "     fs: fsspec.filesystem | None = None,\n",
      "     ... (22 more lines)\n",
      "\n",
      "8. parquet_to_table\n",
      "   Type: function_definition\n",
      "   Size: 251 tokens, 46 lines\n",
      "   Content preview:\n",
      "     def parquet_to_table(\n",
      "     path: PathLikeT | list[PathLikeT],\n",
      "     fs: fsspec.filesystem | None = None,\n",
      "     ... (43 more lines)\n",
      "\n",
      "9. read_csv\n",
      "   Type: function_definition\n",
      "   Size: 601 tokens, 81 lines\n",
      "   Content preview:\n",
      "     def read_csv(  # noqa: PLR0912\n",
      "     path: str | zipfile.ZipFile,\n",
      "     columns: list[str] | None = None,\n",
      "     ... (78 more lines)\n",
      "\n",
      "10. read_json\n",
      "   Type: function_definition\n",
      "   Size: 443 tokens, 62 lines\n",
      "   Content preview:\n",
      "     def read_json(\n",
      "     path: str,\n",
      "     columns: list[str] | None = None,\n",
      "     ... (59 more lines)\n",
      "\n",
      "11. read_parquet\n",
      "   Type: function_definition\n",
      "   Size: 235 tokens, 30 lines\n",
      "   Content preview:\n",
      "     def read_parquet(\n",
      "     path: PathLikeT,\n",
      "     columns: list[str] | None = None,\n",
      "     ... (27 more lines)\n",
      "\n",
      "12. _normalise_dt_param\n",
      "   Type: function_definition\n",
      "   Size: 250 tokens, 36 lines\n",
      "   Content preview:\n",
      "     def _normalise_dt_param(dt: str | int | datetime | date) -> str:\n",
      "     \"\"\"Convert dates into a normalised string representation.\n",
      "     \n",
      "     ... (33 more lines)\n",
      "\n",
      "13. normalise_dt_param_str\n",
      "   Type: function_definition\n",
      "   Size: 146 tokens, 15 lines\n",
      "   Content preview:\n",
      "     def normalise_dt_param_str(dt: str) -> tuple[str, ...]:\n",
      "     \"\"\"Convert a date parameter which may be a single date or a date range into a tuple.\n",
      "     \n",
      "     ... (12 more lines)\n",
      "\n",
      "14. distribution_to_filename\n",
      "   Type: function_definition\n",
      "   Size: 258 tokens, 33 lines\n",
      "   Content preview:\n",
      "     def distribution_to_filename(\n",
      "     root_folder: str,\n",
      "     dataset: str,\n",
      "     ... (30 more lines)\n",
      "\n",
      "15. _filename_to_distribution\n",
      "   Type: function_definition\n",
      "   Size: 126 tokens, 12 lines\n",
      "   Content preview:\n",
      "     def _filename_to_distribution(file_name: str) -> tuple[str, str, str, str]:\n",
      "     \"\"\"Breaks a filename down into the components that represent a dataset distribution in the catalog.\n",
      "     \n",
      "     ... (9 more lines)\n",
      "\n",
      "16. distribution_to_url\n",
      "   Type: function_definition\n",
      "   Size: 295 tokens, 32 lines\n",
      "   Content preview:\n",
      "     def distribution_to_url(\n",
      "     root_url: str,\n",
      "     dataset: str,\n",
      "     ... (29 more lines)\n",
      "\n",
      "17. _get_canonical_root_url\n",
      "   Type: function_definition\n",
      "   Size: 89 tokens, 13 lines\n",
      "   Content preview:\n",
      "     def _get_canonical_root_url(any_url: str) -> str:\n",
      "     \"\"\"Get the full URL for the API endpoint.\n",
      "     \n",
      "     ... (10 more lines)\n",
      "\n",
      "18. get_client\n",
      "   Type: function_definition\n",
      "   Size: 234 tokens, 29 lines\n",
      "   Content preview:\n",
      "     async def get_client(credentials: FusionCredentials, **kwargs: Any) -> FusionAiohttpSession:  # noqa: PLR0915\n",
      "     \"\"\"Gets session for async.\n",
      "     \n",
      "     ... (26 more lines)\n",
      "\n",
      "19. get_session\n",
      "   Type: function_definition\n",
      "   Size: 237 tokens, 27 lines\n",
      "   Content preview:\n",
      "     def get_session(\n",
      "     credentials: FusionCredentials, root_url: str, get_retries: int | Retry | None = None\n",
      "     ) -> requests.Session:\n",
      "     ... (24 more lines)\n",
      "\n",
      "20. validate_file_names\n",
      "   Type: function_definition\n",
      "   Size: 196 tokens, 25 lines\n",
      "   Content preview:\n",
      "     def validate_file_names(paths: list[str]) -> list[bool]:\n",
      "     \"\"\"Validate if the file name format adheres to the standard.\n",
      "     \n",
      "     ... (22 more lines)\n",
      "\n",
      "21. is_dataset_raw\n",
      "   Type: function_definition\n",
      "   Size: 175 tokens, 20 lines\n",
      "   Content preview:\n",
      "     def is_dataset_raw(paths: list[str], fs_fusion: fsspec.AbstractFileSystem) -> list[bool]:\n",
      "     \"\"\"Check if the files correspond to a raw dataset.\n",
      "     \n",
      "     ... (17 more lines)\n",
      "\n",
      "22. path_to_url\n",
      "   Type: function_definition\n",
      "   Size: 148 tokens, 14 lines\n",
      "   Content preview:\n",
      "     def path_to_url(x: str, is_raw: bool = False, is_download: bool = False) -> str:\n",
      "     \"\"\"Convert file name to fusion url.\n",
      "     \n",
      "     ... (11 more lines)\n",
      "\n",
      "23. upload_files\n",
      "   Type: function_definition\n",
      "   Size: 684 tokens, 82 lines\n",
      "   Content preview:\n",
      "     def upload_files(  # noqa: PLR0913\n",
      "     fs_fusion: fsspec.AbstractFileSystem,\n",
      "     fs_local: fsspec.AbstractFileSystem,\n",
      "     ... (79 more lines)\n",
      "\n",
      "24. camel_to_snake\n",
      "   Type: function_definition\n",
      "   Size: 60 tokens, 4 lines\n",
      "   Content preview:\n",
      "     def camel_to_snake(name: str) -> str:\n",
      "     \"\"\"Convert camelCase to snake_case.\"\"\"\n",
      "     s1 = re.sub(re_str_1, r\"\\1_\\2\", name)\n",
      "     ... (1 more lines)\n",
      "\n",
      "25. CamelCaseMeta\n",
      "   Type: class_definition\n",
      "   Size: 253 tokens, 24 lines\n",
      "   Content preview:\n",
      "     class CamelCaseMeta(type):\n",
      "     \"\"\"Metaclass to support both snake and camel case typing.\"\"\"\n",
      "     \n",
      "     ... (21 more lines)\n",
      "\n",
      "26. snake_to_camel\n",
      "   Type: function_definition\n",
      "   Size: 49 tokens, 4 lines\n",
      "   Content preview:\n",
      "     def snake_to_camel(name: str) -> str:\n",
      "     \"\"\"Convert snake_case to camelCase.\"\"\"\n",
      "     components = name.lower().split(\"_\")\n",
      "     ... (1 more lines)\n",
      "\n",
      "27. tidy_string\n",
      "   Type: function_definition\n",
      "   Size: 71 tokens, 14 lines\n",
      "   Content preview:\n",
      "     def tidy_string(x: str) -> str:\n",
      "     \"\"\"Tidy string.\n",
      "     \n",
      "     ... (11 more lines)\n",
      "\n",
      "28. make_list\n",
      "   Type: function_definition\n",
      "   Size: 75 tokens, 12 lines\n",
      "   Content preview:\n",
      "     def make_list(obj: Any) -> list[str]:\n",
      "     \"\"\"Make list.\"\"\"\n",
      "     if isinstance(obj, list):\n",
      "     ... (9 more lines)\n",
      "\n",
      "29. make_bool\n",
      "   Type: function_definition\n",
      "   Size: 66 tokens, 10 lines\n",
      "   Content preview:\n",
      "     def make_bool(obj: Any) -> bool:\n",
      "     \"\"\"Make boolean.\"\"\"\n",
      "     if isinstance(obj, str):\n",
      "     ... (7 more lines)\n",
      "\n",
      "30. convert_date_format\n",
      "   Type: function_definition\n",
      "   Size: 57 tokens, 6 lines\n",
      "   Content preview:\n",
      "     def convert_date_format(date_str: str) -> Any:\n",
      "     \"\"\"Convert date to YYYY-MM-DD format.\"\"\"\n",
      "     desired_format = \"%Y-%m-%d\"\n",
      "     ... (3 more lines)\n",
      "\n",
      "31. _is_json\n",
      "   Type: function_definition\n",
      "   Size: 48 tokens, 7 lines\n",
      "   Content preview:\n",
      "     def _is_json(data: str) -> bool:\n",
      "     \"\"\"Check if the data is in JSON format.\"\"\"\n",
      "     try:\n",
      "     ... (4 more lines)\n",
      "\n",
      "32. requests_raise_for_status\n",
      "   Type: function_definition\n",
      "   Size: 54 tokens, 8 lines\n",
      "   Content preview:\n",
      "     def requests_raise_for_status(response: requests.Response) -> None:\n",
      "     \"\"\"Send response text into raise for status.\"\"\"\n",
      "     real_reason = \"\"\n",
      "     ... (5 more lines)\n",
      "\n",
      "33. validate_file_formats\n",
      "   Type: function_definition\n",
      "   Size: 263 tokens, 28 lines\n",
      "   Content preview:\n",
      "     def validate_file_formats(fs_fusion: fsspec.AbstractFileSystem, path: str) -> None:\n",
      "     \"\"\"\n",
      "     Validate file formats in the given folder and subfolders.\n",
      "     ... (25 more lines)\n",
      "\n",
      "34. file_name_to_url\n",
      "   Type: function_definition\n",
      "   Size: 216 tokens, 26 lines\n",
      "   Content preview:\n",
      "     def file_name_to_url(\n",
      "     file_name: str, dataset: str, catalog: str, is_download: bool = False\n",
      "     ) -> str:\n",
      "     ... (23 more lines)\n",
      "\n",
      "\n",
      "=== Processing: fusion.py ===\n",
      "File size: 121500 characters\n",
      "Found 29 semantic units\n",
      "  - import_statement: import_import_copy (2 tokens)\n",
      "    Preview: import copy...\n",
      "  - import_statement: import_import_json (4 tokens)\n",
      "    Preview: import json as js...\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_import_re (2 tokens)\n",
      "    Preview: import re...\n",
      "  - import_statement: import_import_sys (2 tokens)\n",
      "    Preview: import sys...\n",
      "  - import_statement: import_import_warnings (2 tokens)\n",
      "    Preview: import warnings...\n",
      "  - import_statement: import_from_http (5 tokens)\n",
      "    Preview: from http import HTTPStatus...\n",
      "  - import_statement: import_from_io (5 tokens)\n",
      "    Preview: from io import BytesIO...\n",
      "  - import_statement: import_from_pathlib (4 tokens)\n",
      "    Preview: from pathlib import Path...\n",
      "  - import_statement: import_from_typing (8 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any...\n",
      "  - import_statement: import_from_zipfile (5 tokens)\n",
      "    Preview: from zipfile import ZipFile...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_import_pyarrow (5 tokens)\n",
      "    Preview: import pyarrow as pa...\n",
      "  - import_statement: import_from_rich.progress (5 tokens)\n",
      "    Preview: from rich.progress import Progress...\n",
      "  - import_statement: import_from_tabulate (6 tokens)\n",
      "    Preview: from tabulate import tabulate...\n",
      "  - import_statement: import_from_fusion._fusion (7 tokens)\n",
      "    Preview: from fusion._fusion import FusionCredentials...\n",
      "  - import_statement: import_from_fusion.attributes (7 tokens)\n",
      "    Preview: from fusion.attributes import Attribute, Attributes...\n",
      "  - import_statement: import_from_fusion.dataflow (12 tokens)\n",
      "    Preview: from fusion.dataflow import InputDataFlow, OutputDataFlow...\n",
      "  - import_statement: import_from_fusion.dataset (5 tokens)\n",
      "    Preview: from fusion.dataset import Dataset...\n",
      "  - import_statement: import_from_fusion.fusion_types (7 tokens)\n",
      "    Preview: from fusion.fusion_types import Types...\n",
      "  - import_statement: import_from_fusion.product (5 tokens)\n",
      "    Preview: from fusion.product import Product...\n",
      "  - import_statement: import_from_fusion.report (8 tokens)\n",
      "    Preview: from fusion.report import Report, ReportsWrapper...\n",
      "  - import_statement: import_from_fusion.report_attributes (10 tokens)\n",
      "    Preview: from fusion.report_attributes import ReportAttribute, ReportAttributes...\n",
      "  - import_statement: import_from_.embeddings_utils (17 tokens)\n",
      "    Preview: from .embeddings_utils import _format_full_index_response, _format_summary_index_response...\n",
      "  - import_statement: import_from_.exceptions (14 tokens)\n",
      "    Preview: from .exceptions import APIResponseError, CredentialError, FileFormatError...\n",
      "  - import_statement: import_from_.fusion_filesystem (9 tokens)\n",
      "    Preview: from .fusion_filesystem import FusionHTTPFileSystem...\n",
      "  - import_statement: import_from_.utils (109 tokens)\n",
      "    Preview: from .utils import (     RECOGNIZED_FORMATS,     cpu_count,     csv_to_table,     distribution_to_fi...\n",
      "  - if_statement: if_block (47 tokens)\n",
      "    Preview: if TYPE_CHECKING:     from collections.abc import AsyncGenerator      import fsspec     import reque...\n",
      "  - class_definition: Fusion (24336 tokens)\n",
      "    Preview: class Fusion:     \"\"\"Core Fusion class for API access.\"\"\"      @staticmethod     def _call_for_dataf...\n",
      "Created 3 semantic chunks\n",
      "  Sub-chunking Fusion (24336 tokens)\n",
      "    Breaking down Fusion (24336 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 27 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for fusion.py ---\n",
      "1. imports_27_statements\n",
      "   Type: imports_group\n",
      "   Size: 297 tokens, 48 lines\n",
      "   Content preview:\n",
      "     import copy\n",
      "     import json as js\n",
      "     import logging\n",
      "     ... (45 more lines)\n",
      "\n",
      "2. if_block\n",
      "   Type: if_statement\n",
      "   Size: 47 tokens, 8 lines\n",
      "   Content preview:\n",
      "     if TYPE_CHECKING:\n",
      "     from collections.abc import AsyncGenerator\n",
      "     \n",
      "     ... (5 more lines)\n",
      "\n",
      "  3. Fusion_part_1\n",
      "     Type: class_definition_part\n",
      "     Size: 1015 tokens, 118 lines\n",
      "     Content preview:\n",
      "       class Fusion:\n",
      "       \"\"\"Core Fusion class for API access.\"\"\"\n",
      "       \n",
      "       ... (115 more lines)\n",
      "\n",
      "  4. Fusion_part_2\n",
      "     Type: class_definition_part\n",
      "     Size: 1011 tokens, 142 lines\n",
      "     Content preview:\n",
      "       ]\n",
      "       + [p for p in dir(Fusion) if isinstance(getattr(Fusion, p), property)],\n",
      "       [\n",
      "       ... (139 more lines)\n",
      "\n",
      "  5. Fusion_part_3\n",
      "     Type: class_definition_part\n",
      "     Size: 1011 tokens, 118 lines\n",
      "     Content preview:\n",
      "       products whose identifier matches any of the strings. Defaults to None.\n",
      "       id_contains (bool): Filter datasets only where the string(s) are contained in the identifier,\n",
      "       ignoring description.\n",
      "       ... (115 more lines)\n",
      "\n",
      "  6. Fusion_part_4\n",
      "     Type: class_definition_part\n",
      "     Size: 1006 tokens, 127 lines\n",
      "     Content preview:\n",
      "       ds_df = Fusion._call_for_dataframe(url, self.session)\n",
      "       \n",
      "       if contains:\n",
      "       ... (124 more lines)\n",
      "\n",
      "  7. Fusion_part_5\n",
      "     Type: class_definition_part\n",
      "     Size: 1001 tokens, 144 lines\n",
      "     Content preview:\n",
      "       dataset (str): A dataset identifier\n",
      "       catalog (str, optional): A catalog identifier. Defaults to 'common'.\n",
      "       output (bool, optional): If True then print the dataframe. Defaults to False.\n",
      "       ... (141 more lines)\n",
      "\n",
      "  8. Fusion_part_6\n",
      "     Type: class_definition_part\n",
      "     Size: 997 tokens, 110 lines\n",
      "     Content preview:\n",
      "       url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/datasetseries/{series}/distributions\"\n",
      "       distros_df = Fusion._call_for_dataframe(url, self.session)\n",
      "       \n",
      "       ... (107 more lines)\n",
      "\n",
      "  9. Fusion_part_7\n",
      "     Type: class_definition_part\n",
      "     Size: 1004 tokens, 112 lines\n",
      "     Content preview:\n",
      "       If set to None, the function will download if only one format is available, else it will raise an error.\n",
      "       catalog (str, optional): A catalog identifier. Defaults to 'common'.\n",
      "       n_par (int, optional): Specify how many distributions to download in parallel.\n",
      "       ... (109 more lines)\n",
      "\n",
      "  10. Fusion_part_8\n",
      "     Type: class_definition_part\n",
      "     Size: 1002 tokens, 91 lines\n",
      "     Content preview:\n",
      "       else:\n",
      "       res = [self.get_fusion_filesystem().download(**spec) for spec in download_spec]\n",
      "       \n",
      "       ... (88 more lines)\n",
      "\n",
      "  11. Fusion_part_9\n",
      "     Type: class_definition_part\n",
      "     Size: 1012 tokens, 145 lines\n",
      "     Content preview:\n",
      "       class:`pandas.DataFrame`: a dataframe containing the requested data.\n",
      "       If multiple dataset instances are retrieved then these are concatenated first.\n",
      "       \"\"\"\n",
      "       ... (142 more lines)\n",
      "\n",
      "  12. Fusion_part_10\n",
      "     Type: class_definition_part\n",
      "     Size: 1001 tokens, 104 lines\n",
      "     Content preview:\n",
      "       show_progress: bool = True,\n",
      "       columns: list[str] | None = None,\n",
      "       filters: PyArrowFilterT | None = None,\n",
      "       ... (101 more lines)\n",
      "\n",
      "  13. Fusion_part_11\n",
      "     Type: class_definition_part\n",
      "     Size: 997 tokens, 89 lines\n",
      "     Content preview:\n",
      "       def upload(  # noqa: PLR0912, PLR0913, PLR0915\n",
      "       self,\n",
      "       path: str,\n",
      "       ... (86 more lines)\n",
      "\n",
      "  14. Fusion_part_12\n",
      "     Type: class_definition_part\n",
      "     Size: 1009 tokens, 96 lines\n",
      "     Content preview:\n",
      "       is_raw_lst = is_dataset_raw(file_path_lst, fs_fusion)\n",
      "       local_url_eqiv = [path_to_url(i, r) for i, r in zip(file_path_lst, is_raw_lst)]\n",
      "       if preserve_original_name:\n",
      "       ... (93 more lines)\n",
      "\n",
      "  15. Fusion_part_13\n",
      "     Type: class_definition_part\n",
      "     Size: 1018 tokens, 140 lines\n",
      "     Content preview:\n",
      "       data_map_df.columns = [\"path\", \"url\", \"file_name\"]  # type: ignore[assignment]\n",
      "       \n",
      "       res = upload_files(\n",
      "       ... (137 more lines)\n",
      "\n",
      "  16. Fusion_part_14\n",
      "     Type: class_definition_part\n",
      "     Size: 1004 tokens, 133 lines\n",
      "     Content preview:\n",
      "       event = js.loads(msg.data)\n",
      "       # Preserve the original metaData column\n",
      "       original_meta_data = event.get(\"metaData\", {})\n",
      "       ... (130 more lines)\n",
      "\n",
      "  17. Fusion_part_15\n",
      "     Type: class_definition_part\n",
      "     Size: 990 tokens, 103 lines\n",
      "     Content preview:\n",
      "       HTTPError: If the request is unsuccessful.\n",
      "       \n",
      "       Examples:\n",
      "       ... (100 more lines)\n",
      "\n",
      "  18. Fusion_part_16\n",
      "     Type: class_definition_part\n",
      "     Size: 1003 tokens, 103 lines\n",
      "     Content preview:\n",
      "       category (str | list[str] | None, optional): Category. Defaults to None.\n",
      "       short_abstract (str, optional): Short description. Defaults to \"\".\n",
      "       description (str, optional): Description. If not provided, defaults to identifier.\n",
      "       ... (100 more lines)\n",
      "\n",
      "  19. Fusion_part_17\n",
      "     Type: class_definition_part\n",
      "     Size: 1002 tokens, 94 lines\n",
      "     Content preview:\n",
      "       Defaults to None.\n",
      "       description (str, optional): Dataset description. If not provided, defaults to identifier.\n",
      "       frequency (str, optional): The frequency of the dataset. Defaults to \"Once\".\n",
      "       ... (91 more lines)\n",
      "\n",
      "  20. Fusion_part_18\n",
      "     Type: class_definition_part\n",
      "     Size: 1002 tokens, 113 lines\n",
      "     Content preview:\n",
      "       index: int,\n",
      "       data_type: str | Types = \"String\",\n",
      "       title: str = \"\",\n",
      "       ... (110 more lines)\n",
      "\n",
      "  21. Fusion_part_19\n",
      "     Type: class_definition_part\n",
      "     Size: 1008 tokens, 140 lines\n",
      "     Content preview:\n",
      "       title: str,\n",
      "       sourceIdentifier: str | None = None,\n",
      "       description: str | None = None,\n",
      "       ... (137 more lines)\n",
      "\n",
      "  22. Fusion_part_20\n",
      "     Type: class_definition_part\n",
      "     Size: 994 tokens, 103 lines\n",
      "     Content preview:\n",
      "       return resp if return_resp_obj else None\n",
      "       \n",
      "       def list_registered_attributes(\n",
      "       ... (100 more lines)\n",
      "\n",
      "  23. Fusion_part_21\n",
      "     Type: class_definition_part\n",
      "     Size: 1000 tokens, 97 lines\n",
      "     Content preview:\n",
      "       publisher (str, optional): Name of vendor that publishes the data. Defaults to \"J.P. Morgan\".\n",
      "       product (str | list[str] | None, optional): Product to associate dataset with. Defaults to None.\n",
      "       sub_category (str | list[str] | None, optional): Sub-category. Defaults to None.\n",
      "       ... (94 more lines)\n",
      "\n",
      "  24. Fusion_part_22\n",
      "     Type: class_definition_part\n",
      "     Size: 985 tokens, 70 lines\n",
      "     Content preview:\n",
      "       frequency: str = \"Once\",\n",
      "       is_internal_only_dataset: bool = False,\n",
      "       is_third_party_data: bool = True,\n",
      "       ... (67 more lines)\n",
      "\n",
      "  25. Fusion_part_23\n",
      "     Type: class_definition_part\n",
      "     Size: 1002 tokens, 125 lines\n",
      "     Content preview:\n",
      "       is_pii (bool | None, optional): is_pii. Defaults to None.\n",
      "       is_client (bool | None, optional): is_client. Defaults to None.\n",
      "       is_public (bool | None, optional): is_public. Defaults to None.\n",
      "       ... (122 more lines)\n",
      "\n",
      "  26. Fusion_part_24\n",
      "     Type: class_definition_part\n",
      "     Size: 999 tokens, 120 lines\n",
      "     Content preview:\n",
      "       def get_async_fusion_vector_store_client(self, knowledge_base: str, catalog: str | None = None) -> AsyncOpenSearch:\n",
      "       \"\"\"Returns Fusion Embeddings Search client.\n",
      "       \n",
      "       ... (117 more lines)\n",
      "\n",
      "  27. Fusion_part_25\n",
      "     Type: class_definition_part\n",
      "     Size: 260 tokens, 43 lines\n",
      "     Content preview:\n",
      "       risk_area=risk_area,\n",
      "       risk_stripe=risk_stripe,\n",
      "       sap_code=sap_code,\n",
      "       ... (40 more lines)\n",
      "\n",
      "\n",
      "=== Processing: authentication.py ===\n",
      "File size: 6037 characters\n",
      "Found 13 semantic units\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_from_typing (8 tokens)\n",
      "    Preview: from typing import Any, Optional, Union...\n",
      "  - import_statement: import_from_urllib.parse (5 tokens)\n",
      "    Preview: from urllib.parse import urlparse...\n",
      "  - import_statement: import_import_aiohttp (3 tokens)\n",
      "    Preview: import aiohttp...\n",
      "  - import_statement: import_import_requests (2 tokens)\n",
      "    Preview: import requests...\n",
      "  - import_statement: import_from_requests.adapters (6 tokens)\n",
      "    Preview: from requests.adapters import HTTPAdapter...\n",
      "  - import_statement: import_from_urllib3.util.retry (7 tokens)\n",
      "    Preview: from urllib3.util.retry import Retry...\n",
      "  - import_statement: import_from_fusion._fusion (7 tokens)\n",
      "    Preview: from fusion._fusion import FusionCredentials...\n",
      "  - import_statement: import_from_fusion.exceptions (7 tokens)\n",
      "    Preview: from fusion.exceptions import APIResponseError...\n",
      "  - function_definition: _res_plural (106 tokens)\n",
      "    Preview: def _res_plural(ref_int: int, pluraliser: str = \"s\") -> str:     \"\"\"Private function to return the p...\n",
      "  - function_definition: _is_url (90 tokens)\n",
      "    Preview: def _is_url(url: str) -> bool:     \"\"\"Test whether the content of a string is a valid URL.      Args...\n",
      "  - class_definition: FusionOAuthAdapter (690 tokens)\n",
      "    Preview: class FusionOAuthAdapter(HTTPAdapter):     \"\"\"An OAuth adapter to manage authentication and session...\n",
      "  - class_definition: FusionAiohttpSession (287 tokens)\n",
      "    Preview: class FusionAiohttpSession(aiohttp.ClientSession):     \"\"\"Bespoke aiohttp session.\"\"\"      def __ini...\n",
      "Created 5 semantic chunks\n",
      "Final result: 5 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for authentication.py ---\n",
      "1. imports_9_statements\n",
      "   Type: imports_group\n",
      "   Size: 55 tokens, 9 lines\n",
      "   Content preview:\n",
      "     import logging\n",
      "     from typing import Any, Optional, Union\n",
      "     from urllib.parse import urlparse\n",
      "     ... (6 more lines)\n",
      "\n",
      "2. _res_plural\n",
      "   Type: function_definition\n",
      "   Size: 106 tokens, 11 lines\n",
      "   Content preview:\n",
      "     def _res_plural(ref_int: int, pluraliser: str = \"s\") -> str:\n",
      "     \"\"\"Private function to return the plural form when the number is more than one.\n",
      "     \n",
      "     ... (8 more lines)\n",
      "\n",
      "3. _is_url\n",
      "   Type: function_definition\n",
      "   Size: 90 tokens, 14 lines\n",
      "   Content preview:\n",
      "     def _is_url(url: str) -> bool:\n",
      "     \"\"\"Test whether the content of a string is a valid URL.\n",
      "     \n",
      "     ... (11 more lines)\n",
      "\n",
      "4. FusionOAuthAdapter\n",
      "   Type: class_definition\n",
      "   Size: 690 tokens, 87 lines\n",
      "   Content preview:\n",
      "     class FusionOAuthAdapter(HTTPAdapter):\n",
      "     \"\"\"An OAuth adapter to manage authentication and session tokens.\"\"\"\n",
      "     \n",
      "     ... (84 more lines)\n",
      "\n",
      "5. FusionAiohttpSession\n",
      "   Type: class_definition\n",
      "   Size: 287 tokens, 28 lines\n",
      "   Content preview:\n",
      "     class FusionAiohttpSession(aiohttp.ClientSession):\n",
      "     \"\"\"Bespoke aiohttp session.\"\"\"\n",
      "     \n",
      "     ... (25 more lines)\n",
      "\n",
      "\n",
      "=== Processing: embeddings_utils.py ===\n",
      "File size: 5987 characters\n",
      "Found 9 semantic units\n",
      "  - import_statement: import_import_json (2 tokens)\n",
      "    Preview: import json...\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_from_typing (6 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - function_definition: _format_full_index_response (195 tokens)\n",
      "    Preview: def _format_full_index_response(response: requests.Response) -> pd.DataFrame:     \"\"\"Format get inde...\n",
      "  - function_definition: _format_summary_index_response (217 tokens)\n",
      "    Preview: def _format_summary_index_response(response: requests.Response) -> pd.DataFrame:     \"\"\"Format summa...\n",
      "  - function_definition: _retrieve_index_name_from_bulk_body (113 tokens)\n",
      "    Preview: def _retrieve_index_name_from_bulk_body(body: bytes | None) -> str:     body_str = body.decode(\"utf-...\n",
      "  - function_definition: _modify_post_response_langchain (289 tokens)\n",
      "    Preview: def _modify_post_response_langchain(raw_data: str | bytes | bytearray) -> str | bytes | bytearray:...\n",
      "  - function_definition: _modify_post_haystack (436 tokens)\n",
      "    Preview: def _modify_post_haystack(knowledge_base: str | list[str] | None, body: bytes | None, method: str) -...\n",
      "Created 4 semantic chunks\n",
      "Final result: 4 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for embeddings_utils.py ---\n",
      "1. imports_4_statements\n",
      "   Type: imports_group\n",
      "   Size: 17 tokens, 4 lines\n",
      "   Content preview:\n",
      "     import json\n",
      "     import logging\n",
      "     from typing import TYPE_CHECKING\n",
      "     ... (1 more lines)\n",
      "\n",
      "2. python_group_3_definitions\n",
      "   Type: grouped_content\n",
      "   Size: 526 tokens, 64 lines\n",
      "   Content preview:\n",
      "     def _format_full_index_response(response: requests.Response) -> pd.DataFrame:\n",
      "     \"\"\"Format get index response.\n",
      "     \n",
      "     ... (61 more lines)\n",
      "\n",
      "3. python_group_1_definitions\n",
      "   Type: grouped_content\n",
      "   Size: 289 tokens, 33 lines\n",
      "   Content preview:\n",
      "     def _modify_post_response_langchain(raw_data: str | bytes | bytearray) -> str | bytes | bytearray:\n",
      "     \"\"\"Modify the response from langchain POST request to match the expected format.\n",
      "     \n",
      "     ... (30 more lines)\n",
      "\n",
      "4. _modify_post_haystack\n",
      "   Type: function_definition\n",
      "   Size: 436 tokens, 40 lines\n",
      "   Content preview:\n",
      "     def _modify_post_haystack(knowledge_base: str | list[str] | None, body: bytes | None, method: str) -> bytes | None:\n",
      "     \"\"\"Method to modify haystack POST body to match the embeddings API, which expects the embedding field to be\n",
      "     named \"vector\".\n",
      "     ... (37 more lines)\n",
      "\n",
      "\n",
      "=== Processing: exceptions.py ===\n",
      "File size: 2366 characters\n",
      "Found 7 semantic units\n",
      "  - import_statement: import_from_typing (4 tokens)\n",
      "    Preview: from typing import Optional...\n",
      "  - class_definition: APIResponseError (164 tokens)\n",
      "    Preview: class APIResponseError(Exception):     \"\"\"APIResponseError exception wrapper to handle API response...\n",
      "  - class_definition: APIRequestError (33 tokens)\n",
      "    Preview: class APIRequestError(Exception):     \"\"\"APIRequestError exception wrapper to handle API request ero...\n",
      "  - class_definition: APIConnectError (31 tokens)\n",
      "    Preview: class APIConnectError(Exception):     \"\"\"APIConnectError exception wrapper to handle API connection...\n",
      "  - class_definition: UnrecognizedFormatError (32 tokens)\n",
      "    Preview: class UnrecognizedFormatError(Exception):     \"\"\"UnrecognizedFormatError exception wrapper to handle...\n",
      "  - class_definition: CredentialError (150 tokens)\n",
      "    Preview: class CredentialError(Exception):     \"\"\"CredentialError exception wrapper to handle errors in crede...\n",
      "  - class_definition: FileFormatError (44 tokens)\n",
      "    Preview: class FileFormatError(Exception):     \"\"\"FileFormatRequiredError exception wrapper to handle errors...\n",
      "Created 1 semantic chunks\n",
      "Final result: 1 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for exceptions.py ---\n",
      "1. complete_module_7_parts\n",
      "   Type: complete_module\n",
      "   Size: 459 tokens, 67 lines\n",
      "   Content preview:\n",
      "     from typing import Optional\n",
      "     \n",
      "     class APIResponseError(Exception):\n",
      "     ... (64 more lines)\n",
      "\n",
      "\n",
      "=== Processing: product.py ===\n",
      "File size: 24857 characters\n",
      "Found 6 semantic units\n",
      "  - import_statement: import_import_json (4 tokens)\n",
      "    Preview: import json as js...\n",
      "  - import_statement: import_from_dataclasses (10 tokens)\n",
      "    Preview: from dataclasses import dataclass, field, fields...\n",
      "  - import_statement: import_from_typing (8 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_from_fusion.utils (51 tokens)\n",
      "    Preview: from fusion.utils import (     CamelCaseMeta,     _is_json,     camel_to_snake,     convert_date_for...\n",
      "  - decorated_definition: decorated_Product (5046 tokens)\n",
      "    Preview: @dataclass class Product(metaclass=CamelCaseMeta):     \"\"\"Fusion Product class for managing product...\n",
      "Created 2 semantic chunks\n",
      "  Sub-chunking decorated_Product (5046 tokens)\n",
      "    Breaking down decorated_Product (5046 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 7 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for product.py ---\n",
      "1. imports_5_statements\n",
      "   Type: imports_group\n",
      "   Size: 81 tokens, 15 lines\n",
      "   Content preview:\n",
      "     import json as js\n",
      "     from dataclasses import dataclass, field, fields\n",
      "     from typing import TYPE_CHECKING, Any\n",
      "     ... (12 more lines)\n",
      "\n",
      "  2. decorated_Product_part_1\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1014 tokens, 85 lines\n",
      "     Content preview:\n",
      "       @dataclass\n",
      "       class Product(metaclass=CamelCaseMeta):\n",
      "       \"\"\"Fusion Product class for managing product metadata in a Fusion catalog.\n",
      "       ... (82 more lines)\n",
      "\n",
      "  3. decorated_Product_part_2\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1000 tokens, 126 lines\n",
      "     Content preview:\n",
      "       else make_bool(self.is_restricted)\n",
      "       )\n",
      "       self.maintainer = (\n",
      "       ... (123 more lines)\n",
      "\n",
      "  4. decorated_Product_part_3\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1009 tokens, 119 lines\n",
      "     Content preview:\n",
      "       Product._from_series(data[data[\"identifier\"] == identifier].reset_index(drop=True).iloc[0])\n",
      "       if identifier\n",
      "       else Product._from_series(data.reset_index(drop=True).iloc[0])\n",
      "       ... (116 more lines)\n",
      "\n",
      "  5. decorated_Product_part_4\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1002 tokens, 136 lines\n",
      "     Content preview:\n",
      "       else:\n",
      "       raise TypeError(f\"Could not resolve the object provided: {product_source}\")\n",
      "       product.client = self._client\n",
      "       ... (133 more lines)\n",
      "\n",
      "  6. decorated_Product_part_5\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1011 tokens, 121 lines\n",
      "     Content preview:\n",
      "       delivery_channel = self.delivery_channel if self.delivery_channel else [\"API\"]\n",
      "       \n",
      "       self.release_date = release_date\n",
      "       ... (118 more lines)\n",
      "\n",
      "  7. decorated_Product_part_6\n",
      "     Type: decorated_definition_part\n",
      "     Size: 9 tokens, 1 lines\n",
      "     Content preview:\n",
      "       return resp if return_resp_obj else None\n",
      "\n",
      "\n",
      "=== Processing: dataflow.py ===\n",
      "File size: 4007 characters\n",
      "Found 7 semantic units\n",
      "  - import_statement: import_from_dataclasses (8 tokens)\n",
      "    Preview: from dataclasses import dataclass, field...\n",
      "  - import_statement: import_from_typing (6 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING...\n",
      "  - import_statement: import_from_fusion.dataset (5 tokens)\n",
      "    Preview: from fusion.dataset import Dataset...\n",
      "  - import_statement: import_from_fusion.utils (8 tokens)\n",
      "    Preview: from fusion.utils import requests_raise_for_status...\n",
      "  - decorated_definition: decorated_DataFlow (525 tokens)\n",
      "    Preview: @dataclass class DataFlow(Dataset):     \"\"\"Dataflow class for maintaining data flow metadata.      A...\n",
      "  - decorated_definition: decorated_InputDataFlow (151 tokens)\n",
      "    Preview: @dataclass class InputDataFlow(DataFlow):     \"\"\"InputDataFlow class for maintaining input data flow...\n",
      "  - decorated_definition: decorated_OutputDataFlow (151 tokens)\n",
      "    Preview: @dataclass class OutputDataFlow(DataFlow):     \"\"\"OutputDataFlow class for maintaining output data f...\n",
      "Created 1 semantic chunks\n",
      "Final result: 1 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for dataflow.py ---\n",
      "1. complete_module_7_parts\n",
      "   Type: complete_module\n",
      "   Size: 859 tokens, 101 lines\n",
      "   Content preview:\n",
      "     from dataclasses import dataclass, field\n",
      "     \n",
      "     from typing import TYPE_CHECKING\n",
      "     ... (98 more lines)\n",
      "\n",
      "\n",
      "=== Processing: __main__.py ===\n",
      "File size: 1597 characters\n",
      "Found 4 semantic units\n",
      "  - import_statement: import_import_argparse (2 tokens)\n",
      "    Preview: import argparse...\n",
      "  - import_statement: import_import_inspect (2 tokens)\n",
      "    Preview: import inspect...\n",
      "  - import_statement: import_from_fusion (4 tokens)\n",
      "    Preview: from fusion import Fusion...\n",
      "  - if_statement: main_block (353 tokens)\n",
      "    Preview: if __name__ == \"__main__\":     parser = argparse.ArgumentParser(description=\"Fusion command line env...\n",
      "Created 1 semantic chunks\n",
      "Final result: 1 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for __main__.py ---\n",
      "1. complete_module_4_parts\n",
      "   Type: complete_module\n",
      "   Size: 364 tokens, 49 lines\n",
      "   Content preview:\n",
      "     import argparse\n",
      "     \n",
      "     import inspect\n",
      "     ... (46 more lines)\n",
      "\n",
      "\n",
      "=== Processing: report.py ===\n",
      "File size: 18069 characters\n",
      "Found 11 semantic units\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_from_dataclasses (10 tokens)\n",
      "    Preview: from dataclasses import dataclass, field, fields...\n",
      "  - import_statement: import_from_pathlib (4 tokens)\n",
      "    Preview: from pathlib import Path...\n",
      "  - import_statement: import_from_typing (11 tokens)\n",
      "    Preview: from typing import TYPE_CHECKING, Any, TypedDict...\n",
      "  - import_statement: import_import_numpy (4 tokens)\n",
      "    Preview: import numpy as np...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_from_.utils (37 tokens)\n",
      "    Preview: from .utils import (     CamelCaseMeta,     camel_to_snake,     make_bool,     requests_raise_for_st...\n",
      "  - if_statement: if_block (22 tokens)\n",
      "    Preview: if TYPE_CHECKING:     from collections.abc import Iterator      import requests      from fusion imp...\n",
      "  - decorated_definition: decorated_Report (2942 tokens)\n",
      "    Preview: @dataclass class Report(metaclass=CamelCaseMeta):     \"\"\"     Fusion Report class for managing repor...\n",
      "  - class_definition: ass (503 tokens)\n",
      "    Preview: ass Reports:     def __init__(self, reports: list[Report] | None = None) -> None:         self.repor...\n",
      "  - class_definition: s (141 tokens)\n",
      "    Preview: s ReportsWrapper(Reports) :         def __init__(self, client: Fusion) -> None:             super()....\n",
      "Created 5 semantic chunks\n",
      "  Sub-chunking decorated_Report (2942 tokens)\n",
      "    Breaking down decorated_Report (2942 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 7 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for report.py ---\n",
      "1. imports_7_statements\n",
      "   Type: imports_group\n",
      "   Size: 78 tokens, 14 lines\n",
      "   Content preview:\n",
      "     import logging\n",
      "     from dataclasses import dataclass, field, fields\n",
      "     from pathlib import Path\n",
      "     ... (11 more lines)\n",
      "\n",
      "2. if_block\n",
      "   Type: if_statement\n",
      "   Size: 22 tokens, 6 lines\n",
      "   Content preview:\n",
      "     if TYPE_CHECKING:\n",
      "     from collections.abc import Iterator\n",
      "     \n",
      "     ... (3 more lines)\n",
      "\n",
      "  3. decorated_Report_part_1\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1017 tokens, 116 lines\n",
      "     Content preview:\n",
      "       @dataclass\n",
      "       class Report(metaclass=CamelCaseMeta):\n",
      "       \"\"\"\n",
      "       ... (113 more lines)\n",
      "\n",
      "  4. decorated_Report_part_2\n",
      "     Type: decorated_definition_part\n",
      "     Size: 1011 tokens, 121 lines\n",
      "     Content preview:\n",
      "       converted_data[\"isBCBS239Program\"] = make_bool(converted_data[\"isBCBS239Program\"])\n",
      "       \n",
      "       valid_fields = {f.name for f in fields(cls)}\n",
      "       ... (118 more lines)\n",
      "\n",
      "  5. decorated_Report_part_3\n",
      "     Type: decorated_definition_part\n",
      "     Size: 914 tokens, 127 lines\n",
      "     Content preview:\n",
      "       report_obj.client = client  # Attach the client if provided\n",
      "       \n",
      "       try:\n",
      "       ... (124 more lines)\n",
      "\n",
      "6. ass\n",
      "   Type: class_definition\n",
      "   Size: 503 tokens, 66 lines\n",
      "   Content preview:\n",
      "     ass Reports:\n",
      "     def __init__(self, reports: list[Report] | None = None) -> None:\n",
      "     self.reports = reports or []\n",
      "     ... (63 more lines)\n",
      "\n",
      "7. s\n",
      "   Type: class_definition\n",
      "   Size: 141 tokens, 15 lines\n",
      "   Content preview:\n",
      "     s ReportsWrapper(Reports) :\n",
      "     def __init__(self, client: Fusion) -> None:\n",
      "     super().__init__([])\n",
      "     ... (12 more lines)\n",
      "\n",
      "\n",
      "=== Processing: __init__.py ===\n",
      "File size: 27 characters\n",
      "Found 0 semantic units\n",
      "Created 0 semantic chunks\n",
      "Final result: 0 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for __init__.py ---\n",
      "\n",
      "=== Processing: authentication.py ===\n",
      "File size: 9393 characters\n",
      "Found 15 semantic units\n",
      "  - import_statement: import_import_json (2 tokens)\n",
      "    Preview: import json...\n",
      "  - import_statement: import_import_logging (2 tokens)\n",
      "    Preview: import logging...\n",
      "  - import_statement: import_import_os (2 tokens)\n",
      "    Preview: import os...\n",
      "  - import_statement: import_from_datetime (6 tokens)\n",
      "    Preview: from datetime import datetime, timezone...\n",
      "  - import_statement: import_from_pathlib (4 tokens)\n",
      "    Preview: from pathlib import Path...\n",
      "  - import_statement: import_from_typing (6 tokens)\n",
      "    Preview: from typing import Any, Optional...\n",
      "  - import_statement: import_import_fsspec (4 tokens)\n",
      "    Preview: import fsspec...\n",
      "  - import_statement: import_import_pandas (4 tokens)\n",
      "    Preview: import pandas as pd...\n",
      "  - import_statement: import_from_fusion.exceptions (6 tokens)\n",
      "    Preview: from fusion.exceptions import CredentialError...\n",
      "  - import_statement: import_from_fusion.utils (7 tokens)\n",
      "    Preview: from fusion.utils import get_default_fs...\n",
      "  - function_definition: try_get_env_var (107 tokens)\n",
      "    Preview: def try_get_env_var(var_name: str, default: Optional[str] = None) -> Optional[str]:     \"\"\"Get the v...\n",
      "  - function_definition: try_get_client_id (67 tokens)\n",
      "    Preview: def try_get_client_id(client_id: Optional[str]) -> Optional[str]:     \"\"\"Get the client ID from the...\n",
      "  - function_definition: try_get_client_secret (67 tokens)\n",
      "    Preview: def try_get_client_secret(client_secret: Optional[str]) -> Optional[str]:     \"\"\"Get the client secr...\n",
      "  - function_definition: try_get_fusion_e2e (81 tokens)\n",
      "    Preview: def try_get_fusion_e2e(fusion_e2e: Optional[str]) -> Optional[str]:     \"\"\"Get the Fusion E2E token...\n",
      "  - class_definition: FusionCredentials (1529 tokens)\n",
      "    Preview: class FusionCredentials:     \"\"\"Utility functions to manage credentials.\"\"\"      def __init__(  # no...\n",
      "Created 6 semantic chunks\n",
      "  Sub-chunking FusionCredentials (1529 tokens)\n",
      "    Breaking down FusionCredentials (1529 tokens) into smaller pieces...\n",
      "Sub-chunked 1 oversized chunks\n",
      "Final result: 7 total chunks\n",
      "\n",
      "--- Semantic Chunk Summary for authentication.py ---\n",
      "1. imports_10_statements\n",
      "   Type: imports_group\n",
      "   Size: 52 tokens, 10 lines\n",
      "   Content preview:\n",
      "     import json\n",
      "     import logging\n",
      "     import os\n",
      "     ... (7 more lines)\n",
      "\n",
      "2. try_get_env_var\n",
      "   Type: function_definition\n",
      "   Size: 107 tokens, 11 lines\n",
      "   Content preview:\n",
      "     def try_get_env_var(var_name: str, default: Optional[str] = None) -> Optional[str]:\n",
      "     \"\"\"Get the value of an environment variable or return a default value.\n",
      "     \n",
      "     ... (8 more lines)\n",
      "\n",
      "3. try_get_client_id\n",
      "   Type: function_definition\n",
      "   Size: 67 tokens, 9 lines\n",
      "   Content preview:\n",
      "     def try_get_client_id(client_id: Optional[str]) -> Optional[str]:\n",
      "     \"\"\"Get the client ID from the environment variable or return None.\n",
      "     \n",
      "     ... (6 more lines)\n",
      "\n",
      "4. try_get_client_secret\n",
      "   Type: function_definition\n",
      "   Size: 67 tokens, 9 lines\n",
      "   Content preview:\n",
      "     def try_get_client_secret(client_secret: Optional[str]) -> Optional[str]:\n",
      "     \"\"\"Get the client secret from the environment variable or return None.\n",
      "     \n",
      "     ... (6 more lines)\n",
      "\n",
      "5. try_get_fusion_e2e\n",
      "   Type: function_definition\n",
      "   Size: 81 tokens, 9 lines\n",
      "   Content preview:\n",
      "     def try_get_fusion_e2e(fusion_e2e: Optional[str]) -> Optional[str]:\n",
      "     \"\"\"Get the Fusion E2E token from the environment variable or return None.\n",
      "     \n",
      "     ... (6 more lines)\n",
      "\n",
      "  6. FusionCredentials_part_1\n",
      "     Type: class_definition_part\n",
      "     Size: 1008 tokens, 100 lines\n",
      "     Content preview:\n",
      "       class FusionCredentials:\n",
      "       \"\"\"Utility functions to manage credentials.\"\"\"\n",
      "       \n",
      "       ... (97 more lines)\n",
      "\n",
      "  7. FusionCredentials_part_2\n",
      "     Type: class_definition_part\n",
      "     Size: 521 tokens, 64 lines\n",
      "     Content preview:\n",
      "       grant_type = credentials.get(\"grant_type\", \"client_credentials\")\n",
      "       try:\n",
      "       if grant_type == \"client_credentials\":\n",
      "       ... (61 more lines)\n",
      "\n",
      "\n",
      "📊 Processing Summary:\n",
      "   Files processed: 20\n",
      "   Total chunks: 151\n",
      "   Total tokens: 81,739\n",
      "   Average tokens per chunk: 541.3\n",
      "\n",
      "💾 Saving chunks to markdown files...\n",
      "✅ Saved 151 chunk files to /Users/tiyadiashok/python-projects/code_chunker/rag_chunks/pre_processed/code_sources/python/fusion_latest_chunks\n",
      "📁 Directory structure preserved in output\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import secrets\n",
    "import string\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import tiktoken\n",
    "\n",
    "# Tree-sitter setup for Python\n",
    "try:\n",
    "    from tree_sitter_language_pack import get_language, get_parser\n",
    "    python_language = get_language('python')\n",
    "    python_parser = get_parser('python')\n",
    "    print(\"✅ Using Python parser\")\n",
    "except ImportError:\n",
    "    print(\"Please install: pip install tree-sitter-languages\")\n",
    "    exit(1)\n",
    "\n",
    "# Configuration\n",
    "MAX_CHUNK_TOKENS = 1000\n",
    "MAX_RECURSION_DEPTH = 3\n",
    "\n",
    "# Supported file extensions and their parsers\n",
    "SUPPORTED_EXTENSIONS = ['.py']\n",
    "PARSER_MAP = {\n",
    "    '.py': python_parser\n",
    "}\n",
    "\n",
    "# Initialize token encoder\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in text\"\"\"\n",
    "    return len(encoder.encode(text))\n",
    "\n",
    "def get_syntax_highlighting_language(file_extension: str) -> str:\n",
    "    \"\"\"Get appropriate syntax highlighting language for markdown output\"\"\"\n",
    "    language_map = {\n",
    "        '.py': 'python'\n",
    "    }\n",
    "    return language_map.get(file_extension, 'text')\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self, start_byte: int, end_byte: int, content: str, node_type: str, name: str, depth: int = 0):\n",
    "        self.start_byte = start_byte\n",
    "        self.end_byte = end_byte\n",
    "        self.content = content\n",
    "        self.node_type = node_type\n",
    "        self.name = name\n",
    "        self.depth = depth\n",
    "        self.token_count = count_tokens(content)\n",
    "        self.sub_chunks = []\n",
    "\n",
    "def extract_node_name(node, source_code: str) -> str:\n",
    "    \"\"\"Extract meaningful name from AST node\"\"\"\n",
    "    node_text = source_code[node.start_byte:node.end_byte]\n",
    "    lines = node_text.split('\\n')\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    # Try different patterns based on node type\n",
    "    if node.type == 'function_definition':\n",
    "        # Look for function name: def function_name(\n",
    "        match = re.search(r'def\\s+([a-zA-Z_][a-zA-Z0-9_]*)', node_text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    \n",
    "    elif node.type == 'class_definition':\n",
    "        # Look for class name: class ClassName\n",
    "        match = re.search(r'class\\s+([a-zA-Z_][a-zA-Z0-9_]*)', node_text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    \n",
    "    elif node.type == 'assignment':\n",
    "        # Look for variable assignment: variable_name = \n",
    "        match = re.search(r'^([a-zA-Z_][a-zA-Z0-9_]*)\\s*=', node_text.strip())\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    \n",
    "    elif node.type == 'import_statement' or node.type == 'import_from_statement':\n",
    "        # Handle import statements\n",
    "        if 'import' in node_text:\n",
    "            # Extract what's being imported\n",
    "            if 'from' in node_text:\n",
    "                # from module import something\n",
    "                match = re.search(r'from\\s+([a-zA-Z0-9_.]+)', node_text)\n",
    "                if match:\n",
    "                    return f\"from_{match.group(1)}\"\n",
    "            else:\n",
    "                # import module\n",
    "                match = re.search(r'import\\s+([a-zA-Z0-9_.]+)', node_text)\n",
    "                if match:\n",
    "                    return f\"import_{match.group(1)}\"\n",
    "    \n",
    "    elif node.type == 'decorated_definition':\n",
    "        # Handle decorated functions/classes (with @decorator)\n",
    "        # Look for the actual function/class name after decorators\n",
    "        lines_list = node_text.split('\\n')\n",
    "        for line in lines_list:\n",
    "            if line.strip().startswith('def '):\n",
    "                match = re.search(r'def\\s+([a-zA-Z_][a-zA-Z0-9_]*)', line)\n",
    "                if match:\n",
    "                    return f\"decorated_{match.group(1)}\"\n",
    "            elif line.strip().startswith('class '):\n",
    "                match = re.search(r'class\\s+([a-zA-Z_][a-zA-Z0-9_]*)', line)\n",
    "                if match:\n",
    "                    return f\"decorated_{match.group(1)}\"\n",
    "    \n",
    "    elif node.type == 'async_function_definition':\n",
    "        # Look for async function name: async def function_name(\n",
    "        match = re.search(r'async\\s+def\\s+([a-zA-Z_][a-zA-Z0-9_]*)', node_text)\n",
    "        if match:\n",
    "            return f\"async_{match.group(1)}\"\n",
    "    \n",
    "    elif node.type == 'if_statement':\n",
    "        # Special handling for if __name__ == \"__main__\"\n",
    "        if '__name__' in node_text and '__main__' in node_text:\n",
    "            return \"main_block\"\n",
    "        else:\n",
    "            return \"if_block\"\n",
    "    \n",
    "    elif node.type == 'try_statement':\n",
    "        return \"try_block\"\n",
    "    \n",
    "    elif node.type == 'with_statement':\n",
    "        return \"with_block\"\n",
    "    \n",
    "    elif node.type == 'for_statement':\n",
    "        return \"for_loop\"\n",
    "    \n",
    "    elif node.type == 'while_statement':\n",
    "        return \"while_loop\"\n",
    "    \n",
    "    # Fallback - try to extract first identifier\n",
    "    first_line = lines[0][:50].strip()\n",
    "    simple_match = re.search(r'\\b([a-zA-Z_][a-zA-Z0-9_]*)', first_line)\n",
    "    if simple_match:\n",
    "        return simple_match.group(1)\n",
    "    \n",
    "    return f\"{node.type}_{node.start_byte}\"\n",
    "\n",
    "def find_semantic_chunks(tree, source_code: str, file_extension: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Find semantic chunks - complete, meaningful code blocks\"\"\"\n",
    "    semantic_nodes = []\n",
    "    \n",
    "    def traverse(node, parent_types=None):\n",
    "        if parent_types is None:\n",
    "            parent_types = []\n",
    "        \n",
    "        current_parent_types = parent_types + [node.type]\n",
    "        node_text = source_code[node.start_byte:node.end_byte]\n",
    "        \n",
    "        # Python semantic boundaries\n",
    "        if file_extension == '.py':\n",
    "            # Top-level semantic boundaries\n",
    "            if node.type in ['import_statement', 'import_from_statement'] and len(parent_types) <= 1:\n",
    "                name = extract_node_name(node, source_code)\n",
    "                semantic_nodes.append({\n",
    "                    'node': node, 'name': f\"import_{name}\", 'start_byte': node.start_byte,\n",
    "                    'end_byte': node.end_byte, 'type': 'import_statement', 'content': node_text\n",
    "                })\n",
    "                return\n",
    "            \n",
    "            # Function definitions (including async)\n",
    "            elif node.type in ['function_definition', 'async_function_definition'] and len(parent_types) <= 2:\n",
    "                name = extract_node_name(node, source_code)\n",
    "                semantic_nodes.append({\n",
    "                    'node': node, 'name': name, 'start_byte': node.start_byte,\n",
    "                    'end_byte': node.end_byte, 'type': node.type, 'content': node_text\n",
    "                })\n",
    "                return\n",
    "            \n",
    "            # Class definitions\n",
    "            elif node.type == 'class_definition' and len(parent_types) <= 1:\n",
    "                name = extract_node_name(node, source_code)\n",
    "                semantic_nodes.append({\n",
    "                    'node': node, 'name': name, 'start_byte': node.start_byte,\n",
    "                    'end_byte': node.end_byte, 'type': node.type, 'content': node_text\n",
    "                })\n",
    "                return\n",
    "            \n",
    "            # Decorated definitions (functions/classes with decorators)\n",
    "            elif node.type == 'decorated_definition' and len(parent_types) <= 1:\n",
    "                name = extract_node_name(node, source_code)\n",
    "                semantic_nodes.append({\n",
    "                    'node': node, 'name': name, 'start_byte': node.start_byte,\n",
    "                    'end_byte': node.end_byte, 'type': node.type, 'content': node_text\n",
    "                })\n",
    "                return\n",
    "            \n",
    "            # Global assignments and constants\n",
    "            elif (node.type == 'assignment' and len(parent_types) <= 1 and \n",
    "                  len(node_text.strip()) > 50):  # Only substantial assignments\n",
    "                name = extract_node_name(node, source_code)\n",
    "                semantic_nodes.append({\n",
    "                    'node': node, 'name': name, 'start_byte': node.start_byte,\n",
    "                    'end_byte': node.end_byte, 'type': node.type, 'content': node_text\n",
    "                })\n",
    "                return\n",
    "            \n",
    "            # Control structures at module level\n",
    "            elif (node.type in ['if_statement', 'try_statement', 'with_statement', 'for_statement', 'while_statement'] \n",
    "                  and len(parent_types) <= 1 and len(node_text.strip()) > 100):\n",
    "                name = extract_node_name(node, source_code)\n",
    "                semantic_nodes.append({\n",
    "                    'node': node, 'name': name, 'start_byte': node.start_byte,\n",
    "                    'end_byte': node.end_byte, 'type': node.type, 'content': node_text\n",
    "                })\n",
    "                return\n",
    "        \n",
    "        # Continue traversing children\n",
    "        for child in node.children:\n",
    "            traverse(child, current_parent_types)\n",
    "    \n",
    "    traverse(tree.root_node)\n",
    "    \n",
    "    # Filter out overlapping nodes (keep the largest/most specific)\n",
    "    filtered_nodes = []\n",
    "    for node in semantic_nodes:\n",
    "        is_contained = False\n",
    "        for other in semantic_nodes:\n",
    "            if (other != node and \n",
    "                other['start_byte'] <= node['start_byte'] and \n",
    "                other['end_byte'] >= node['end_byte'] and\n",
    "                other['end_byte'] - other['start_byte'] > node['end_byte'] - node['start_byte']):\n",
    "                is_contained = True\n",
    "                break\n",
    "        \n",
    "        if not is_contained:\n",
    "            filtered_nodes.append(node)\n",
    "    \n",
    "    return filtered_nodes\n",
    "\n",
    "def create_semantic_chunks(semantic_nodes: List[Dict[str, Any]]) -> List[Chunk]:\n",
    "    \"\"\"Create chunks from semantic nodes\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    for node_info in semantic_nodes:\n",
    "        chunk = Chunk(\n",
    "            start_byte=node_info['start_byte'],\n",
    "            end_byte=node_info['end_byte'],\n",
    "            content=node_info['content'],\n",
    "            node_type=node_info['type'],\n",
    "            name=node_info['name'],\n",
    "            depth=0\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def group_small_chunks(chunks: List[Chunk], target_tokens: int = 600, file_extension: str = '') -> List[Chunk]:\n",
    "    \"\"\"Group small chunks together to reach reasonable size for Python files\"\"\"\n",
    "    if not chunks:\n",
    "        return chunks\n",
    "    \n",
    "    # Separate imports from other chunks\n",
    "    import_chunks = [c for c in chunks if c.node_type in ['import_statement', 'import_from_statement']]\n",
    "    other_chunks = [c for c in chunks if c.node_type not in ['import_statement', 'import_from_statement']]\n",
    "    \n",
    "    # Check if everything together is under limit\n",
    "    total_tokens = sum(c.token_count for c in chunks)\n",
    "    if total_tokens <= MAX_CHUNK_TOKENS:\n",
    "        # Combine everything into one chunk\n",
    "        combined_content = '\\n\\n'.join(c.content for c in chunks)\n",
    "        combined_chunk = Chunk(\n",
    "            start_byte=chunks[0].start_byte,\n",
    "            end_byte=chunks[-1].end_byte,\n",
    "            content=combined_content,\n",
    "            node_type='complete_module',\n",
    "            name=f\"complete_module_{len(chunks)}_parts\",\n",
    "            depth=0\n",
    "        )\n",
    "        return [combined_chunk]\n",
    "    \n",
    "    # If too large, handle imports separately\n",
    "    if import_chunks:\n",
    "        total_import_tokens = sum(c.token_count for c in import_chunks)\n",
    "        if total_import_tokens <= MAX_CHUNK_TOKENS:\n",
    "            # Combine all imports into one chunk\n",
    "            combined_imports = '\\n'.join(c.content for c in import_chunks)\n",
    "            imports_chunk = Chunk(\n",
    "                start_byte=import_chunks[0].start_byte,\n",
    "                end_byte=import_chunks[-1].end_byte,\n",
    "                content=combined_imports,\n",
    "                node_type='imports_group',\n",
    "                name=f\"imports_{len(import_chunks)}_statements\",\n",
    "                depth=0\n",
    "            )\n",
    "            import_chunks = [imports_chunk]\n",
    "    \n",
    "    # Group other chunks\n",
    "    grouped_others = group_chunks_by_size(other_chunks, target_tokens, file_extension)\n",
    "    \n",
    "    # Combine imports + other chunks\n",
    "    return import_chunks + grouped_others\n",
    "\n",
    "def group_chunks_by_size(chunks: List[Chunk], target_tokens: int = 600, file_extension: str = '') -> List[Chunk]:\n",
    "    \"\"\"Group chunks by size logic\"\"\"\n",
    "    if not chunks:\n",
    "        return chunks\n",
    "    \n",
    "    # Skip grouping if we already have reasonably sized chunks\n",
    "    if len(chunks) == 1 or any(c.token_count > target_tokens for c in chunks):\n",
    "        total_tokens = sum(c.token_count for c in chunks)\n",
    "        if total_tokens <= MAX_CHUNK_TOKENS:\n",
    "            # All chunks together are still under limit - combine them\n",
    "            if len(chunks) > 1:\n",
    "                combined_content = '\\n\\n'.join(c.content for c in chunks)\n",
    "                group_name = f\"python_module_{len(chunks)}_definitions\"\n",
    "                \n",
    "                combined_chunk = Chunk(\n",
    "                    start_byte=chunks[0].start_byte,\n",
    "                    end_byte=chunks[-1].end_byte,\n",
    "                    content=combined_content,\n",
    "                    node_type='grouped_content',\n",
    "                    name=group_name,\n",
    "                    depth=0\n",
    "                )\n",
    "                return [combined_chunk]\n",
    "        return chunks\n",
    "    \n",
    "    # Group small chunks\n",
    "    grouped_chunks = []\n",
    "    current_group = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if current_tokens + chunk.token_count > target_tokens and current_group:\n",
    "            # Finalize current group\n",
    "            group_content = '\\n\\n'.join(c.content for c in current_group)\n",
    "            group_name = f\"python_group_{len(current_group)}_definitions\"\n",
    "            \n",
    "            grouped_chunk = Chunk(\n",
    "                start_byte=current_group[0].start_byte,\n",
    "                end_byte=current_group[-1].end_byte,\n",
    "                content=group_content,\n",
    "                node_type='grouped_content',\n",
    "                name=group_name,\n",
    "                depth=0\n",
    "            )\n",
    "            grouped_chunks.append(grouped_chunk)\n",
    "            \n",
    "            # Start new group\n",
    "            current_group = [chunk]\n",
    "            current_tokens = chunk.token_count\n",
    "        else:\n",
    "            current_group.append(chunk)\n",
    "            current_tokens += chunk.token_count\n",
    "    \n",
    "    # Add final group\n",
    "    if current_group:\n",
    "        if len(current_group) == 1:\n",
    "            grouped_chunks.append(current_group[0])\n",
    "        else:\n",
    "            group_content = '\\n\\n'.join(c.content for c in current_group)\n",
    "            group_name = f\"python_group_{len(current_group)}_definitions\"\n",
    "            \n",
    "            grouped_chunk = Chunk(\n",
    "                start_byte=current_group[0].start_byte,\n",
    "                end_byte=current_group[-1].end_byte,\n",
    "                content=group_content,\n",
    "                node_type='grouped_content',\n",
    "                name=group_name,\n",
    "                depth=0\n",
    "            )\n",
    "            grouped_chunks.append(grouped_chunk)\n",
    "    \n",
    "    return grouped_chunks\n",
    "\n",
    "def sub_chunk_by_statements(chunk: Chunk, tree, source_code: str, depth: int = 0) -> List[Chunk]:\n",
    "    \"\"\"Sub-chunk by breaking down into logical statements/blocks\"\"\"\n",
    "    if depth >= MAX_RECURSION_DEPTH or chunk.token_count <= MAX_CHUNK_TOKENS:\n",
    "        return [chunk]\n",
    "    \n",
    "    print(f\"    Breaking down {chunk.name} ({chunk.token_count} tokens) into smaller pieces...\")\n",
    "    \n",
    "    # Simple line-based splitting for now\n",
    "    lines = chunk.content.split('\\n')\n",
    "    sub_chunks = []\n",
    "    current_lines = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        line_tokens = count_tokens(line)\n",
    "        \n",
    "        if current_size + line_tokens > MAX_CHUNK_TOKENS and current_lines:\n",
    "            # Create sub-chunk\n",
    "            sub_content = '\\n'.join(current_lines)\n",
    "            if sub_content.strip():\n",
    "                sub_chunk = Chunk(\n",
    "                    start_byte=chunk.start_byte,  # Approximate\n",
    "                    end_byte=chunk.start_byte + len(sub_content),\n",
    "                    content=sub_content,\n",
    "                    node_type=f\"{chunk.node_type}_part\",\n",
    "                    name=f\"{chunk.name}_part_{len(sub_chunks)+1}\",\n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                sub_chunks.append(sub_chunk)\n",
    "            \n",
    "            current_lines = [line]\n",
    "            current_size = line_tokens\n",
    "        else:\n",
    "            current_lines.append(line)\n",
    "            current_size += line_tokens\n",
    "    \n",
    "    # Add remaining lines\n",
    "    if current_lines:\n",
    "        sub_content = '\\n'.join(current_lines)\n",
    "        if sub_content.strip():\n",
    "            sub_chunk = Chunk(\n",
    "                start_byte=chunk.start_byte,\n",
    "                end_byte=chunk.end_byte,\n",
    "                content=sub_content,\n",
    "                node_type=f\"{chunk.node_type}_part\",\n",
    "                name=f\"{chunk.name}_part_{len(sub_chunks)+1}\",\n",
    "                depth=depth + 1\n",
    "            )\n",
    "            sub_chunks.append(sub_chunk)\n",
    "    \n",
    "    chunk.sub_chunks = sub_chunks\n",
    "    return sub_chunks if len(sub_chunks) > 1 else [chunk]\n",
    "\n",
    "def process_python_file(file_path: Path) -> List[Chunk]:\n",
    "    \"\"\"Process a single .py file and return semantic chunks\"\"\"\n",
    "    print(f\"\\n=== Processing: {file_path.name} ===\")\n",
    "    \n",
    "    # Read file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        source_code = f.read()\n",
    "    \n",
    "    print(f\"File size: {len(source_code)} characters\")\n",
    "    \n",
    "    # Parse with Tree-sitter Python parser\n",
    "    tree = python_parser.parse(source_code.encode('utf-8'))\n",
    "    \n",
    "    if tree.root_node.has_error:\n",
    "        print(\"⚠️ Parse errors detected\")\n",
    "    \n",
    "    # Find semantic chunks\n",
    "    semantic_nodes = find_semantic_chunks(tree, source_code, file_path.suffix)\n",
    "    print(f\"Found {len(semantic_nodes)} semantic units\")\n",
    "    \n",
    "    # Show what we found\n",
    "    for node in semantic_nodes:\n",
    "        preview = node['content'][:100].replace('\\n', ' ').strip()\n",
    "        print(f\"  - {node['type']}: {node['name']} ({count_tokens(node['content'])} tokens)\")\n",
    "        print(f\"    Preview: {preview}...\")\n",
    "    \n",
    "    # Create chunks\n",
    "    base_chunks = create_semantic_chunks(semantic_nodes)\n",
    "    \n",
    "    # Group small chunks\n",
    "    base_chunks = group_small_chunks(base_chunks, target_tokens=600, file_extension=file_path.suffix)\n",
    "    \n",
    "    print(f\"Created {len(base_chunks)} semantic chunks\")\n",
    "    \n",
    "    # Apply sub-chunking for oversized chunks\n",
    "    final_chunks = []\n",
    "    oversized_count = 0\n",
    "    \n",
    "    for chunk in base_chunks:\n",
    "        if chunk.token_count > MAX_CHUNK_TOKENS:\n",
    "            print(f\"  Sub-chunking {chunk.name} ({chunk.token_count} tokens)\")\n",
    "            sub_chunks = sub_chunk_by_statements(chunk, tree, source_code)\n",
    "            final_chunks.extend(sub_chunks)\n",
    "            oversized_count += 1\n",
    "        else:\n",
    "            final_chunks.append(chunk)\n",
    "    \n",
    "    if oversized_count > 0:\n",
    "        print(f\"Sub-chunked {oversized_count} oversized chunks\")\n",
    "    print(f\"Final result: {len(final_chunks)} total chunks\")\n",
    "    \n",
    "    return final_chunks\n",
    "\n",
    "def generate_unique_id(length: int = 6) -> str:\n",
    "    \"\"\"Generate a random unique ID\"\"\"\n",
    "    alphabet = string.ascii_lowercase + string.digits\n",
    "    return ''.join(secrets.choice(alphabet) for _ in range(length))\n",
    "\n",
    "def create_chunk_filename(original_filename: str, chunk_number: int, unique_id: str) -> str:\n",
    "    \"\"\"Create chunk filename: script.py_chunk_001_a1s2d3.md\"\"\"\n",
    "    return f\"{original_filename}_chunk_{chunk_number:03d}_{unique_id}.md\"\n",
    "\n",
    "def get_markdown_language(file_extension: str) -> str:\n",
    "    \"\"\"Get markdown language for code blocks\"\"\"\n",
    "    lang_map = {\n",
    "        '.py': 'python'\n",
    "    }\n",
    "    return lang_map.get(file_extension, 'text')\n",
    "\n",
    "def create_chunk_markdown(chunk: Chunk, source_file_path: str, file_extension: str) -> str:\n",
    "    \"\"\"Create markdown content with YAML frontmatter\"\"\"\n",
    "    language = get_markdown_language(file_extension)\n",
    "    unique_id = generate_unique_id()\n",
    "    \n",
    "    frontmatter = f\"\"\"---\n",
    "file_path: \"{source_file_path}\"\n",
    "chunk_id: \"{unique_id}\"\n",
    "chunk_type: \"{chunk.node_type}\"\n",
    "chunk_name: \"{chunk.name}\"\n",
    "start_byte: {chunk.start_byte}\n",
    "end_byte: {chunk.end_byte}\n",
    "token_count: {chunk.token_count}\n",
    "depth: {chunk.depth}\n",
    "language: \"{language}\"\n",
    "---\n",
    "\n",
    "# {chunk.name}\n",
    "\n",
    "**Type:** {chunk.node_type}  \n",
    "**Tokens:** {chunk.token_count}  \n",
    "**Depth:** {chunk.depth}\n",
    "\n",
    "```{language}\n",
    "{chunk.content}\n",
    "```\n",
    "\"\"\"\n",
    "    return frontmatter\n",
    "\n",
    "def print_chunk_summary(chunks: List[Chunk], file_name: str):\n",
    "    \"\"\"Print detailed summary of chunks\"\"\"\n",
    "    print(f\"\\n--- Semantic Chunk Summary for {file_name} ---\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        indent = \"  \" * chunk.depth\n",
    "        content_lines = len(chunk.content.split('\\n'))\n",
    "        \n",
    "        print(f\"{indent}{i}. {chunk.name}\")\n",
    "        print(f\"{indent}   Type: {chunk.node_type}\")\n",
    "        print(f\"{indent}   Size: {chunk.token_count} tokens, {content_lines} lines\")\n",
    "        print(f\"{indent}   Content preview:\")\n",
    "        \n",
    "        # Show first few lines of actual content\n",
    "        content_lines_list = chunk.content.split('\\n')\n",
    "        for j, line in enumerate(content_lines_list[:3]):\n",
    "            print(f\"{indent}     {line.strip()}\")\n",
    "        if len(content_lines_list) > 3:\n",
    "            print(f\"{indent}     ... ({len(content_lines_list) - 3} more lines)\")\n",
    "        print()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for Python semantic chunking\"\"\"\n",
    "    print(\"🚀 Python Semantic Chunking\")\n",
    "    print(f\"Max chunk tokens: {MAX_CHUNK_TOKENS}\")\n",
    "    print(f\"Max recursion depth: {MAX_RECURSION_DEPTH}\")\n",
    "    print(f\"Supported extensions: {', '.join(SUPPORTED_EXTENSIONS)}\")\n",
    "    \n",
    "    # Get directory from user or use current directory\n",
    "    directory = input(\"\\nEnter source directory path (or press Enter for current directory): \").strip()\n",
    "    if not directory:\n",
    "        directory = \".\"\n",
    "    \n",
    "    target_dir = Path(directory).resolve()\n",
    "    if not target_dir.exists():\n",
    "        print(f\"❌ Directory not found: {directory}\")\n",
    "        return\n",
    "    \n",
    "    # Create output directory parallel to source directory\n",
    "    output_dir = target_dir.parent / f\"{target_dir.name}_chunks\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    print(f\"📁 Output directory: {output_dir}\")\n",
    "    \n",
    "    # Find all Python files\n",
    "    all_files = list(target_dir.rglob(\"*.py\"))\n",
    "    \n",
    "    if not all_files:\n",
    "        print(f\"❌ No Python files found in {directory}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📁 Found {len(all_files)} Python files:\")\n",
    "    \n",
    "    # Group by directory for summary\n",
    "    by_dir = {}\n",
    "    for f in all_files:\n",
    "        rel_path = f.relative_to(target_dir)\n",
    "        dir_path = str(rel_path.parent) if rel_path.parent != Path('.') else '.'\n",
    "        by_dir[dir_path] = by_dir.get(dir_path, []) + [f.name]\n",
    "    \n",
    "    for dir_path, files in sorted(by_dir.items()):\n",
    "        print(f\"  📂 {dir_path}: {len(files)} files\")\n",
    "        for file_name in sorted(files)[:3]:  # Show first 3 files\n",
    "            print(f\"    📄 {file_name}\")\n",
    "        if len(files) > 3:\n",
    "            print(f\"    ... and {len(files) - 3} more\")\n",
    "    \n",
    "    # Process all files automatically\n",
    "    print(f\"\\n🔄 Processing all {len(all_files)} file(s)...\")\n",
    "    all_chunks = {}\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            chunks = process_python_file(file_path)\n",
    "            all_chunks[file_path] = chunks\n",
    "            print_chunk_summary(chunks, file_path.name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Summary\n",
    "    total_chunks = sum(len(chunks) for chunks in all_chunks.values())\n",
    "    total_tokens = sum(chunk.token_count for chunks in all_chunks.values() for chunk in chunks)\n",
    "    \n",
    "    print(f\"\\n📊 Processing Summary:\")\n",
    "    print(f\"   Files processed: {len(all_chunks)}\")\n",
    "    print(f\"   Total chunks: {total_chunks}\")\n",
    "    print(f\"   Total tokens: {total_tokens:,}\")\n",
    "    print(f\"   Average tokens per chunk: {total_tokens/total_chunks:.1f}\" if total_chunks > 0 else \"   No chunks created\")\n",
    "    \n",
    "    # Save chunks automatically with parallel directory structure\n",
    "    print(f\"\\n💾 Saving chunks to markdown files...\")\n",
    "    saved_count = 0\n",
    "    \n",
    "    for file_path, chunks in all_chunks.items():\n",
    "        # Calculate relative path from source directory\n",
    "        rel_path = file_path.relative_to(target_dir)\n",
    "        \n",
    "        # Create corresponding directory structure in output directory\n",
    "        chunk_dir = output_dir / rel_path.parent\n",
    "        chunk_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            chunk_filename = create_chunk_filename(file_path.name, i, generate_unique_id())\n",
    "            chunk_content = create_chunk_markdown(chunk, str(rel_path), file_path.suffix)\n",
    "            \n",
    "            output_path = chunk_dir / chunk_filename\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(chunk_content)\n",
    "            saved_count += 1\n",
    "    \n",
    "    print(f\"✅ Saved {saved_count} chunk files to {output_dir}\")\n",
    "    print(f\"📁 Directory structure preserved in output\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
